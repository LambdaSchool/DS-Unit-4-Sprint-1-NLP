{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Coding Challenge - Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd1ltfV8QShV",
        "colab_type": "text"
      },
      "source": [
        "### Coding Challenge: Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWWDecybQ1zz",
        "colab_type": "text"
      },
      "source": [
        "In this Coding Challenge, you will cover **Word2vec** which is a popular algorithm for building vector representations of words (i.e. word embeddings). The concept behind Word2Vec is quite straightforward - an assumption is made that the meaning of a word can be inferred by the *context it appears in* or *the company it keeps*. This is similar to stating: “tell me about your friends, and I will tell who you are”. \n",
        "\n",
        "If **2** words  have very similar neighbors (meaning: the context in which it is used is similar), then the words are most likely quite similar.\n",
        "\n",
        "In this Coding Challenge, you will go through the process of training a Word2vec model with a sample set of documents and then examine certain attributes of the model. After that, you will train a Word2vec model with a large corpus of text and then ascertain the similarity among words in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U6ACjkamo-G",
        "colab_type": "code",
        "outputId": "30386347-7244-4ea3-b5fe-942ca538fcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "# https://radimrehurek.com/gensim/install.html\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.10.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.7)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LHb47usm2_L",
        "colab_type": "code",
        "outputId": "a2263c21-cdf4-4b75-81c8-5d6b021fbd6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAQZT_ZgEEk",
        "colab_type": "text"
      },
      "source": [
        "**Step #1:** Tokenize the sample set of documents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoEKZI3SMJZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1\n",
        "import gensim\n",
        "\n",
        "raw_content = [('The dog ran up the steps and entered the owner\\'s room to '\n",
        "                'check if the owner was in the room.'),\n",
        "               ('My name is Thomson Comer, commander of the Machine Learning '\n",
        "                'program at Lambda school.'),\n",
        "               ('I am creating the curriculum for the Machine Learning program '\n",
        "                'and will be teaching the full-time Machine Learning program.'),\n",
        "               ('Machine Learning is one of my favorite subjects.'),\n",
        "               ('I am excited about taking the Machine Learning class at the '\n",
        "                'Lambda school starting in April.'),\n",
        "               ('When does the Machine Learning program kick-off at Lambda '\n",
        "                'school?'),\n",
        "               ('The batter hit the ball out off AT&T park into the pacific '\n",
        "                'ocean.'),\n",
        "               'The pitcher threw the ball into the dug-out.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAh1YHFFqc_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_content = [s.lower().replace('\\s+', ' ') for s in raw_content]\n",
        "simple_content = [s.lower().replace('\\'', '') for s in simple_content]\n",
        "simple_content = [s.lower().replace('-', '') for s in simple_content]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1iWMfj5poqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "tokenized_content = [tokenizer.tokenize(s) for s in simple_content]\n",
        "tokenized_content = [[lemmatizer.lemmatize(t) for t in tokenlist if \\\n",
        "                      t not in stop_words] for tokenlist in tokenized_content]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C9c8rH4p8lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebdbc5f1-bbae-446e-e0ac-8f036bdb0a3c"
      },
      "source": [
        "tokenized_content[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog', 'ran', 'step', 'entered', 'owner', 'room', 'check', 'owner', 'room']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aews-ibejOF7",
        "colab_type": "text"
      },
      "source": [
        "**Step #2:** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "657QHc_Rnw1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2\n",
        "model = gensim.models.Word2Vec(tokenized_content,\n",
        "                                size=50,\n",
        "                                window=10,\n",
        "                                min_count=1,\n",
        "                                workers=10,\n",
        "                                iter=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wor0XODRoc0M",
        "colab_type": "text"
      },
      "source": [
        "**Step #3:** Output the number of words as well as the list of words in the model's vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbP2v2erouxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "660101a7-1b8c-428d-c388-2e42c7962c9c"
      },
      "source": [
        "# Step 3\n",
        "len(model.wv.vocab)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbU9buFQs4x6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "d5679391-dd50-4846-8647-ac0432ee4a1c"
      },
      "source": [
        "for key in model.wv.vocab.keys():\n",
        "  print(key)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n",
            "ran\n",
            "step\n",
            "entered\n",
            "owner\n",
            "room\n",
            "check\n",
            "name\n",
            "thomson\n",
            "comer\n",
            "commander\n",
            "machine\n",
            "learning\n",
            "program\n",
            "lambda\n",
            "school\n",
            "creating\n",
            "curriculum\n",
            "teaching\n",
            "fulltime\n",
            "one\n",
            "favorite\n",
            "subject\n",
            "excited\n",
            "taking\n",
            "class\n",
            "starting\n",
            "april\n",
            "kickoff\n",
            "batter\n",
            "hit\n",
            "ball\n",
            "park\n",
            "pacific\n",
            "ocean\n",
            "pitcher\n",
            "threw\n",
            "dugout\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0uQwqdLpcpk",
        "colab_type": "text"
      },
      "source": [
        "**Step #4:** Output the vector of words for the following tokens: **a)** curriculum, **b)** ocean, and **c)** pitcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v86ElbNTp4TY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "1aa6e076-73b6-44ed-dc28-ecc3ab4c4c99"
      },
      "source": [
        "# Step 4\n",
        "model['curriculum']"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0078973 ,  0.00900684, -0.00148584, -0.00964239,  0.00205151,\n",
              "        0.00615953, -0.00172741, -0.00412862, -0.00307044, -0.00361187,\n",
              "        0.00161317, -0.00070418,  0.00229991, -0.00167504,  0.00807413,\n",
              "       -0.00904608, -0.00427291,  0.0086932 ,  0.00106635, -0.00428049,\n",
              "       -0.0047743 , -0.00514426,  0.00827205,  0.00878079, -0.00558444,\n",
              "       -0.00423884,  0.00248194,  0.00764772, -0.00977805,  0.0065634 ,\n",
              "        0.00692894, -0.00653495,  0.001719  ,  0.00543719, -0.00163612,\n",
              "        0.00565811, -0.00258197, -0.00619078,  0.00199268,  0.00101762,\n",
              "        0.00792677, -0.00434621,  0.00810428, -0.00185776, -0.00555382,\n",
              "       -0.00183626,  0.00511515, -0.00263665,  0.00847675,  0.00133379],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdNhwD3zujPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "72b37868-fd14-42a6-d932-e4a68cefbb23"
      },
      "source": [
        "model['ocean']"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00922351,  0.00657452, -0.00377176,  0.00106541, -0.0083246 ,\n",
              "       -0.00507166,  0.00896357,  0.00470665, -0.0019717 ,  0.00184349,\n",
              "        0.00068058, -0.00321811, -0.00614252, -0.00031749, -0.00151758,\n",
              "       -0.0024629 , -0.00344092, -0.00024497,  0.00463977,  0.00440908,\n",
              "       -0.00127755, -0.00998037, -0.00791304, -0.0017962 , -0.00366164,\n",
              "       -0.00394903, -0.00662783, -0.00541074,  0.00599303, -0.00153011,\n",
              "       -0.00405876,  0.00664217,  0.00574318,  0.00257567, -0.00303558,\n",
              "        0.00853761, -0.00551205, -0.00138665,  0.00916395, -0.00356858,\n",
              "       -0.00658254,  0.00440606, -0.0006951 , -0.00529999,  0.00625745,\n",
              "       -0.0078224 , -0.00920512, -0.00369702, -0.00414902, -0.00067215],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQKmv7EUuqEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "dc99ca45-391b-4f17-dd3a-95fd917c2fad"
      },
      "source": [
        "model['pitcher']"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00303153,  0.0085448 ,  0.00118706, -0.00557101, -0.00078175,\n",
              "        0.00610707,  0.00010288,  0.00682049, -0.00532219,  0.00613966,\n",
              "       -0.00198091,  0.00525637,  0.00083343,  0.00510037,  0.00417584,\n",
              "        0.00510645,  0.00760938, -0.00514985,  0.0056145 ,  0.00110501,\n",
              "       -0.00507975, -0.0053293 , -0.00990335, -0.00927391, -0.00679142,\n",
              "        0.002471  ,  0.00588116,  0.00451168,  0.00798106, -0.00736037,\n",
              "       -0.00797739,  0.00615937,  0.004915  , -0.00532823, -0.00862616,\n",
              "       -0.00556596, -0.00777058,  0.00679734,  0.00011425, -0.00884646,\n",
              "        0.00526021,  0.00024323, -0.00376152,  0.00411266,  0.0090164 ,\n",
              "        0.0018343 ,  0.00252077,  0.00663791,  0.00014646, -0.00459627],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwJ_9R2q0qH",
        "colab_type": "text"
      },
      "source": [
        "**Step #5:** Now we are going to train the model with more data - larger corpus i.e. the 20 newsgroups text dataset. Fetch the data from the training subset\n",
        "\n",
        "*Reference*: http://scikit-learn.org/stable/datasets/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNmfKhit91S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5\n",
        "import sklearn\n",
        "\n",
        "data = sklearn.datasets.fetch_20newsgroups(subset='train', remove=('headers', \n",
        "                                                                   'footers', \n",
        "                                                                   'quotes'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmnUlfUMufpb",
        "colab_type": "text"
      },
      "source": [
        "**Step #6:** Output the metadata for the data that is fetched"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wytJ9wL7u-Zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "044424ce-6624-4f50-ec2e-672af9cbee07"
      },
      "source": [
        "# Step 6\n",
        "data.keys()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKoWJaQx1ArB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cacb3ce-12d1-4435-c13e-e522c9692fa3"
      },
      "source": [
        "print(data['DESCR'])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "\n",
            "Usage\n",
            "~~~~~\n",
            "\n",
            "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
            "fetching / caching functions that downloads the data archive from\n",
            "the original `20 newsgroups website`_, extracts the archive contents\n",
            "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
            ":func:`sklearn.datasets.load_files` on either the training or\n",
            "testing set folder, or both of them::\n",
            "\n",
            "  >>> from sklearn.datasets import fetch_20newsgroups\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
            "\n",
            "  >>> from pprint import pprint\n",
            "  >>> pprint(list(newsgroups_train.target_names))\n",
            "  ['alt.atheism',\n",
            "   'comp.graphics',\n",
            "   'comp.os.ms-windows.misc',\n",
            "   'comp.sys.ibm.pc.hardware',\n",
            "   'comp.sys.mac.hardware',\n",
            "   'comp.windows.x',\n",
            "   'misc.forsale',\n",
            "   'rec.autos',\n",
            "   'rec.motorcycles',\n",
            "   'rec.sport.baseball',\n",
            "   'rec.sport.hockey',\n",
            "   'sci.crypt',\n",
            "   'sci.electronics',\n",
            "   'sci.med',\n",
            "   'sci.space',\n",
            "   'soc.religion.christian',\n",
            "   'talk.politics.guns',\n",
            "   'talk.politics.mideast',\n",
            "   'talk.politics.misc',\n",
            "   'talk.religion.misc']\n",
            "\n",
            "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
            "attribute is the integer index of the category::\n",
            "\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
            "\n",
            "It is possible to load only a sub-selection of the categories by passing the\n",
            "list of the categories to load to the\n",
            ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
            "\n",
            "  >>> cats = ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
            "\n",
            "  >>> list(newsgroups_train.target_names)\n",
            "  ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "\n",
            "Converting text to vectors\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "In order to feed predictive or clustering models with the text data,\n",
            "one first need to turn the text into vectors of numerical values suitable\n",
            "for statistical analysis. This can be achieved with the utilities of the\n",
            "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
            "example that extract `TF-IDF`_ vectors of unigram tokens\n",
            "from a subset of 20news::\n",
            "\n",
            "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
            "  ...               'comp.graphics', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectorizer = TfidfVectorizer()\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> vectors.shape\n",
            "  (2034, 34118)\n",
            "\n",
            "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
            "components by sample in a more than 30000-dimensional space\n",
            "(less than .5% non-zero features)::\n",
            "\n",
            "  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n",
            "  159.01327...\n",
            "\n",
            ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
            "returns ready-to-use token counts features instead of file names.\n",
            "\n",
            ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
            ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
            "\n",
            "\n",
            "Filtering text for more realistic training\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "It is easy for a classifier to overfit on particular things that appear in the\n",
            "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
            "high F-scores, but their results would not generalize to other documents that\n",
            "aren't from this window of time.\n",
            "\n",
            "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
            "which is fast to train and achieves a decent F-score::\n",
            "\n",
            "  >>> from sklearn.naive_bayes import MultinomialNB\n",
            "  >>> from sklearn import metrics\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
            "  0.88213...\n",
            "\n",
            "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
            "the training and test data, instead of segmenting by time, and in that case\n",
            "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
            "yet of what's going on inside this classifier?)\n",
            "\n",
            "Let's take a look at what the most informative features are:\n",
            "\n",
            "  >>> import numpy as np\n",
            "  >>> def show_top10(classifier, vectorizer, categories):\n",
            "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
            "  ...     for i, category in enumerate(categories):\n",
            "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
            "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
            "  ...\n",
            "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
            "  alt.atheism: edu it and in you that is of to the\n",
            "  comp.graphics: edu in graphics it is for and of to the\n",
            "  sci.space: edu it that is in and space to of the\n",
            "  talk.religion.misc: not it you in is that and to of the\n",
            "\n",
            "\n",
            "You can now see many things that these features have overfit to:\n",
            "\n",
            "- Almost every group is distinguished by whether headers such as\n",
            "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
            "- Another significant feature involves whether the sender is affiliated with\n",
            "  a university, as indicated either by their headers or their signature.\n",
            "- The word \"article\" is a significant feature, based on how often people quote\n",
            "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
            "  wrote:\"\n",
            "- Other features match the names and e-mail addresses of particular people who\n",
            "  were posting at the time.\n",
            "\n",
            "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
            "barely have to identify topics from text at all, and they all perform at the\n",
            "same high level.\n",
            "\n",
            "For this reason, the functions that load 20 Newsgroups data provide a\n",
            "parameter called **remove**, telling it what kinds of information to strip out\n",
            "of each file. **remove** should be a tuple containing any subset of\n",
            "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
            "blocks, and quotation blocks respectively.\n",
            "\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n",
            "  0.77310...\n",
            "\n",
            "This classifier lost over a lot of its F-score, just because we removed\n",
            "metadata that has little to do with topic classification.\n",
            "It loses even more if we also strip this metadata from the training data:\n",
            "\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
            "  0.76995...\n",
            "\n",
            "Some other classifiers cope better with this harder version of the task. Try\n",
            "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
            "the ``--filter`` option to compare the results.\n",
            "\n",
            ".. topic:: Recommendation\n",
            "\n",
            "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
            "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
            "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
            "  lower because it is more realistic.\n",
            "\n",
            ".. topic:: Examples\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ZdFHdV1tZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = data['data']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb54jbjPwIxO",
        "colab_type": "text"
      },
      "source": [
        "**Step #7:** Output the # of posts across the different categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__SJ7enmwQtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 7\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'text': data['data'], \n",
        "                   'category': data['target']})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP7dESf73VoI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "142069fe-ed47-4b60-f97d-022437913eda"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was wondering if anyone out there could enli...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A fair number of brave souls who upgraded thei...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category\n",
              "0  I was wondering if anyone out there could enli...         7\n",
              "1  A fair number of brave souls who upgraded thei...         4\n",
              "2  well folks, my mac plus finally gave up the gh...         4\n",
              "3  \\nDo you have Weitek's address/phone number?  ...         1\n",
              "4  From article <C5owCB.n3p@world.std.com>, by to...        14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTKAEKDZ38fn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "7884972e-e240-440f-caef-bbcedb2c2c7e"
      },
      "source": [
        "data['target_names']"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvnOofQs3-nu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "43f91447-0513-4dca-e0bd-8708fc6a6648"
      },
      "source": [
        "df.category.value_counts().sort_index()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     480\n",
              "1     584\n",
              "2     591\n",
              "3     590\n",
              "4     578\n",
              "5     593\n",
              "6     585\n",
              "7     594\n",
              "8     598\n",
              "9     597\n",
              "10    600\n",
              "11    595\n",
              "12    591\n",
              "13    594\n",
              "14    593\n",
              "15    599\n",
              "16    546\n",
              "17    564\n",
              "18    465\n",
              "19    377\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQLf2PXwcwd",
        "colab_type": "text"
      },
      "source": [
        "**Step #8**: Tokenize the body of text for each post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rj_dddBw5_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 8\n",
        "df['text'] = df['text'].str.lower().str.replace('\\s+', ' ')\n",
        "df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '')\n",
        "docs = [s.replace('\\n', ' ') for s in df['text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBL06_jjFMVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = [tokenizer.tokenize(doc) for doc in docs]\n",
        "tokens = [[lemmatizer.lemmatize(t) for t in tokenlist if \\\n",
        "                      t not in stop_words] for tokenlist in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYgCc4N-EMnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "0ab3809f-bb7e-4371-b324-27376a73b473"
      },
      "source": [
        "tokens[0]"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wondering',\n",
              " 'anyone',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'car',\n",
              " 'saw',\n",
              " 'day',\n",
              " '2door',\n",
              " 'sport',\n",
              " 'car',\n",
              " 'looked',\n",
              " 'late',\n",
              " '60',\n",
              " 'early',\n",
              " '70',\n",
              " 'called',\n",
              " 'bricklin',\n",
              " 'door',\n",
              " 'really',\n",
              " 'small',\n",
              " 'addition',\n",
              " 'front',\n",
              " 'bumper',\n",
              " 'separate',\n",
              " 'rest',\n",
              " 'body',\n",
              " 'know',\n",
              " 'anyone',\n",
              " 'tellme',\n",
              " 'model',\n",
              " 'name',\n",
              " 'engine',\n",
              " 'spec',\n",
              " 'year',\n",
              " 'production',\n",
              " 'car',\n",
              " 'made',\n",
              " 'history',\n",
              " 'whatever',\n",
              " 'info',\n",
              " 'funky',\n",
              " 'looking',\n",
              " 'car',\n",
              " 'please',\n",
              " 'email']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRgRpvJxN0h",
        "colab_type": "text"
      },
      "source": [
        "**Step #9**: Train the Word2vec model - words should show up at least 3 times in the corpus of text\n",
        "and the size of each word vector is 200 (i.e. dimension = 200)\n",
        "\n",
        "Reference\" Scroll down to the section \"A closer look at the parameter settings\" to review the parameters that can be set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PInKDc0vxXr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 9\n",
        "model = gensim.models.Word2Vec(tokens,\n",
        "                               size=200,\n",
        "                               window=10,\n",
        "                               min_count=3,\n",
        "                               workers=4,\n",
        "                               iter=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIg7JCdcxrCr",
        "colab_type": "text"
      },
      "source": [
        "**Step #10**:  List the number of words in the model's vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYEmDcwoxxCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91db31c5-2604-4884-8957-44396c3fc949"
      },
      "source": [
        "# Step 10\n",
        "len(model.wv.vocab)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF-RZy-1CRH2",
        "colab_type": "text"
      },
      "source": [
        "**Step #11:** Examine word similarity to the word \"Christ\" (find other words most similar to it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoYABV8E5FgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "0a2f309d-0a78-44b9-bb4e-6cafd2ac3c8f"
      },
      "source": [
        "# Step 11\n",
        "model.wv.most_similar(positive='christ')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jesus', 0.9507708549499512),\n",
              " ('resurrection', 0.9398853778839111),\n",
              " ('holy', 0.9357596635818481),\n",
              " ('heaven', 0.933609127998352),\n",
              " ('spirit', 0.9299333095550537),\n",
              " ('salvation', 0.9290993213653564),\n",
              " ('lord', 0.9289507269859314),\n",
              " ('grace', 0.9260685443878174),\n",
              " ('sin', 0.9168611168861389),\n",
              " ('disciple', 0.9151330590248108)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPOj0D8PDCBh",
        "colab_type": "text"
      },
      "source": [
        "**Step #12**: Examine document similarity with Doc2vec to any body of text of your choice\n",
        "\n",
        "*Reference*: https://radimrehurek.com/gensim/models/doc2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1QS5OoVEKjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrJvs1E0Gg8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine the first document in the list above to gauge the similarity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtiIzORkpTes",
        "colab_type": "text"
      },
      "source": [
        "**Stretch Goal: **\n",
        "\n",
        "Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "Load the pre-trained word vectors and train the **Word2vec** model\n",
        "\n",
        "Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "For example: \n",
        "\n",
        "model.similarity('house', 'bungalow')\n",
        "\n",
        "model.similarity('house', 'umbrella')\n"
      ]
    }
  ]
}