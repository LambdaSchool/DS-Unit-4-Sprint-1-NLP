{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 1, Module 4*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirchilet Allocation (LDA) Models (Prepare)\n",
    "<a id=\"#p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "LDA is a \"generative probabilistic model\". \n",
    "\n",
    "Let's play with a modoel available [here](https://lettier.com/projects/lda-topic-modeling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating LDA Models with Gensim (Learn)\n",
    "<a id=\"#p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "### A Litterary Introduction: *Jane Austen V. Charlotte Bronte*\n",
    "Despite being born nearly forty years apart, modern fans often pit Jane Austen & Charlotte Bronte against one another in a battle for litterary  supremacy. The battle centers around the topics of education for women, courting, and marriage. The authors' similiar backgrounds naturally draw comparisons, but the modern fascination is probably due to novelility of British women publishing novels during the early 19th century. \n",
    "\n",
    "Can we help close a litterary battle for supremacy and simply acknowledge that the authors addressed different topics and deserve to be acknowledged as excellent authors each in their own right?\n",
    "\n",
    "We're going to apply Latent Dirichlet Allocation a machine learning alogrithm for topic modeling to each of the author's novels to compare the distribution of topics in their novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import os\n",
    "import re\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Data\n",
    "I grabbed the novel data pre-split into a bunch of smaller chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/austen-brontÃ«-split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def gather_data(path_to_data): \n",
    "    data = []\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isdir(f) == False:\n",
    "            if f[-3:] == 'txt':\n",
    "                with open(os.path.join(path,f)) as t:\n",
    "                    text = t.read().strip('\\n')\n",
    "                    data.append(tokenize(str(text)))       \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = gather_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=titles, data={'tokens':tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = gather_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"this is a sample string with a \\n newline character\".replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "**Challenge**: update the function `tokenize` with any technique you have learned so far this week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [t[:-4] for t in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(STOPWORDS).union(set(['said', 'mr', 'mrs']))\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"Hello World! This a test of the tokenization method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=titles, data={'tokens':tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen_Emma0000</th>\n",
       "      <td>[emma, jane, austen, volume, chapter, emma, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_Emma0001</th>\n",
       "      <td>[taylor, wish, pity, weston, thought, agree, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_Emma0002</th>\n",
       "      <td>[behaved, charmingly, body, punctual, body, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_Emma0003</th>\n",
       "      <td>[native, highbury, born, respectable, family, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_Emma0004</th>\n",
       "      <td>[mention, handsome, letter, weston, received, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            tokens\n",
       "Austen_Emma0000  [emma, jane, austen, volume, chapter, emma, wo...\n",
       "Austen_Emma0001  [taylor, wish, pity, weston, thought, agree, p...\n",
       "Austen_Emma0002  [behaved, charmingly, body, punctual, body, be...\n",
       "Austen_Emma0003  [native, highbury, born, respectable, family, ...\n",
       "Austen_Emma0004  [mention, handsome, letter, weston, received, ..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df.reset_index()['index'].apply(lambda x: x.split('_')[0]).tolist()\n",
    "df['book'] = df.reset_index()['index'].apply(lambda x: x.split('_')[1][:-4]).tolist()\n",
    "df['section'] = df.reset_index()['index'].apply(lambda x: x[-4:]).tolist()\n",
    "df['section'] = df['section'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].map({'Austen':1, 'CBronte':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Documents\n",
    "Here we use a new pythonic thingy: the `yield` statement in our fucntion. This allows us to iterate over a bunch of documents without actually reading them into memory. You can see how we use this fucntion later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overkill function but it prevents overloading from gathering all that data in a list\n",
    "\n",
    "def doc_stream(path):\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isdir(f) == False:\n",
    "            if f[-3:] == 'txt':\n",
    "                with open(os.path.join(path,f)) as t:\n",
    "                    text = t.read().strip('\\n')\n",
    "                    tokens = tokenize(str(text))\n",
    "                yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call that stream-e-doo function on our path...\n",
    "\n",
    "streaming_data = doc_stream(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I call the streamer-thingy...\n",
    "\n",
    "streaming_data\n",
    "# It's saying \"yes, this is a streaming object.\", but can't really see it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But... if I use the next command:\n",
    "\n",
    "next(streaming_data)\n",
    "\n",
    "# It looks at the next document. Run it again, it looks at the next one, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim works WILL work with a generator (scikit learn will not)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dictionary Representation of all the words in our corpus\n",
    "\n",
    "# AKA \"Bag of Words\" -- (no ... you're a bag of words)\n",
    "\n",
    "id2word = corpora.Dictionary(doc_stream(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can look up the id of a word in the dic\n",
    "id2word.token2id['chip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other way around: look up word with given id\n",
    "\n",
    "id2word[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks up the word and frequency of each in the \"sentence\" given\n",
    "\n",
    "id2word.doc2bow(tokenize(\"This is a sample message Darcy England England England\"))\n",
    "\n",
    "# returns a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove extreme values from the dataset\n",
    "\n",
    "# This is a way of tuning/trimming the words we use\n",
    "\n",
    "id2word.filter_extremes(no_below=5, no_above=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the number of words left after trimming\n",
    "\n",
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bag of words(bow) representation of our corpus\n",
    "# Note: we haven't actually read any text into memory here\n",
    "\n",
    "# \n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in doc_stream(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (2, 1),\n",
       " (21, 2),\n",
       " (23, 3),\n",
       " (26, 1),\n",
       " (29, 2),\n",
       " (31, 2),\n",
       " (32, 1),\n",
       " (36, 1),\n",
       " (37, 1)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first ten words in the first document of our corpus\n",
    "\n",
    "corpus[4][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   random_state=723812,\n",
    "                   num_topics = 15,\n",
    "                   passes=10,\n",
    "                   workers=4\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oooooo \"topics\"\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegEx away all the extra silliness from the above output\n",
    "\n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda.print_topics()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [' '.join(t[0:5]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print-job of topics without all the silly extra stuff\n",
    "\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge \n",
    "\n",
    "You will apply an LDA model to a customer review dataset to practice the fitting and estimation of LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret LDA Results (Learn)\n",
    "<a id=\"#p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Distance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to become our visualizer \n",
    "\n",
    "pyLDAvis.gensim.prepare(lda, corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Model / Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distro = [lda[d] for d in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distro[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jon-Cody Special: takes the distrobutions and puts them in \"toodense\" format \n",
    "#                   (with bunch zeros...) See df.head() below\n",
    "\n",
    "distro = [lda[d] for d in corpus]\n",
    "\n",
    "def update(doc):\n",
    "        d_dist = {k:0 for k in range(0,15)}\n",
    "        for t in doc:\n",
    "            d_dist[t[0]] = t[1]\n",
    "        return d_dist\n",
    "    \n",
    "new_distro = [update(d) for d in distro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns jon_cody output into a pandas df\n",
    "\n",
    "df = pd.DataFrame.from_records(new_distro, index=titles)\n",
    "df.columns = topics\n",
    "df['author'] = df.reset_index()['index'].apply(lambda x: x.split('_')[0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very Nice!!! Check that shit out.\n",
    "\n",
    "df.groupby('author').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "### *Can we see if one of the authors focus more on men than women?*\n",
    "\n",
    "*  Use Spacy for text prepocessing\n",
    "*  Extract the Named Entities from the documents using Spacy (command is fairly straight forward)\n",
    "*  Create unique list of names from the authors (you'll find that there are different types of named entities not all people)\n",
    "*  Label the names with genders (can you this by hand or you use the US census name lists)\n",
    "*  Customize your processing to replace the proper name with your gender from the previous step's lookup table\n",
    "*  Then follow the rest of the LDA flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the Number of Topics (Learn)\n",
    "<a id=\"#p4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, path, limit, start=2, step=3, passes=5):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    path : path to input texts\n",
    "    limit : Max num of topics\n",
    "    passes: the number of times the entire lda model & coherence values are calculated\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    tokens = list(doc_stream(path))\n",
    "    \n",
    "    for iter_ in range(passes):\n",
    "        for num_topics in range(start, limit, step):\n",
    "            stream = doc_stream(path)\n",
    "            model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=4)\n",
    "            coherencemodel = CoherenceModel(model=model,dictionary=dictionary,corpus=corpus, coherence='u_mass')\n",
    "            coherence_values.append({'pass': iter_, \n",
    "                                     'num_topics': num_topics, \n",
    "                                     'coherence_score': coherencemodel.get_coherence()\n",
    "                                    })\n",
    "\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        path=path, \n",
    "                                                        start=2, \n",
    "                                                        limit=40, \n",
    "                                                        step=6,\n",
    "                                                        passes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_coherence = pd.DataFrame.from_records(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_coherence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.lineplot(x=\"num_topics\", y=\"coherence_score\", data=topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda[id2word.doc2bow(tokenize(\"This is a sample document to score with a topic distribution.\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "### *Can we see if one of the authors focus more on men than women?*\n",
    "\n",
    "*  Use Spacy for text prepocessing\n",
    "*  Extract the Named Entities from the documents using Spacy (command is fairly straight forward)\n",
    "*  Create unique list of names from the authors (you'll find that there are different types of named entities not all people)\n",
    "*  Label the names with genders (can you this by hand or you use the US census name lists)\n",
    "*  Customize your processing to replace the proper name with your gender from the previous step's lookup table\n",
    "*  Then follow the rest of the LDA flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Ned asked me a question about England today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_stream(path):\n",
    "    for f in os.listdir(path):\n",
    "        with open(os.path.join(path,f)) as t:\n",
    "            text = t.read().strip('\\n')\n",
    "            yield text\n",
    "\n",
    "def get_people(docstream):\n",
    "    \n",
    "    ppl = []\n",
    "    \n",
    "    for d in docstream:\n",
    "        \n",
    "        doc = nlp(d)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            \n",
    "            if ent.label_ == \"PERSON\":\n",
    "                ppl.append(ent.lemma_)\n",
    "                \n",
    "    return set(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = get_people(doc_stream(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(next(doc_stream(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "### *References*\n",
    "* [Andrew Ng et al paper on LDA](https://ai.stanford.edu/~ang/papers/jair03-lda.pdf)\n",
    "* On [Coherence](https://pdfs.semanticscholar.org/1521/8d9c029cbb903ae7c729b2c644c24994c201.pdf)\n",
    "\n",
    "### *Resources*\n",
    "\n",
    "* [Gensim](https://radimrehurek.com/gensim/): Python package for topic modeling, nlp, word vectorization, and few other things. Well maintained and well documented.\n",
    "* [Topic Modeling with Gensim](http://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#11createthedictionaryandcorpusneededfortopicmodeling): A kind of cookbook for LDA with gensim. Excellent overview, but the you need to be aware of missing import statements and assumed prior knowledge.\n",
    "* [Chinese Restuarant Process](https://en.wikipedia.org/wiki/Chinese_restaurant_process): That really obscure stats thing I mentioned... \n",
    "* [PyLDAvis](https://github.com/bmabey/pyLDAvis): Library for visualizing the topic model and performing some exploratory work. Works well. Has a direct parrell implementation in R as well. \n",
    "* [Rare Technologies](https://rare-technologies.com/): The people that made & maintain gensim and a few other libraries.\n",
    "* [Jane Austen v. Charlotte Bronte](https://www.literaryladiesguide.com/literary-musings/jane-austen-charlotte-bronte-different-alike/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
