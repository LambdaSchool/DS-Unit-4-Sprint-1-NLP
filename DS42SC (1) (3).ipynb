{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Sprint Challenge*\n",
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "**Part 1 - Working with Text Data**\n",
    "Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [],
   "source": [
    "no_white_space = whitespace_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a    string   that has  \\n a lot of  extra \\n   whitespace.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_white_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_to_go = (\" \".join(no_white_space.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has a lot of extra whitespace.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_to_go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as urler \n",
    "\n",
    "text_url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt'\n",
    "date_list = []\n",
    "for line in urler.urlopen(text_url):\n",
    "    date_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'March 8, 2015\\r\\n',\n",
       " b'March 15, 2015\\r\\n',\n",
       " b'March 22, 2015\\r\\n',\n",
       " b'March 29, 2015\\r\\n',\n",
       " b'April 5, 2015\\r\\n',\n",
       " b'April 12, 2015\\r\\n',\n",
       " b'April 19, 2015\\r\\n',\n",
       " b'April 26, 2015\\r\\n',\n",
       " b'May 3, 2015\\r\\n',\n",
       " b'May 10, 2015\\r\\n',\n",
       " b'May 17, 2015\\r\\n',\n",
       " b'May 24, 2015\\r\\n',\n",
       " b'May 31, 2015\\r\\n',\n",
       " b'June 7, 2015\\r\\n',\n",
       " b'June 14, 2015\\r\\n',\n",
       " b'June 21, 2015\\r\\n',\n",
       " b'June 28, 2015\\r\\n',\n",
       " b'July 5, 2015\\r\\n',\n",
       " b'July 12, 2015\\r\\n',\n",
       " b'July 19, 2015']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringlist=[date.decode('utf-8') for date in date_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March 8, 2015\\r\\n',\n",
       " 'March 15, 2015\\r\\n',\n",
       " 'March 22, 2015\\r\\n',\n",
       " 'March 29, 2015\\r\\n',\n",
       " 'April 5, 2015\\r\\n',\n",
       " 'April 12, 2015\\r\\n',\n",
       " 'April 19, 2015\\r\\n',\n",
       " 'April 26, 2015\\r\\n',\n",
       " 'May 3, 2015\\r\\n',\n",
       " 'May 10, 2015\\r\\n',\n",
       " 'May 17, 2015\\r\\n',\n",
       " 'May 24, 2015\\r\\n',\n",
       " 'May 31, 2015\\r\\n',\n",
       " 'June 7, 2015\\r\\n',\n",
       " 'June 14, 2015\\r\\n',\n",
       " 'June 21, 2015\\r\\n',\n",
       " 'June 28, 2015\\r\\n',\n",
       " 'July 5, 2015\\r\\n',\n",
       " 'July 12, 2015\\r\\n',\n",
       " 'July 19, 2015']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\n",
    "    \"(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|\"\n",
    "    \"Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|\"\n",
    "    \"Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_dates = []\n",
    "for date in stringlist:\n",
    "    cleaner_dates.append(pattern.search(date).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March 8, 2015',\n",
       " 'March 15, 2015',\n",
       " 'March 22, 2015',\n",
       " 'March 29, 2015',\n",
       " 'April 5, 2015',\n",
       " 'April 12, 2015',\n",
       " 'April 19, 2015',\n",
       " 'April 26, 2015',\n",
       " 'May 3, 2015',\n",
       " 'May 10, 2015',\n",
       " 'May 17, 2015',\n",
       " 'May 24, 2015',\n",
       " 'May 31, 2015',\n",
       " 'June 7, 2015',\n",
       " 'June 14, 2015',\n",
       " 'June 21, 2015',\n",
       " 'June 28, 2015',\n",
       " 'July 5, 2015',\n",
       " 'July 12, 2015',\n",
       " 'July 19, 2015']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = []\n",
    "for date in cleaner_dates:\n",
    "    months.append(re.findall('^[a-zA-Z]+', date)[0]) \n",
    "\n",
    "\n",
    "days = []\n",
    "for date in cleaner_dates:\n",
    "    days.append(re.findall('[0-9]{2}', date)[0])\n",
    "    \n",
    "    \n",
    "years = []\n",
    "for date in cleaner_dates:\n",
    "    years.append(re.findall('[0-9]{4}', date)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_data = {'Day': days,\n",
    "            'Month': months,\n",
    "            'Year': years}\n",
    "\n",
    "date_df = pd.DataFrame(date_data, columns=['Day', 'Month', 'Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Month  Year\n",
       "0   20  March  2015\n",
       "1   15  March  2015\n",
       "2   22  March  2015\n",
       "3   29  March  2015\n",
       "4   20  April  2015\n",
       "5   12  April  2015\n",
       "6   19  April  2015\n",
       "7   26  April  2015\n",
       "8   20    May  2015\n",
       "9   10    May  2015\n",
       "10  17    May  2015\n",
       "11  24    May  2015\n",
       "12  31    May  2015\n",
       "13  20   June  2015\n",
       "14  14   June  2015\n",
       "15  21   June  2015\n",
       "16  28   June  2015\n",
       "17  20   July  2015\n",
       "18  12   July  2015\n",
       "19  19   July  2015"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/william/anaconda3/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/william/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/william/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/william/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sad', 'apl', 'friend'], ['missed', 'new', 'moon', 'trailer'], ['omg', 'already', '730'], ['', 'omgaga', 'im', 'sooo', 'im', 'gunna', 'cry', 'ive', 'dentist', 'since', '11', 'suposed', '2', 'get', 'crown', 'put', '30mins'], ['think', 'mi', 'bf', 'cheating', 'tt'], ['worry', 'much'], ['juuuuuuuuuuuuuuuuussssst', 'chillin'], ['sunny', 'work', 'tomorrow', '', 'tv', 'tonight'], ['handed', 'uniform', 'today', '', 'miss', 'already'], ['hmmmm', 'wonder', 'number', ''], ['must', 'think', 'positive'], ['thanks', 'haters', 'face', 'day', '112102'], ['weekend', 'sucked', 'far'], ['jb', 'isnt', 'showing', 'australia'], ['ok', 'thats', 'win'], ['lt', 'way', 'feel', 'right'], ['awhhe', 'man', 'im', 'completely', 'useless', 'rt', 'funny', 'twitter', 'httpmylocme27hx'], ['feeling', 'strangely', 'fine', 'im', 'gonna', 'go', 'listen', 'semisonic', 'celebrate'], ['huge', 'roll', 'thunder', 'nowso', 'scary'], ['cut', 'beard', 'growing', 'well', 'year', 'im', 'gonna', 'start', 'shaunamanu', 'happy', 'meantime'], ['sad', 'iran'], ['wompppp', 'wompp'], ['youre', 'one', 'see', 'cause', 'one', 'else', 'following', 'youre', 'pretty', 'awesome'], ['ltsad', 'level', '3', 'writing', 'massive', 'blog', 'tweet', 'myspace', 'comp', 'shut', 'lost', 'lays', 'fetal', 'position'], ['', 'headed', 'hospitol', '', 'pull', 'golf', 'tourny', '3rd', 'place', 'think', 'reripped', 'something', '', 'yeah', ''], ['boring', '', 'whats', 'wrong', 'please', 'tell', ''], ['cant', 'bothered', 'wish', 'could', 'spend', 'rest', 'life', 'sat', 'going', 'gigs', 'seriously'], ['feeeling', 'like', 'shit', 'right', 'really', 'want', 'sleep', 'nooo', '3', 'hours', 'dancing', 'art', 'assignment', 'finish'], ['goodbye', 'exams', 'hello', 'alcohol', 'tonight'], ['didnt', 'realize', 'deep', 'geez', 'give', 'girl', 'warning', 'atleast'], ['hate', 'athlete', 'appears', 'tear', 'acl', 'live', 'television'], ['miss', 'guys', 'think', 'im', 'wearing', 'skinny', 'jeans', 'cute', 'sweater', 'heels', 'really', 'sure', 'today'], ['', 'meet', 'meat', 'httpbitly15ssci'], ['horsie', 'moving', 'saturday', 'morning'], ['sat', 'offneed', 'work', '6', 'days', 'week'], ['really', 'dont', 'like', 'room', 'boring', 'sick', 'wardrobe', 'cant', 'waiit', 'till', 'walk', 'one', 'yay'], ['sox', 'floyd', 'great', 'relievers', 'need', 'scolding'], ['times', 'like', 'million'], ['uploading', 'pictures', 'friendster'], ['type', 'spaz', 'downloads', 'virus', 'brother', 'thats', '', 'msn', 'fucked', 'forever', ''], ['ampampfightiin', 'wiit', 'babes'], ['', '', '', 'wrote', 'something', 'last', 'week', 'got', 'call', 'someone', 'new', 'york', 'office', 'httptumblrcomxcn21w6o7'], ['enough', 'said'], ['', 'need', 'even', 'say', 'well', 'go', 'anyways', 'chris', 'cornell', 'chicago', '', 'tonight']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_listings = []\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Translation tables are a cool trick to remove punctation from lists of strings\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stop_words)\n",
    "for tweet in tweet_df.SentimentText:\n",
    "  # split into words by whitespace\n",
    "  words = tweet.split()\n",
    "  # remove punctuation from words\n",
    "  stripped = [w.translate(table) for w in words]\n",
    "  # normalize case\n",
    "  lowercase = [x.lower() for x in stripped]\n",
    "  words = [w for w in lowercase if not w in stop_words] \n",
    "  tokenized_listings.append(words)\n",
    "print(tokenized_listings[0:44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tokens'] = tokenized_listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tweet_text):\n",
    "    return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\ / \\ / \\S+)\",\"\", tweet_text.lower()))\n",
    "\n",
    "tweet_df['clean_tweets'] = tweet_df['SentimentText'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[, omgaga, im, sooo, im, gunna, cry, ive, dent...</td>\n",
       "      <td>omgaga im sooo  im gunna cry ive be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>i think mi bf is cheating on me       tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "      <td>[worry, much]</td>\n",
       "      <td>or i just worry too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "      <td>juuuuuuuuuuuuuuuuussssst chillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "      <td>[sunny, work, tomorrow, , tv, tonight]</td>\n",
       "      <td>sunny again        work tomorrow       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "      <td>[handed, uniform, today, , miss, already]</td>\n",
       "      <td>handed in my uniform today  i miss you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "      <td>[hmmmm, wonder, number, ]</td>\n",
       "      <td>hmmmm i wonder how she my number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "5          0                  or i just worry too much?           \n",
       "6          1                 Juuuuuuuuuuuuuuuuussssst Chillin!!   \n",
       "7          0         Sunny Again        Work Tomorrow  :-|  ...   \n",
       "8          1        handed in my uniform today . i miss you ...   \n",
       "9          1           hmmmm.... i wonder how she my number @-)   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                 [sad, apl, friend]   \n",
       "1                       [missed, new, moon, trailer]   \n",
       "2                                [omg, already, 730]   \n",
       "3  [, omgaga, im, sooo, im, gunna, cry, ive, dent...   \n",
       "4                      [think, mi, bf, cheating, tt]   \n",
       "5                                      [worry, much]   \n",
       "6                [juuuuuuuuuuuuuuuuussssst, chillin]   \n",
       "7             [sunny, work, tomorrow, , tv, tonight]   \n",
       "8          [handed, uniform, today, , miss, already]   \n",
       "9                          [hmmmm, wonder, number, ]   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0                        is so sad for my apl friend  \n",
       "1                      i missed the new moon trailer  \n",
       "2                              omg its already 730 o  \n",
       "3             omgaga im sooo  im gunna cry ive be...  \n",
       "4           i think mi bf is cheating on me       tt  \n",
       "5                   or i just worry too much          \n",
       "6                   juuuuuuuuuuuuuuuuussssst chillin  \n",
       "7         sunny again        work tomorrow       ...  \n",
       "8        handed in my uniform today  i miss you a...  \n",
       "9                  hmmmm i wonder how she my number   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "#### Your Answer Here #####\n",
    "\n",
    "\n",
    "TF_IDF implies that words appearing more frequently in the document or text in question should be more important. \n",
    "\n",
    "The opposite relationship is true in the case of the corpus, as words appearing more frequently in the corpus are supposed to be less important. \n",
    "\n",
    "Term Frequency is multilpied by Inverse Document Frequency to get TF-IDF. \n",
    "\n",
    "Term Frequency is simply the number of times a term appears in the document, whereas IDF is the log of instances containing the word divided by the total number of instances. The IDF is intended to take into account words that may be mentioned frequency with respect to total document instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/94/05785b66fb679ca9c906bad8bf27f6e8ac00c3d41061aca5911dae997fd3/gensim-3.7.2-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/william/anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/ba/7eaf3c0dbe601c43d88e449dcd7b61d385fe07c0167163f63f58ece7c1b5/smart_open-1.8.3.tar.gz (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /home/william/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/william/anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: boto>=2.32 in /home/william/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /home/william/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/99/cef419ab955dde1c35d24c4c8dca3f76c72bc81605af6ca36394871fbaa8/boto3-1.9.141-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 14.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/william/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/william/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/william/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/william/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Collecting botocore<1.13.0,>=1.12.141 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/72/bb5092d4f8a7b6c9a4508b784cdfed6d856e2a202383c345a66da71cc612/botocore-1.12.141-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 3.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 9.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/william/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.141->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/william/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.141->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/william/.cache/pip/wheels/b8/cb/43/c0ba52baf2b0e371ec1d5b2d4685d6d24617b1391f3eeacda5\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.141 botocore-1.12.141 gensim-3.7.2 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.3\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "X = tweet_df.clean_tweets\n",
    "y = tweet_df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_count_vect = make_pipeline(CountVectorizer(stop_words=stop_words),\n",
    "                        LogisticRegression(solver='lbfgs',max_iter=1000))\n",
    "\n",
    "log_reg_gs_params = [{'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                  'countvectorizer__max_features' : [50, 100, None]}]\n",
    "\n",
    "log_reg_gs = GridSearchCV(log_reg_count_vect, log_reg_gs_params, cv=5, scoring='roc_auc')\n",
    "log_reg_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "\n",
      "\n",
      "Some solid parameters: {'countvectorizer__max_features': None, 'countvectorizer__ngram_range': (1, 3)}\n",
      "\n",
      "\n",
      "ROC_AUC of training set: 0.982096263138287\n",
      "\n",
      "\n",
      "ROC_AUC of test set: 0.7631125091301693\n"
     ]
    }
   ],
   "source": [
    "print ('RESULTS:')\n",
    "print ('\\n')\n",
    "print ('Some solid parameters:', log_reg_gs.best_params_)\n",
    "print ('\\n')\n",
    "print ('ROC_AUC of training set:', roc_auc_score(log_reg_gs.predict(X_train), y_train))\n",
    "print ('\\n')\n",
    "print ('ROC_AUC of test set:', roc_auc_score(log_reg_gs.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "word_2_vec_model = Word2Vec(tweet_df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.7998294830322266),\n",
       " ('list', 0.7823313474655151),\n",
       " ('following', 0.7658044695854187),\n",
       " ('page', 0.7645360827445984),\n",
       " ('link', 0.763768196105957),\n",
       " ('others', 0.7562284469604492),\n",
       " ('follow', 0.754357635974884),\n",
       " ('dm', 0.7535531520843506),\n",
       " ('email', 0.7526899576187134),\n",
       " ('sent', 0.7515875101089478)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_vec_model.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
