{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "#Eliminate all new lines and anything more than a single space\n",
    "less_whitespace = \" \".join(whitespace_string.split())\n",
    "print(less_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'March 8, 2015\\r\\nMarch 15, 2015\\r\\nMarch 22, 2015\\r\\nMarch 29, 2015\\r\\nApril 5, 2015\\r\\nApril 12, 2015\\r\\nApril 19, 2015\\r\\nApril 26, 2015\\r\\nMay 3, 2015\\r\\nMay 10, 2015\\r\\nMay 17, 2015\\r\\nMay 24, 2015\\r\\nMay 31, 2015\\r\\nJune 7, 2015\\r\\nJune 14, 2015\\r\\nJune 21, 2015\\r\\nJune 28, 2015\\r\\nJuly 5, 2015\\r\\nJuly 12, 2015\\r\\nJuly 19, 2015'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get text\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt'\n",
    "contents= requests.get(url)\n",
    "contents.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Create list of dates only\n",
    "lines = contents.text.split('\\r\\n')\n",
    "#Separate everything before first whitespace for month\n",
    "#Day and year will automatically separate because of comma\n",
    "month = []\n",
    "for x in range(0,len(lines)):\n",
    "    month.append(lines[x].split(' '))\n",
    "#Name columns and convert to dataframe\n",
    "cols = ['Month', 'Day', 'Year']\n",
    "df = pd.DataFrame(month, columns = cols)\n",
    "#Get rid of comma in Day column\n",
    "drop_comma = df['Day'].replace( '[,]','', regex=True )\n",
    "df['Day'] = drop_comma\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(99989, 2)\n",
      "Check NaN values:\n",
      "Sentiment        0\n",
      "SentimentText    0\n",
      "dtype: int64\n",
      "datatypes:\n",
      "Sentiment         int64\n",
      "SentimentText    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\"\n",
    "tweetdf = pd.read_csv(url)\n",
    "tweettxt = contents = requests.get(url).text\n",
    "print(f'Shape:\\n{tweetdf.shape}')\n",
    "print(f'Check NaN values:\\n{tweetdf.isnull().sum()}')\n",
    "print(f'datatypes:\\n{tweetdf.dtypes}')\n",
    "tweetdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rick1270/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rick1270/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment',\n",
       " 'sentimenttext',\n",
       " '0',\n",
       " 'sad',\n",
       " 'apl',\n",
       " 'friend',\n",
       " '0',\n",
       " 'missed',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'trailer',\n",
       " '1',\n",
       " 'omg',\n",
       " 'already',\n",
       " '730',\n",
       " '0',\n",
       " 'omgaga',\n",
       " 'im',\n",
       " 'sooo',\n",
       " 'im',\n",
       " 'gunna',\n",
       " 'cry',\n",
       " 'dentist',\n",
       " 'since',\n",
       " '11',\n",
       " 'suposed',\n",
       " '2',\n",
       " 'get',\n",
       " 'crown',\n",
       " 'put',\n",
       " '30mins',\n",
       " '0',\n",
       " 'think',\n",
       " 'mi',\n",
       " 'bf',\n",
       " 'cheating',\n",
       " 'tt',\n",
       " '0',\n",
       " 'worry',\n",
       " 'much',\n",
       " '1',\n",
       " 'juuuuuuuuuuuuuuuuussssst',\n",
       " 'chillin',\n",
       " '0',\n",
       " 'sunny',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'tv',\n",
       " 'tonight',\n",
       " '1',\n",
       " 'handed',\n",
       " 'uniform',\n",
       " 'today',\n",
       " 'miss',\n",
       " 'already',\n",
       " '1',\n",
       " 'hmmmm',\n",
       " 'wonder',\n",
       " 'number',\n",
       " '0',\n",
       " 'must',\n",
       " 'think',\n",
       " 'positive',\n",
       " '1',\n",
       " 'thanks',\n",
       " 'haters',\n",
       " 'face',\n",
       " 'day',\n",
       " '112102',\n",
       " '0',\n",
       " 'weekend',\n",
       " 'sucked',\n",
       " 'far',\n",
       " '0',\n",
       " 'jb',\n",
       " 'isnt',\n",
       " 'showing',\n",
       " 'australia',\n",
       " '0',\n",
       " 'ok',\n",
       " 'thats',\n",
       " 'win',\n",
       " '0',\n",
       " 'lt',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'right',\n",
       " '0',\n",
       " 'awhhe',\n",
       " 'man',\n",
       " 'completely',\n",
       " 'useless',\n",
       " 'rt',\n",
       " 'funny',\n",
       " 'twitter',\n",
       " 'http',\n",
       " 'mylocme27hx',\n",
       " '1',\n",
       " 'feeling',\n",
       " 'strangely',\n",
       " 'fine',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'go',\n",
       " 'listen',\n",
       " 'semisonic',\n",
       " 'celebrate',\n",
       " '0',\n",
       " 'huge',\n",
       " 'roll',\n",
       " 'thunder',\n",
       " 'scary',\n",
       " '0',\n",
       " 'cut',\n",
       " 'beard',\n",
       " 'growing',\n",
       " 'well',\n",
       " 'year',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'start',\n",
       " 'shaunamanu',\n",
       " 'happy',\n",
       " 'meantime',\n",
       " '0',\n",
       " 'sad',\n",
       " 'iran',\n",
       " '0',\n",
       " 'wompppp',\n",
       " 'wompp',\n",
       " '1',\n",
       " 'one',\n",
       " 'see',\n",
       " 'cause',\n",
       " 'one',\n",
       " 'else',\n",
       " 'following',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " '0',\n",
       " 'lt',\n",
       " 'sad',\n",
       " 'level',\n",
       " '3',\n",
       " 'writing',\n",
       " 'massive',\n",
       " 'blog',\n",
       " 'tweet',\n",
       " 'myspace',\n",
       " 'comp',\n",
       " 'shut',\n",
       " 'lost',\n",
       " 'lays',\n",
       " 'fetal',\n",
       " 'position',\n",
       " '0',\n",
       " 'headed',\n",
       " 'hospitol',\n",
       " 'pull',\n",
       " 'golf',\n",
       " 'tourny',\n",
       " '3rd',\n",
       " 'place',\n",
       " 'think',\n",
       " 'reripped',\n",
       " 'something',\n",
       " 'yeah',\n",
       " '0',\n",
       " 'boring',\n",
       " 'whats',\n",
       " 'wrong',\n",
       " 'please',\n",
       " 'tell',\n",
       " '0',\n",
       " 'ca',\n",
       " 'nt',\n",
       " 'bothered',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'rest',\n",
       " 'life',\n",
       " 'sat',\n",
       " 'going',\n",
       " 'gigs',\n",
       " 'seriously',\n",
       " '0',\n",
       " 'feeeling',\n",
       " 'like',\n",
       " 'shit',\n",
       " 'right',\n",
       " 'really',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'nooo',\n",
       " '3',\n",
       " 'hours',\n",
       " 'dancing',\n",
       " 'art',\n",
       " 'assignment',\n",
       " 'finish',\n",
       " '1',\n",
       " 'goodbye',\n",
       " 'exams',\n",
       " 'hello',\n",
       " 'alcohol',\n",
       " 'tonight',\n",
       " '0',\n",
       " 'nt',\n",
       " 'realize',\n",
       " 'deep',\n",
       " 'geez',\n",
       " 'give',\n",
       " 'girl',\n",
       " 'warning',\n",
       " 'atleast',\n",
       " '0',\n",
       " 'hate',\n",
       " 'athlete',\n",
       " 'appears',\n",
       " 'tear',\n",
       " 'acl',\n",
       " 'live',\n",
       " 'television',\n",
       " '0',\n",
       " 'miss',\n",
       " 'guys',\n",
       " 'think',\n",
       " 'wearing',\n",
       " 'skinny',\n",
       " 'jeans',\n",
       " 'cute',\n",
       " 'sweater',\n",
       " 'heels',\n",
       " 'really',\n",
       " 'sure',\n",
       " 'today',\n",
       " '0',\n",
       " 'meet',\n",
       " 'meat',\n",
       " 'http',\n",
       " 'bitly15ssci',\n",
       " '0',\n",
       " 'horsie',\n",
       " 'moving',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " '0',\n",
       " 'sat',\n",
       " 'need',\n",
       " 'work',\n",
       " '6',\n",
       " 'days',\n",
       " 'week',\n",
       " '0',\n",
       " 'really',\n",
       " 'dont',\n",
       " 'like',\n",
       " 'room',\n",
       " 'boring',\n",
       " 'sick',\n",
       " 'wardrobe',\n",
       " 'cant',\n",
       " 'waiit',\n",
       " 'till',\n",
       " 'walk',\n",
       " 'one',\n",
       " 'yay',\n",
       " '0',\n",
       " 'sox',\n",
       " 'floyd',\n",
       " 'great',\n",
       " 'relievers',\n",
       " 'need',\n",
       " 'scolding',\n",
       " '0',\n",
       " 'times',\n",
       " 'like',\n",
       " 'million',\n",
       " '1',\n",
       " 'uploading',\n",
       " 'pictures',\n",
       " 'friendster',\n",
       " '0',\n",
       " 'type',\n",
       " 'spaz',\n",
       " 'downloads',\n",
       " 'virus',\n",
       " 'brother',\n",
       " 'msn',\n",
       " 'fucked',\n",
       " 'forever',\n",
       " '0',\n",
       " 'amp',\n",
       " 'amp',\n",
       " 'fightiin',\n",
       " 'wiit',\n",
       " 'babes',\n",
       " '1',\n",
       " 'wrote',\n",
       " 'something',\n",
       " 'last',\n",
       " 'week',\n",
       " 'got',\n",
       " 'call',\n",
       " 'someone',\n",
       " 'new',\n",
       " 'york',\n",
       " 'office',\n",
       " 'http',\n",
       " 'tumblrcomxcn21w6o7',\n",
       " '0',\n",
       " 'enough',\n",
       " 'said',\n",
       " '1',\n",
       " 'need',\n",
       " 'even',\n",
       " 'say',\n",
       " 'well',\n",
       " 'go',\n",
       " 'anyways',\n",
       " 'chris',\n",
       " 'cornell',\n",
       " 'chicago',\n",
       " 'tonight',\n",
       " '1',\n",
       " 'health',\n",
       " 'class',\n",
       " 'joke',\n",
       " '1',\n",
       " 'ginaaa',\n",
       " 'lt',\n",
       " '3',\n",
       " 'go',\n",
       " 'show',\n",
       " 'tonight',\n",
       " '0',\n",
       " 'spiralgalaxy',\n",
       " 'ymptweet',\n",
       " 'really',\n",
       " 'makes',\n",
       " 'sad',\n",
       " 'look',\n",
       " 'muslims',\n",
       " 'reality',\n",
       " '0',\n",
       " 'time',\n",
       " 'low',\n",
       " 'shall',\n",
       " 'motivation',\n",
       " 'rest',\n",
       " 'week',\n",
       " '0',\n",
       " 'entertainment',\n",
       " 'someone',\n",
       " 'complained',\n",
       " 'properly',\n",
       " 'rupturerapture',\n",
       " 'experimental',\n",
       " 'say',\n",
       " 'experiment',\n",
       " 'melody',\n",
       " '0',\n",
       " 'another',\n",
       " 'year',\n",
       " 'lakers',\n",
       " 'neither',\n",
       " 'magic',\n",
       " 'fun',\n",
       " '0',\n",
       " 'baddest',\n",
       " 'day',\n",
       " 'eveer',\n",
       " '1',\n",
       " 'bathroom',\n",
       " 'clean',\n",
       " 'enjoyable',\n",
       " 'tasks',\n",
       " '1',\n",
       " 'boom',\n",
       " 'boom',\n",
       " 'pow',\n",
       " '0',\n",
       " 'proud',\n",
       " '0',\n",
       " 'congrats',\n",
       " 'helio',\n",
       " 'though',\n",
       " '0',\n",
       " 'david',\n",
       " 'must',\n",
       " 'hospitalized',\n",
       " 'five',\n",
       " 'days',\n",
       " 'end',\n",
       " 'july',\n",
       " 'palatine',\n",
       " 'tonsils',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'see',\n",
       " 'katie',\n",
       " 'concert',\n",
       " '0',\n",
       " 'friends',\n",
       " 'leaving',\n",
       " 'cause',\n",
       " 'stupid',\n",
       " 'love',\n",
       " 'http',\n",
       " 'bitlyzoxzc',\n",
       " '1',\n",
       " 'go',\n",
       " 'give',\n",
       " 'ur',\n",
       " 'mom',\n",
       " 'hug',\n",
       " 'right',\n",
       " 'http',\n",
       " 'bitlyazfwv',\n",
       " '1',\n",
       " 'going',\n",
       " 'see',\n",
       " 'harry',\n",
       " 'sunday',\n",
       " 'happiness',\n",
       " '0',\n",
       " 'hand',\n",
       " 'quilting',\n",
       " '0',\n",
       " 'hate',\n",
       " 'u',\n",
       " 'leysh',\n",
       " 't9ar5',\n",
       " '1',\n",
       " 'always',\n",
       " 'get',\n",
       " 'want',\n",
       " '1',\n",
       " 'bend',\n",
       " 'backwards',\n",
       " '1',\n",
       " 'get',\n",
       " 'work',\n",
       " 'sooooon',\n",
       " 'miss',\n",
       " 'cody',\n",
       " 'booo',\n",
       " 'nt',\n",
       " 'seen',\n",
       " 'foreverr',\n",
       " '1',\n",
       " 'hate',\n",
       " 'allergies',\n",
       " 'get',\n",
       " 'hair',\n",
       " 'cut',\n",
       " 'tomorrow',\n",
       " 'taking',\n",
       " 'public',\n",
       " 'poll',\n",
       " '0',\n",
       " 'love',\n",
       " 'guys',\n",
       " 'much',\n",
       " 'hurts',\n",
       " 'http',\n",
       " 'tumblrcomxkh1z19us',\n",
       " '0',\n",
       " 'miss',\n",
       " 'earl',\n",
       " '0',\n",
       " 'miss',\n",
       " 'new',\n",
       " 'jersey',\n",
       " '0',\n",
       " 'missed',\n",
       " 'first',\n",
       " 'hour',\n",
       " 'sytycd',\n",
       " 'last',\n",
       " 'night',\n",
       " 'ca',\n",
       " 'nt',\n",
       " 'find',\n",
       " 'online',\n",
       " '0',\n",
       " 'need',\n",
       " 'u2',\n",
       " 'fix',\n",
       " '0',\n",
       " 'never',\n",
       " 'thought',\n",
       " 'become',\n",
       " 'second',\n",
       " 'choice',\n",
       " '0',\n",
       " 'think',\n",
       " 'may',\n",
       " 'friendly',\n",
       " 'lol',\n",
       " 'well',\n",
       " '0',\n",
       " 'think',\n",
       " 'manuel',\n",
       " 'basil',\n",
       " 'plant',\n",
       " 'days',\n",
       " 'live',\n",
       " '0',\n",
       " 'wan',\n",
       " 'na',\n",
       " 'home',\n",
       " 'church',\n",
       " 'wonder',\n",
       " 'wht',\n",
       " '0',\n",
       " 'wan',\n",
       " 'na',\n",
       " 'make',\n",
       " 'pizza',\n",
       " '0',\n",
       " 'want',\n",
       " '120gb',\n",
       " 'harddrive',\n",
       " '37',\n",
       " 'inch',\n",
       " 'tv',\n",
       " 'new',\n",
       " 'guitar',\n",
       " 'anyonefeeling',\n",
       " 'generous',\n",
       " 'p',\n",
       " 'x',\n",
       " '0',\n",
       " 'want',\n",
       " 'hug',\n",
       " '0',\n",
       " 'want',\n",
       " 'miley',\n",
       " 'tour',\n",
       " 'australia',\n",
       " '0',\n",
       " 'wanted',\n",
       " 'sleep',\n",
       " 'morning',\n",
       " 'mean',\n",
       " 'kid',\n",
       " 'popsicle',\n",
       " 'stick',\n",
       " 'head',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'fly',\n",
       " 'away',\n",
       " 'like',\n",
       " 'squirrels',\n",
       " '0',\n",
       " 'slow',\n",
       " 'get',\n",
       " '1',\n",
       " 'tix',\n",
       " '0',\n",
       " 'send',\n",
       " 'sunshine',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'going',\n",
       " 'swimming',\n",
       " 'today',\n",
       " 'kezbat',\n",
       " '0',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'go',\n",
       " 't4',\n",
       " 'beach',\n",
       " 'would',\n",
       " 'great',\n",
       " 'see',\n",
       " 'shontellelayne',\n",
       " 'amp',\n",
       " 'danmerriweather',\n",
       " '0',\n",
       " 'would',\n",
       " 'much',\n",
       " 'happier',\n",
       " 'walls',\n",
       " 'bedroom',\n",
       " 'painted',\n",
       " 'white',\n",
       " '0',\n",
       " 'idk',\n",
       " 'wat',\n",
       " '2',\n",
       " 'trust',\n",
       " 'im',\n",
       " 'sorry',\n",
       " '4',\n",
       " 'da',\n",
       " 'pain',\n",
       " 'caused',\n",
       " 'nebody',\n",
       " 'ima',\n",
       " 'take',\n",
       " 'dis',\n",
       " 'time',\n",
       " '2',\n",
       " 'straighten',\n",
       " 'luv',\n",
       " 'yall',\n",
       " '0',\n",
       " 'finding',\n",
       " 'intercept',\n",
       " 'slopeand',\n",
       " 'banging',\n",
       " 'head',\n",
       " 'wallmath',\n",
       " 'brain',\n",
       " 'heads',\n",
       " 'come',\n",
       " 'save',\n",
       " '1',\n",
       " 'really',\n",
       " 'going',\n",
       " 'bed',\n",
       " '0',\n",
       " 'im',\n",
       " 'sick',\n",
       " 'cough',\n",
       " 'cough',\n",
       " '0',\n",
       " 'cab',\n",
       " 'headed',\n",
       " 'airport',\n",
       " 'going',\n",
       " 'home',\n",
       " 'lt',\n",
       " 'christy',\n",
       " 'gt',\n",
       " '0',\n",
       " 'case',\n",
       " 'feel',\n",
       " 'emo',\n",
       " 'camp',\n",
       " 'feeling',\n",
       " 'wee',\n",
       " 'bit',\n",
       " 'alr',\n",
       " 'bringing',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'watch',\n",
       " 'world',\n",
       " 'report',\n",
       " '2009hope',\n",
       " 'work',\n",
       " '1',\n",
       " 'jin',\n",
       " 'twitter',\n",
       " '0',\n",
       " 'jonas',\n",
       " 'day',\n",
       " 'almost',\n",
       " '0',\n",
       " 'jus',\n",
       " 'got',\n",
       " 'hom',\n",
       " 'fr',\n",
       " 'tda',\n",
       " 'funeral',\n",
       " 'sad',\n",
       " 'cried',\n",
       " 'much',\n",
       " 'times',\n",
       " 'much',\n",
       " 'love',\n",
       " 'grandpa',\n",
       " 'lt',\n",
       " '3',\n",
       " 'never',\n",
       " 'got',\n",
       " 'say',\n",
       " 'last',\n",
       " 'quot',\n",
       " 'goodbye',\n",
       " 'quot',\n",
       " '1',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'smile',\n",
       " 'cuz',\n",
       " 'isand',\n",
       " 'im',\n",
       " 'sure',\n",
       " 'could',\n",
       " 'want',\n",
       " '1',\n",
       " 'got',\n",
       " 'home',\n",
       " 'got',\n",
       " 'see',\n",
       " 'friend',\n",
       " 'zahra',\n",
       " 'nt',\n",
       " 'seen',\n",
       " 'since',\n",
       " 'graduated',\n",
       " 'makes',\n",
       " 'happy',\n",
       " '0',\n",
       " 'longest',\n",
       " 'night',\n",
       " 'ever',\n",
       " 'ugh',\n",
       " 'http',\n",
       " 'tumblrcomxwp1yxhi6',\n",
       " '0',\n",
       " 'mi',\n",
       " 'momacita',\n",
       " 'wo',\n",
       " 'nt',\n",
       " 'let',\n",
       " 'go',\n",
       " 'bf',\n",
       " 'bball',\n",
       " 'game',\n",
       " 'grrr',\n",
       " '0',\n",
       " 'mom',\n",
       " 'says',\n",
       " 'get',\n",
       " 'new',\n",
       " 'phone',\n",
       " 'immediately',\n",
       " 'tmobile',\n",
       " 'paying',\n",
       " '0',\n",
       " 'new',\n",
       " 'car',\n",
       " 'stolen',\n",
       " 'mother',\n",
       " 'wanted',\n",
       " 'go',\n",
       " 'pose',\n",
       " 'church',\n",
       " '0',\n",
       " 'hang',\n",
       " 'girls',\n",
       " '2day',\n",
       " '2moro',\n",
       " 'hope',\n",
       " '0',\n",
       " 'movie',\n",
       " 'times',\n",
       " 'sunday',\n",
       " 'rats',\n",
       " 'plan',\n",
       " 'tomorrow',\n",
       " 'guess',\n",
       " 'means',\n",
       " 'work',\n",
       " 'two',\n",
       " 'presentations',\n",
       " '0',\n",
       " 'pavel',\n",
       " 'tonight',\n",
       " 'lt',\n",
       " 'tigersfan',\n",
       " 'gt',\n",
       " '0',\n",
       " 'cool',\n",
       " 'night',\n",
       " '1',\n",
       " 'oh',\n",
       " 'thank',\n",
       " '1',\n",
       " 'pleased',\n",
       " '0',\n",
       " 'probably',\n",
       " 'guna',\n",
       " 'get',\n",
       " 'soon',\n",
       " 'since',\n",
       " 'one',\n",
       " 'talkin',\n",
       " '0',\n",
       " 'really',\n",
       " 'wanted',\n",
       " 'safina',\n",
       " 'pull',\n",
       " 'win',\n",
       " 'amp',\n",
       " 'lose',\n",
       " 'like',\n",
       " '0',\n",
       " 'rip',\n",
       " 'david',\n",
       " 'eddings',\n",
       " '1',\n",
       " 'rose',\n",
       " 'ood',\n",
       " 'back',\n",
       " 'xmas',\n",
       " 'special',\n",
       " 'yay',\n",
       " 'damn',\n",
       " 'half',\n",
       " 'year',\n",
       " 'away',\n",
       " '0',\n",
       " 'sannesias',\n",
       " 'aww',\n",
       " 'neighbor',\n",
       " 'need',\n",
       " 'watch',\n",
       " 'mean',\n",
       " 'girls',\n",
       " 'http',\n",
       " 'tumblrcomxv31s2pi8',\n",
       " '0',\n",
       " 'whats',\n",
       " 'status',\n",
       " 'next',\n",
       " 'weekend',\n",
       " '0',\n",
       " 'sorry',\n",
       " 'gigi4462',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'overdosed',\n",
       " 'daily',\n",
       " 'dose',\n",
       " 'haterade',\n",
       " '0',\n",
       " 'thanks',\n",
       " 'definition',\n",
       " 'throwbie',\n",
       " 'editors',\n",
       " 'reviewed',\n",
       " 'entry',\n",
       " 'decided',\n",
       " 'publish',\n",
       " '1',\n",
       " 'thanks',\n",
       " 'need',\n",
       " 'help',\n",
       " 'get',\n",
       " '1',\n",
       " 'explains',\n",
       " 'alot',\n",
       " '1',\n",
       " 'going',\n",
       " 'heathers',\n",
       " 'sequel',\n",
       " 'winona4ever',\n",
       " 'better',\n",
       " 'fuck',\n",
       " '0',\n",
       " 'told',\n",
       " 'drink',\n",
       " 'wmy',\n",
       " 'rx',\n",
       " 'gave',\n",
       " 'bb',\n",
       " 'porter',\n",
       " 'amp',\n",
       " 'obs',\n",
       " 'stout',\n",
       " 'brought',\n",
       " 'home',\n",
       " 'neighbors',\n",
       " 'v',\n",
       " 'v',\n",
       " 'sad',\n",
       " '1',\n",
       " 'trae',\n",
       " 'sweet',\n",
       " 'bought',\n",
       " 'new',\n",
       " 'baithing',\n",
       " 'suit',\n",
       " 'wove',\n",
       " '1',\n",
       " 'true',\n",
       " 'highly',\n",
       " 'subjective',\n",
       " 'tombre',\n",
       " 'actually',\n",
       " 'favorite',\n",
       " 'character',\n",
       " 'book',\n",
       " 'got',\n",
       " 'http',\n",
       " 'isgd13be0',\n",
       " 'rishabh',\n",
       " '0',\n",
       " 'u',\n",
       " 'guys',\n",
       " 'knw',\n",
       " 'whyy',\n",
       " '0',\n",
       " 'much',\n",
       " '0',\n",
       " 'waahhh',\n",
       " 'getting',\n",
       " 'sad',\n",
       " 'miss',\n",
       " 'hub',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " 'quot',\n",
       " '0',\n",
       " 'standing',\n",
       " 'pouring',\n",
       " 'rain',\n",
       " 'sitting',\n",
       " 'top',\n",
       " 'world',\n",
       " 'summer',\n",
       " 'sucks',\n",
       " '0',\n",
       " 'went',\n",
       " 'get',\n",
       " 'dog',\n",
       " 'vets',\n",
       " 'theyve',\n",
       " 'stitched',\n",
       " 'ear',\n",
       " 'charged',\n",
       " 'us',\n",
       " 'still',\n",
       " 'bleeds',\n",
       " 'like',\n",
       " 'waterfall',\n",
       " 'everytime',\n",
       " 'moves',\n",
       " '0',\n",
       " 'picture',\n",
       " 'feel',\n",
       " 'naked',\n",
       " '0',\n",
       " 'nt',\n",
       " 'trains',\n",
       " 'going',\n",
       " 'manchester',\n",
       " 'sunday',\n",
       " 'ca',\n",
       " 'nt',\n",
       " 'go',\n",
       " 'jonasbrothers',\n",
       " 'concert',\n",
       " '0',\n",
       " 'must',\n",
       " 'people',\n",
       " 'picky',\n",
       " 'mean',\n",
       " '6',\n",
       " 'hours',\n",
       " 'work',\n",
       " 'dice',\n",
       " 'whatï¿½s',\n",
       " 'http',\n",
       " 'tumblrcomxsg1m3ufn',\n",
       " '0',\n",
       " 'twitter',\n",
       " 'soon',\n",
       " 'become',\n",
       " 'obsolete',\n",
       " 'http',\n",
       " 'wwwimediaconnectioncomcontent23465asp',\n",
       " '1',\n",
       " 'wide',\n",
       " 'awake',\n",
       " '1',\n",
       " 'yeah',\n",
       " 'awful',\n",
       " 'weather',\n",
       " 'last',\n",
       " 'week',\n",
       " 'stinks',\n",
       " 'kristis',\n",
       " 'comin',\n",
       " 'moe',\n",
       " 'said',\n",
       " 'mmmaybe',\n",
       " 'take',\n",
       " 'us',\n",
       " '0',\n",
       " 'hi',\n",
       " 'stranger',\n",
       " 'hello',\n",
       " 'frank',\n",
       " 'lampard',\n",
       " 'fine',\n",
       " ...]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = str.maketrans('','', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nltk_tokenize(input):\n",
    "  # Tokenize by word\n",
    "  tokens = word_tokenize(input)\n",
    "  # Make all words lowercase\n",
    "  lowercase_tokens = [w.lower() for w in tokens]\n",
    "  # Strip punctuation from within words\n",
    "  no_punctuation = [x.translate(table) for x in lowercase_tokens]\n",
    "  # Remove stopwords\n",
    "  no_stopwords = [w for w in no_punctuation if not w in stop_words]\n",
    "  # Remove empty strings\n",
    "  words = [w for w in no_stopwords if len(w) > 0]\n",
    "  return words\n",
    "nltk_tokenize(tweettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentText_Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                             SentimentText_Tokenized  \n",
       "0                                 [sad, apl, friend]  \n",
       "1                       [missed, new, moon, trailer]  \n",
       "2                                [omg, already, 730]  \n",
       "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...  \n",
       "4                      [think, mi, bf, cheating, tt]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk_tokenize sentimentText\n",
    "tweetdf['SentimentText_Tokenized'] = tweetdf['SentimentText'].apply(nltk_tokenize)\n",
    "tweetdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "Put simply, the higher the TF*IDF score (weight), the rarer the term and vice versa.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991,)\n",
      "(19998,)\n",
      "(79991,)\n",
      "(19998,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweetdf.SentimentText\n",
    "y = tweetdf.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=300, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>ah</th>\n",
       "      <th>amazing</th>\n",
       "      <th>amp</th>\n",
       "      <th>anymore</th>\n",
       "      <th>...</th>\n",
       "      <th>xx</th>\n",
       "      <th>ya</th>\n",
       "      <th>yay</th>\n",
       "      <th>yea</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  able  actually  add  ago  agree  ah  amazing  amp  anymore    ...      \\\n",
       "0   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "1   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "2   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "3   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "4   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "\n",
       "   xx  ya  yay  yea  yeah  year  years  yep  yes  yesterday  \n",
       "0   0   0    0    0     0     0      0    0    0          0  \n",
       "1   0   0    0    0     0     0      0    0    0          0  \n",
       "2   0   0    0    0     0     0      0    0    0          0  \n",
       "3   0   0    0    0     0     0      0    0    0          0  \n",
       "4   0   0    0    0     0     0      0    0    0          0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>ah</th>\n",
       "      <th>amazing</th>\n",
       "      <th>amp</th>\n",
       "      <th>anymore</th>\n",
       "      <th>...</th>\n",
       "      <th>xx</th>\n",
       "      <th>ya</th>\n",
       "      <th>yay</th>\n",
       "      <th>yea</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  able  actually  add  ago  agree  ah  amazing  amp  anymore    ...      \\\n",
       "0   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "1   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "2   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "3   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "4   0     0         0    0    0      0   0        0    0        0    ...       \n",
       "\n",
       "   xx  ya  yay  yea  yeah  year  years  yep  yes  yesterday  \n",
       "0   0   0    0    0     0     0      0    0    0          0  \n",
       "1   0   1    0    0     0     0      0    0    0          0  \n",
       "2   0   0    0    0     0     0      0    0    0          0  \n",
       "3   0   0    0    0     0     0      0    0    0          0  \n",
       "4   0   0    0    0     0     0      0    0    0          0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.701391406533235\n",
      "Test Accuracy: 0.6985698569856986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression(solver='lbfgs', random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3730"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(tweetdf.SentimentText_Tokenized, min_count=20, window=3, \n",
    "               size=300, negative=20)\n",
    "words = list(w2v.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('account', 0.7438366413116455),\n",
       " ('email', 0.7323489189147949),\n",
       " ('updates', 0.7306891679763794),\n",
       " ('info', 0.7264102697372437),\n",
       " ('facebook', 0.7244384288787842),\n",
       " ('page', 0.7240075469017029),\n",
       " ('fb', 0.7183884382247925),\n",
       " ('message', 0.715042233467102),\n",
       " ('myspace', 0.7150341272354126),\n",
       " ('dm', 0.7144572734832764)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('twitter', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
