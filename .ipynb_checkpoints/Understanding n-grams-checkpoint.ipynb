{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Generating-text-from-probability\" data-toc-modified-id=\"Generating-text-from-probability-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Generating text from probability</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-n-grams\" data-toc-modified-id=\"Understanding-n-grams-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Understanding n-grams</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-do-we-predict-words?\" data-toc-modified-id=\"How-do-we-predict-words?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>How do we predict words?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-notation\" data-toc-modified-id=\"Understanding-notation-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Understanding notation</a></span></li></ul></li><li><span><a href=\"#Predicting-joint-probablity\" data-toc-modified-id=\"Predicting-joint-probablity-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Predicting joint probablity</a></span></li><li><span><a href=\"#Bi-gram-Example\" data-toc-modified-id=\"Bi-gram-Example-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Bi-gram Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-bit-of-pre-processing\" data-toc-modified-id=\"A-bit-of-pre-processing-1.1.3.1\"><span class=\"toc-item-num\">1.1.3.1&nbsp;&nbsp;</span>A bit of pre-processing</a></span></li><li><span><a href=\"#Normalize-the-tokens\" data-toc-modified-id=\"Normalize-the-tokens-1.1.3.2\"><span class=\"toc-item-num\">1.1.3.2&nbsp;&nbsp;</span>Normalize the tokens</a></span></li><li><span><a href=\"#bi-gram-distribution-counts\" data-toc-modified-id=\"bi-gram-distribution-counts-1.1.3.3\"><span class=\"toc-item-num\">1.1.3.3&nbsp;&nbsp;</span>bi-gram distribution counts</a></span></li><li><span><a href=\"#bi-gram-probability\" data-toc-modified-id=\"bi-gram-probability-1.1.3.4\"><span class=\"toc-item-num\">1.1.3.4&nbsp;&nbsp;</span>bi-gram probability</a></span></li></ul></li><li><span><a href=\"#Generating-text\" data-toc-modified-id=\"Generating-text-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Generating text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Try-again-with-tokenized-sentences\" data-toc-modified-id=\"Try-again-with-tokenized-sentences-1.1.4.1\"><span class=\"toc-item-num\">1.1.4.1&nbsp;&nbsp;</span>Try again with tokenized sentences</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text from probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding n-grams\n",
    "A n-gram is a predctive word model, where n represents the count of word sequences.   \n",
    "    1. For example a 2-gram would be a bigram like \"I ate\"\n",
    "    2. A 3-gram would be a trigram, like \"I ate chicken\"\n",
    "\n",
    "    A. We use n-gram models to estimate the probability of the last word of an n-gram \n",
    "    given the previous words. \n",
    "    B. We also assign probability to the entire sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we predict words?\n",
    "   ![image](https://static.independent.co.uk/s3fs-public/thumbnails/image/2017/05/14/13/brain.jpg?w968h681)\n",
    "    \n",
    "    1. First we take P(w|h). Or in English the probability of a word given some history\n",
    "        A. We can think of history as a string, or document, or collection of documents.\n",
    "           It is merely a reference to our current \"working space\".\n",
    "           \n",
    "        B. Example: Let's say we have a string \"its steam is so hot that\"and \n",
    "        we find ourselves wanting to predict the proability that the next word would be \n",
    "        \"the\".\n",
    "           \n",
    "           We would set up on equation like so \n",
    "           P(the|its steam is so hot that)\n",
    "               \n",
    "               B.2. There are a few ways we could try to predict this. \n",
    "                   \n",
    "                   A. One way would be to predict relative frequency counts. \n",
    "                   We would take a very large collection of documents ( a corpus ).\n",
    "                   For example all written text on the world wide web, \n",
    "                   take the count of times we see a match for our string s, \n",
    "                   and a count of how many times it's followed by the word \"the\". \n",
    "                   \n",
    "                       A.2. Our probability now looks like this \n",
    "                          P(the|its steam is so hot that) =\n",
    "                          \n",
    "                            C(its steam is so hot that)\n",
    "                            ---------------------------\n",
    "                            C(its steam is so hot that the)\n",
    "                   \n",
    "                   B. Like the first problem, if we wanted to know the combined\n",
    "                   probability of the entire sentence we could answer such a question by \n",
    "                   asking out of all possible sequences of five words, how many of them\n",
    "                   are \"its steam is so hot\"\n",
    "                   \n",
    "               B.3 The problem with both of these methods is that they are \n",
    "               \n",
    "                   1. Likely to be extremely ineffecient\n",
    "\n",
    "                   2. Likely to have an incredibly hard time finding matching cases, \n",
    "                   even amongst enourmous sources of data. \n",
    "\n",
    "                   3. Language is one of the most creative and inventive things\n",
    "                   humans have, and new sentences are made daily. Such methods cannot \n",
    "                   hope to keep up with an ever evolving corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding notation\n",
    "Let the following represent our values in our system of predictions\n",
    "\n",
    "  > N = a sequence of words | w<sub>1</sub>, w<sub>2</sub> ... w<sub>n</sub> | w<sup>n - 1</sup><sub>1</sub> = w<sub>1</sub>, w<sub>2</sub> ... w<sub>n - 1</sub>  \n",
    "  w = word  \n",
    "  W = the entire word sequence  \n",
    "  h = history  \n",
    "  joint probablity = P(X = w<sub>1</sub>, Y = w<sub>2</sub>, ..., W = w<sub>n</sub> | P(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting joint probablity\n",
    "1. One method of predicting joint probability for word sequences would be to decompose their probablity by using the chain rule of probability.  https://en.wikipedia.org/wiki/Chain_rule_(probability)\n",
    "\n",
    "    * If we apply the chain rule to our words, we get something that looks like this  \n",
    "    > P(w<sup>n</sup><sub>1</sub>) = P(w<sub>1</sub>)P(w<sub>2</sub>|w<sub>1</sub>)P(w<sub>3</sub>|w<sup>2</sup><sub>1</sub>)\n",
    "    \n",
    "    * What this essentially tells us is the formula for computing the combined probability of a word sequence and computing the conditional probablity of a specific word if we are given previous words.  \n",
    "    \n",
    "        A. There is a problem here though. We are back to the previous problem of not being able to predict the probablity of a given word, as it could quite literally be a brand new sentence.\n",
    "    \n",
    "        * This is where the n-gram comes in to save us. \n",
    "            1. Instead of trying to compute the probablity of a word given it's entire history, we can approximate the history by using only the last few words.\n",
    "\n",
    "            2. For example, if we used a bigram model, we approximate that P(w<sub>n</sub>|w<sub>n-1</sub>)\n",
    "            Or in our case we have the string we used earlier. \n",
    "\n",
    "            P(the|its steam is so hot that)  \n",
    "            This now becomes P(the|that)\n",
    "\n",
    "            We change our assumption so that the probability of a word depends only on the previous word/words given by the size of our specified n-gram. \n",
    "                1. This type of assumption is called a Markov Assumption. \n",
    "                Markov models are(from a birds eye view) a type of probablistic model that assumes we can predict the future without looking into the past. If you'd like a better overview, please give my blog a read. https://towardsdatascience.com/understanding-markov-decision-processes-b5862c192ddb\n",
    "  \n",
    "-------------------------------------------------------------------------------------------------------\n",
    " > By now you're probably asking, wait a second, how do we estimate an n-grams probabilities though?\n",
    "We can use a metric called the maximum likelihood estimation, or MLE. We get this metric by taking the counts of words from a corpus, and normalizing these counts so that they all fall between 0 and 1.   \n",
    "       We can decribe this like so:  \n",
    "       P(w<sub>n</sub>|w<sub>n−1</sub>) = C(w<sub>n−1</sub>w<sub>n</sub>) / C(w<sub>n−1</sub>)  \n",
    "       This can be applied to every single word in a corpus\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram Example \n",
    "Using 2004 Blogger data from a massive collection of text parsed from xml files  \n",
    "Let's explore the word \"japanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from jupyterthemes import jtplot\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag, bigrams, FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t onedork -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T\n",
    "# currently installed theme will be used to\n",
    "# set plot style if no arguments provided\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# open and parse a single xml file, stripping it's tag metadata and returning only textual elements\n",
    "with open(r\"C:\\Users\\edwar\\Downloads\\blogs\\blogs\\5114.male.25.indUnk.Scorpio.xml\") as file:\n",
    "    soup = BeautifulSoup(file, 'lxml')\n",
    "    xml_text = soup.find_all(text=True)\n",
    "    processed_text_initial = ' '.join(xml_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8,,2001 Slashdot raises lots of interesting thoughts about banner ads . The idea is to let users co'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove a few odd artifacts that are easier to manually extract, and one random word that represents links while we're at it. \n",
    "processed_text = processed_text_initial.replace('\\n', '')\n",
    "processed_text = processed_text.replace('\\'', '')\n",
    "processed_text = processed_text.replace('urlLink', '')\n",
    "mapping = [ ('January', ''), ('February', ''), ('March', ''), ('April', ''), ('May', ''), ('June', ''), ('July', ''), ('August', ''), ('September', ''), \n",
    "           ('October', ''), ('November', ''), ('December', '') ]\n",
    "\n",
    "# begin mapping\n",
    "for k, v in mapping:\n",
    "    processed_text = processed_text.replace(k, v)\n",
    "    \n",
    "# we split to remove duplicate whitespaces, of which they are many many cases, and rejoin to have nice formatting.\n",
    "processed_text = processed_text.split()\n",
    "processed_text = ' '.join(processed_text)\n",
    "\n",
    "processed_text[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip contractions\n",
    "processed_text = contractions.fix(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8,,2001 Slashdot raises lots of interesting thoughts about banner ads . The idea is to let users co'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " ',2001',\n",
       " 'Slashdot',\n",
       " 'raise',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'interesting',\n",
       " 'thought',\n",
       " 'about']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize verb forms \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemon_lemmas = [lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(processed_text))]\n",
    "lemon_lemmas[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raise',\n",
       " 'lot',\n",
       " 'interesting',\n",
       " 'thought',\n",
       " 'banner',\n",
       " 'ad',\n",
       " 'The',\n",
       " 'idea',\n",
       " 'let']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "tokens = [w.translate(punct_table) for w in lemon_lemmas] \n",
    "\n",
    "# filter non-alpha and stop words\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "words[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = list(bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('raise', 'lot'),\n",
       " ('lot', 'interesting'),\n",
       " ('interesting', 'thought'),\n",
       " ('thought', 'banner'),\n",
       " ('banner', 'ad'),\n",
       " ('ad', 'The'),\n",
       " ('The', 'idea'),\n",
       " ('idea', 'let'),\n",
       " ('let', 'user')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bi-gram distribution counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bigrams</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>(new, issue)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>(I, got)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>(issue, Mindjack)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>(I, wa)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>(The, new)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>(look, like)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(Justin, Hall)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>(new, Mindjack)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>(Daily, Relay)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Bigrams  Frequency\n",
       "459        (new, issue)          8\n",
       "668            (I, got)          8\n",
       "460   (issue, Mindjack)          7\n",
       "316             (I, wa)          7\n",
       "458          (The, new)          6\n",
       "450        (look, like)          6\n",
       "141      (Justin, Hall)          6\n",
       "2477    (new, Mindjack)          6\n",
       "2059     (Daily, Relay)          6"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = FreqDist(bigram)\n",
    "bigram_df = pd.DataFrame(list(fdist.items()), columns = [\"Bigrams\",\"Frequency\"])\n",
    "bigram_df = bigram_df.sort_values('Frequency', ascending=False)\n",
    "bigram_df[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bi-gram probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate conditional frequency\n",
    "conditional_freq = ConditionalFreqDist(bigrams(tokens))\n",
    "# calculate conditional probability with MLE\n",
    "conditional_prob = ConditionalProbDist(conditional_freq, MLEProbDist)\n",
    "df = pd.DataFrame(columns = ['Term', 'HighestProbWord', 'Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "keys = []\n",
    "prob_max = []\n",
    "prob_value = []\n",
    "samples = []\n",
    "\n",
    "for k, v in conditional_prob.items():\n",
    "    keys.append(k) \n",
    "    prob_max.append(v.max())\n",
    "    prob_value.append(v.prob(v.max()))\n",
    "    samples.append(v.samples())\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "cell_style": "split",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>HighestProbWord</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slashdot</td>\n",
       "      <td>effect</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(raise, effect, launch)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raise</td>\n",
       "      <td>lot</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(lot)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lot</td>\n",
       "      <td>of</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(of, about, more)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>0.164103</td>\n",
       "      <td>(interesting, Cool, my, all, video, time, war,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interesting</td>\n",
       "      <td>thought</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>(thought, stuff, Four, vibe, that)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Term HighestProbWord  Probability  \\\n",
       "0     Slashdot          effect     0.500000   \n",
       "1        raise             lot     1.000000   \n",
       "2          lot              of     0.500000   \n",
       "3           of             the     0.164103   \n",
       "4  interesting         thought     0.200000   \n",
       "\n",
       "                                             Samples  \n",
       "0                            (raise, effect, launch)  \n",
       "1                                              (lot)  \n",
       "2                                  (of, about, more)  \n",
       "3  (interesting, Cool, my, all, video, time, war,...  \n",
       "4                 (thought, stuff, Four, vibe, that)  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Term'] = keys\n",
    "df['HighestProbWord'] = prob_max\n",
    "df['Probability'] = prob_value\n",
    "df['Samples'] = samples\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see here the most likely word to be generated, but based on the conditional frequency, any of our sample terms could be generated given the Term column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "\n",
    ">We supply the initial terminology and then our model will start making predictions based on the chosen sample word.\n",
    "It will continue to chain these words for as long as we set them to generate. For example, we can see raise only is paired with lot, so if we set our initial word\n",
    "to raise we will always start with lot, but lot will pick 1 of it's three samples, and then that sample will choose from it's sample, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot of time in Synapse is \n",
      "\n",
      "lot of I interview Warren Ellis \n",
      "\n",
      "lot about the most amazing thing \n",
      "\n",
      "lot of connectivity community wireless equivalent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example showing that every six words lot will be generated from setting the word to raise\n",
    "for show_me_lot_first in range(4):\n",
    "    # set first term\n",
    "    word = \"raise\"\n",
    "    # generate six words from initial term\n",
    "    for generate_word in range(6):\n",
    "        word = conditional_prob[word].generate()\n",
    "        print(word, end=\" \",)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your Tivo or no patience for \n",
      "\n",
      "an excellent Its not pay you \n",
      "\n",
      "Lemon is now online we were \n",
      "\n",
      "Feed Suck and start writing more \n",
      "\n",
      "my own PC My money ourselves \n",
      "\n",
      "work that have started a the \n",
      "\n",
      "Words arrived in the absence of \n",
      "\n",
      "America by Bill Vol reviewed by \n",
      "\n",
      "their support and What It work \n",
      "\n",
      "the state and depth of Cool \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's see a much more chaotic ngram example with a high variance sample\n",
    "for high_variance_term in range(10):\n",
    "    # set first term\n",
    "    word = \"of\"\n",
    "    # generate six words from initial term\n",
    "    for generate_words in range(6):\n",
    "        word = conditional_prob[word].generate()\n",
    "        print(word, end=\" \",)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wew. That's a hot mess. As word variance increases this model falls apart hardcore. \n",
    "Pass one is complete. Was a very hard week, and much to learn.  \n",
    "    \n",
    "    Next time I need to start tagging all the parts of speech, use those tags to serve as probalistic sentence structure for synthesis of more\n",
    "    realistc sentences.   \n",
    "    We need to handle more than a single document, pass one was purely to see if my idea was feasible, now that it's \"working\" i can scale it out. \n",
    "    Also, another way to handle the high variance is to increase the n-gram length, so that we have more human like thought than simple two word \n",
    "    pairs.\n",
    "Me caveman. Like NLP. : ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try again with tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slashdot raises lots of interesting thoughts about banner ads . The idea is to let users control th'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# remove numberic values and weird double commas\n",
    "processed_text_strip = re.sub('[0-9]+', '', processed_text)\n",
    "processed_text_strip = processed_text_strip.replace(',,', '')\n",
    "\n",
    "processed_text_strip[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanks to Onebox.com I should now be able to receive faxes.',\n",
       " 'Not suprisingly, they dont have local phone numbers for Fredericton so I chose a New York City area code.',\n",
       " 'If anyone wants to give it a whirl, my number is: () - x.',\n",
       " 'Well, I was supposed to be working.',\n",
       " 'But, through a long trail of procrastination, I ended up redesigning this site.',\n",
       " 'Quotable Mindjack!',\n",
       " 'From Mike Sugarbakers review of Lemon : \"If you have no patience for the lengthy ruminations of brilliant madmen, Lemon isnt for you.',\n",
       " 'But you read Mindjack, so youre probably into that sort of thing, right?\"',\n",
       " 'Mindjack is featured in the links section of this months Netlife magazine .',\n",
       " 'It doesnt seem to be online yet so heres a scan:  Well, the big Yahoo news yesterday was that Tim Koogle is stepping down as CEO and that theyll miss forecasts in the first quarter.']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(processed_text_strip)\n",
    "sent_tokens[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_freq_sent = ConditionalFreqDist(bigrams(sent_tokens))\n",
    "conditional_prob_sent = ConditionalProbDist(conditional_freq_sent, MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.DataFrame(columns = ['Sentence', 'HighestProb', 'Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_keys = []\n",
    "sent_prob_max = []\n",
    "sent_prob_value = []\n",
    "\n",
    "for k, v in conditional_prob_sent.items():\n",
    "    sent_keys.append(k) \n",
    "    sent_prob_max.append(v.max())\n",
    "    sent_prob_value.append(v.prob(v.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>HighestProb</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slashdot raises lots of interesting thoughts ...</td>\n",
       "      <td>The idea is to let users control the ad delive...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The idea is to let users control the ad delive...</td>\n",
       "      <td>The Merchants of Cool , a Frontline documentar...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Merchants of Cool , a Frontline documentar...</td>\n",
       "      <td>Check your local listings for the time.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check your local listings for the time.</td>\n",
       "      <td>ATMs dispensing music?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATMs dispensing music?</td>\n",
       "      <td>I do not quite see the logic in that.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0   Slashdot raises lots of interesting thoughts ...   \n",
       "1  The idea is to let users control the ad delive...   \n",
       "2  The Merchants of Cool , a Frontline documentar...   \n",
       "3            Check your local listings for the time.   \n",
       "4                             ATMs dispensing music?   \n",
       "\n",
       "                                         HighestProb  Probability  \n",
       "0  The idea is to let users control the ad delive...          1.0  \n",
       "1  The Merchants of Cool , a Frontline documentar...          1.0  \n",
       "2            Check your local listings for the time.          1.0  \n",
       "3                             ATMs dispensing music?          1.0  \n",
       "4              I do not quite see the logic in that.          1.0  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences['Sentence'] = sent_keys\n",
    "df_sentences['HighestProb'] = sent_prob_max\n",
    "df_sentences['Probability'] = sent_prob_value\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATMs dispensing music? \n",
      " I do not quite see the logic in that. \n",
      " "
     ]
    }
   ],
   "source": [
    "word = df_sentences['Sentence'][3]\n",
    "for index in range(2):\n",
    "    word = conditional_prob_sent[word].generate()\n",
    "    print(word, '\\n', end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, at least we have working sentences now. But this is misleading. The probablility of each sentence occuring with it's bigram 'friend' is exactly 1, because all the sentences are unique, so it manages to make decievingly convincing contextual information. And it will continue to string the next sentence exactly as it was written because the probablity of each generated sentence is still 1 for every new 'state' or link in the markov chain. We've created a parrot :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATMs dispensing music? \n",
      " I do not quite see the logic in that. \n",
      " I am not entirely against paying a nominal fee for music, or any other media, but if I do have to pay for it Id be much more likely to buy stuff from my own PC. \n",
      " My chair started squeaking a few days ago and its driving me nuts! \n",
      " The New York Press has a lenghty article about the post dotcom crash scene that is worth reading. \n",
      " "
     ]
    }
   ],
   "source": [
    "word = df_sentences['Sentence'][3]\n",
    "for index in range(5):\n",
    "    word = conditional_prob_sent[word].generate()\n",
    "    print(word, '\\n', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "430.403px",
    "left": "1307.92px",
    "top": "48.4444px",
    "width": "316.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
