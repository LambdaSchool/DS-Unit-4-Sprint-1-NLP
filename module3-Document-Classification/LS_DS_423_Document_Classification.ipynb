{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apR8ejZ7G1ca"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Lesson 3*\n",
    "\n",
    "# Document Classification & Clustering\n",
    "\n",
    "What do you do with all those cool document-term-matrices (dtm[s]) you created yesterday? You could use them in a sweet viz, or we can teach a dope algorithm to do some specific task. :) You have seen both classification and clustering before, so we won't focus on the particulars of algorithms. Instead we'll focus on the unique problems of dealing with text input for these models.\n",
    "\n",
    "## Learning Objectives\n",
    "* [Part 1](#p1): Vectorize a whole Corpus\n",
    "* [Part 2](#p2): Tune the vectorizer\n",
    "* [Part 3](#p3): Apply Vectorizer to Classification problem\n",
    "* [Part 4](#p4): Introduce topic modeling on text data\n",
    "\n",
    "**Business Case**: Your managers at Smartphone Inc. have asked to develop a system to bucket text messages into two categories: spam and not spam (ham). The system will be implemented on your companies products to help users identify suspicious texts.\n",
    "\n",
    "\n",
    "Ham | Spam\n",
    ":----: | :----:\n",
    "<img align=\"left\" src=\"https://images.unsplash.com/photo-1524438418049-ab2acb7aa48f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=3300&q=80\" width=500> | <img align=\"left\" src=\"https://images2.minutemediacdn.com/image/upload/c_crop,h_1576,w_2800,x_0,y_52/f_auto,q_auto,w_1100/v1554931909/shape/mentalfloss/20997-istock-471531747.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PtOOBxaifpY"
   },
   "source": [
    "# Spam Filter - Count Vectorization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuvyMXrajKwR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtQczusKkV6b"
   },
   "source": [
    "## Import the Data\n",
    "\n",
    "Import the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xo0H94dcGv5h"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/sokjc/BayesNotBaes/master/sms.tsv\"\n",
    "\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'msg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xofsmHqhkgjZ"
   },
   "source": [
    "## Tidy up initial DataFrame\n",
    "\n",
    "- Change Pandas display options so that we can see more of the text\n",
    "- Drop the unnamed columns, I'm not sure why they're in there, but we don't need them.\n",
    "- Rename the v1 and v2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tqjh7vXrjRIm",
    "outputId": "f3976c88-9add-4880-f559-4790bd81cfe5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  \\\n",
       "5567  spam   \n",
       "5568   ham   \n",
       "5569   ham   \n",
       "5570   ham   \n",
       "5571   ham   \n",
       "\n",
       "                                                                                                                                                                  text  \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.  \n",
       "5568                                                                                                                              Will ü b going to esplanade fr home?  \n",
       "5569                                                                                                         Pity, * was in mood for that. So...any other suggestions?  \n",
       "5570                                     The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free  \n",
       "5571                                                                                                                                        Rofl. Its true to its name  "
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "df = df.rename(columns={\"msg\":\"text\"})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yczMV4ock3T9"
   },
   "source": [
    "You'll notice right of the bat that this text isn't as coherent as the job listings. We'll proceed like normal though. \n",
    "\n",
    "What is the ratio of Spam to Ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "90ajEqS0jsNH",
    "outputId": "823615d7-db7f-4ee1-90cc-5102adc0567f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wE9G1fxwKcRx",
    "outputId": "632959fb-fa14-4961-a94c-2de32100d078"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgMf4tmoK4pE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHL35a3ElQZZ"
   },
   "source": [
    "## Categorical encoding on labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0pxdU43WlNPi",
    "outputId": "4ff5e03d-3576-4c24-b02d-b72448e20e8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                          text  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_num'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FryOCtIm1ts"
   },
   "source": [
    "## Model Validation - Train Test Split (quick and dirty)\n",
    "Since we're going to do some modeling we're going to need some model validation. For simplicity lets just do a quick train_test_split for today. You can try out Cross Validation on your assignment today, I just want to get to a quick baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xzjs9vdyniq7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.text\n",
    "y = df.label_num\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfXT_v7YvYYz"
   },
   "source": [
    "Look at sizes of our train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "a42hg4S4vaLv",
    "outputId": "d6954710-2151-4130-a1bc-bfe32fcb0d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457,)\n",
      "(1115,)\n",
      "(4457,)\n",
      "(1115,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1091
    },
    "colab_type": "code",
    "id": "eiuNWUiwM-66",
    "outputId": "f83e7f16-c5a9-4fd0-a168-2cd4ebba24cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2757                                                                                                                  Have a good trip. Watch out for . Remember when you get back we must decide about easter.\n",
       "1526                                                                                                                                                                           Pls pls find out from aunt nike.\n",
       "1856                                                                                                                                                         K.:)you are the only girl waiting in reception ah?\n",
       "554                                                                  Ok. Every night take a warm bath drink a cup of milk and you'll see a work of magic. You still need to loose weight. Just so that you know\n",
       "3622             That means from february to april i'll be getting a place to stay down there so i don't have to hustle back and forth during audition season as i have since my sister moved away from harlem.\n",
       "4419                                                                                                                                                                                 When you get free, call me\n",
       "670                                                                                                                                                                                       Did u receive my msg?\n",
       "82                                                                                                                                                                             Ok i am on the way to home hi hi\n",
       "3797                                                           They have a thread on the wishlist section of the forums where ppl post nitro requests. Start from the last page and collect from the bottom up.\n",
       "573                                                                                                                                                                                      Can you open the door?\n",
       "1643                                                                                                                                                                                   Sleeping nt feeling well\n",
       "270                                                                       Ringtone Club: Get the UK singles chart on your mobile each week and choose any top quality ringtone! This message is free of charge.\n",
       "3028                                                                                                                                                                                     You still at the game?\n",
       "495                                                                                                                                                                            Are you free now?can i call now?\n",
       "26                                                                                                                                                                               Lol your always so convincing.\n",
       "4577                                                                                        Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066\n",
       "319                                               December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906\n",
       "5332                                                                                                                                                                      I think steyn surely get one wicket:)\n",
       "1703                                                                                                                                                                          I have lost 10 kilos as of today!\n",
       "4357                                                                                                                                                             Great. So should i send you my account number.\n",
       "2528                                             Mmmmm ... I loved waking to your words this morning ! I miss you too, my Love. I hope your day goes well and you are happy. I wait for us to be together again\n",
       "1460                                                                                                                          Bought one ringtone and now getting texts costing 3 pound offering more tones etc\n",
       "3440                                                                                                                             awesome, how do I deal with the gate? Charles told me last night but, uh, yeah\n",
       "857                                                                                                                                                                               Going to take your babe out ?\n",
       "2997                                                                                           They released vday shirts and when u put it on it makes your bottom half naked instead of those white underwear.\n",
       "4291    For you information, IKEA is spelled with all caps. That is not yelling. when you thought i had left you, you were sitting on the bed among the mess when i came in. i said we were going after you ...\n",
       "3788                                                                                                                                                                                WHORE YOU ARE UNBELIEVABLE.\n",
       "4566                                             Honeybee Said: *I'm d Sweetest in d World* God Laughed &amp; Said: *Wait,U Havnt Met d Person Reading This Msg* MORAL: Even GOD Can Crack Jokes! GM+GN+GE+GN:)\n",
       "1235                                                                              Hello-/@drivby-:0quit edrunk sorry iff pthis makes no senrd-dnot no how ^ dancce 2 drum n basq!ihave fun 2nhite x ros xxxxxxx\n",
       "2409                                                                                                                                                                  Dear where you will be when i reach there\n",
       "                                                                                                         ...                                                                                                   \n",
       "4296                                                HMV BONUS SPECIAL 500 pounds of genuine HMV vouchers to be won. Just answer 4 easy questions. Play Now! Send HMV to 86688 More info:www.100percent-real.com\n",
       "5454                                                                                                                                                               Im just wondering what your doing right now?\n",
       "3599                                                                                                                                                                             Aight, we'll head out in a few\n",
       "3949                                                                                                                                    I like to think there's always the possibility of being in a pub later.\n",
       "2961                                                                                                                                                                           Sir send to group mail check it.\n",
       "186                                                    Hello handsome ! Are you finding that job ? Not being lazy ? Working towards getting back that net for mummy ? Where's my boytoy now ? Does he miss me ?\n",
       "3365                                                                                                                                                            Yo my trip got postponed, you still stocked up?\n",
       "1585                                                                                                                                                                                     Sorry, I'll call later\n",
       "3147                                                                                       SHIT BABE.. THASA BIT MESSED UP.YEH, SHE SHUDVETOLD U. DID URGRAN KNOW?NEWAY, ILLSPEAK 2 U2MORO WEN IM NOT ASLEEP...\n",
       "2069                                                                                                        Its hard to believe things like this. All can say lie but think twice before saying anything to me.\n",
       "2546                                                                                                                           So are you guys asking that i get that slippers again or its gone with last year\n",
       "535                           I've not called you in a while. This is hoping it was l8r malaria and that you know that we miss you guys. I miss Bani big, so pls give her my love especially. Have a great day.\n",
       "1722                                                                   Am watching house – very entertaining – am getting the whole hugh laurie thing – even with the stick – indeed especially with the stick.\n",
       "120                                                            PRIVATE! Your 2004 Account Statement for 07742676969 shows 786 unredeemed Bonus Points. To claim call 08719180248 Identifier Code: 45239 Expires\n",
       "5299                                                                                                                                                    Well good morning mr . Hows london treatin' ya treacle?\n",
       "2112                                                        Yar he quite clever but aft many guesses lor. He got ask me 2 bring but i thk darren not so willing 2 go. Aiya they thk leona still not attach wat.\n",
       "1384    Please reserve ticket on saturday eve from chennai to thirunelvali and again from tirunelvali to chennai on sunday eve...i already see in net..no ticket available..i want to book ticket through ta...\n",
       "3732    Isn't frnd a necesity in life? imagine urself witout a frnd.. hw'd u feel at ur colleg? wat'll u do wth ur cell? wat abt functions? thnk abt events espe'll cared, missed &amp; irritated u? 4wrd it...\n",
       "3475                                                                                                                                                                     , how's things? Just a quick question.\n",
       "3898                                                                                                                                                                       No. Thank you. You've been wonderful\n",
       "4804                                                                                                                                                                             How do you plan to manage that\n",
       "750                                                                                                                                                                  By monday next week. Give me the full gist\n",
       "672                                                                                                               SMS. ac sun0819 posts HELLO:\"You seem cool, wanted to say hi. HI!!!\" Stop? Send STOP to 62468\n",
       "950                                                                                                                                                                      Is that what time you want me to come?\n",
       "603                                                                                                                                                                      Speaking of does he have any cash yet?\n",
       "361                                                                                                                                                                      Ha ha cool cool chikku chikku:-):-DB-)\n",
       "99                                                                                                                                                                              I see a cup of coffee animation\n",
       "4688                                                                                                                                                                                          Eatin my lunch...\n",
       "392                                                                                                                            Hey so this sat are we going for the intro pilates only? Or the kickboxing too? \n",
       "3051                                                                                                                                                                                                         Ok\n",
       "Name: text, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vnv4EIJLmTuM"
   },
   "source": [
    "## Count Vectorizer\n",
    "\n",
    "Today we're just going to let Scikit-Learn do our text cleaning and preprocessing for us.\n",
    "\n",
    "Lets run our vectorizer on our text messages and take a peek at the tokenization of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "I1Vzh1EGrqeR",
    "outputId": "367cf9f0-6656-4301-ba01-7ef9d79acdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['150p16', '150pm', '150ppermesssubscription', '150ppm', '150ppmpobox10183bhamb64xe', '150ppmsg', '150pw', '151', '153', '15541', '16', '165', '1680', '169', '177', '18', '1843', '18p', '18yrs', '195', '1apple', '1b6a5ecef91ff9', '1cup', '1da', '1er']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.get_feature_names()[300:325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WN1oBhrUNpqH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lu0yhnpssWur"
   },
   "source": [
    "Now we'll complete the vectorization by running .transform() and then save the results to a dataframe for viewing.\n",
    "You don't need to save it to a dataframe, you can use most ML models with just the 2D array output.\n",
    "\n",
    "That's a lot of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "t0dz2qUKldSU",
    "outputId": "b8c4a110-5019-4d76-ceca-7c8ed6b06b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 7443)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000pes  008704050406  0089  0121  01223585334  0125698789  02  \\\n",
       "0   0    0       0             0     0     0            0           0   0   \n",
       "1   0    0       0             0     0     0            0           0   0   \n",
       "2   0    0       0             0     0     0            0           0   0   \n",
       "3   0    0       0             0     0     0            0           0   0   \n",
       "4   0    0       0             0     0     0            0           0   0   \n",
       "\n",
       "   0207  ...  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  èn  ú1  〨ud  \n",
       "0     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "1     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "2     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "3     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "4     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "\n",
       "[5 rows x 7443 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CKL75Uqtu8J"
   },
   "source": [
    "We also need to vectorize our X_test data, but we need to use the same vocabulary as the training dataset, so we'll just call .transform() on X_test to get our vectorized X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "iJ2gw3GLt6jE",
    "outputId": "38f663af-8157-405f-8a92-db87037290a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 7443)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000pes  008704050406  0089  0121  01223585334  0125698789  02  \\\n",
       "0   0    0       0             0     0     0            0           0   0   \n",
       "1   0    0       0             0     0     0            0           0   0   \n",
       "2   0    0       0             0     0     0            0           0   0   \n",
       "3   0    0       0             0     0     0            0           0   0   \n",
       "4   0    0       0             0     0     0            0           0   0   \n",
       "\n",
       "   0207  ...  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  èn  ú1  〨ud  \n",
       "0     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "1     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "2     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "3     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "4     0  ...      0       0    0          0     0     0      0   0   0    0  \n",
       "\n",
       "[5 rows x 7443 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAWEHttfszBQ"
   },
   "source": [
    "Lets run some classification models and see what kind of accuracy we can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOaeSYgeOld9"
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-f3Sz4Twl3P"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "qWRLCgevs8tP",
    "outputId": "1022524b-0c69-4183-b522-48f9244a0c10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAY4SVOeupWY"
   },
   "source": [
    "Now we'll evaluate both our training and testing accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1dnkJa9Xp60e",
    "outputId": "63b1d583-ced3-4f25-9811-b2b3d00b7a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9703836661431456\n",
      "Test Accuracy: 0.9551569506726457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "columns = ['model', 'acc_train', 'acc_test', 'vect']\n",
    "\n",
    "lr_result = {}\n",
    "lr_result['model'] = 'Logistic Regression'\n",
    "lr_result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "lr_result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "lr_result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(lr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1J9V6GySxH5J"
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BoNVLAthwhQw",
    "outputId": "226e4bee-212e-42a5-986a-0b6049a7da38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.982499439084586\n",
      "Test Accuracy: 0.9659192825112107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Multinomial Naive Bayes'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "QjHy5VNgSEHS",
    "outputId": "28c92ba0-55c4-4129-cbe4-39e2dd9c098c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc_test': 0.9551569506726457,\n",
       "  'acc_train': 0.9703836661431456,\n",
       "  'model': 'Logistic Regression',\n",
       "  'vect_type': 'Count'},\n",
       " {'acc_test': 0.9659192825112107,\n",
       "  'acc_train': 0.982499439084586,\n",
       "  'model': 'Multinomial Naive Bayes',\n",
       "  'vect_type': 'Count'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBs-rHNGxjJd"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "HqlyPSOVxhJJ",
    "outputId": "c53c0732-0175-4c5a-c444-f35f505caa13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9975319721785955\n",
      "Test Accuracy: 0.9659192825112107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Random Forest'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "8W7Ij07BSXN5",
    "outputId": "4b19a7a7-cf63-4be4-9636-0f7f541279e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955157</td>\n",
       "      <td>0.970384</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.998654</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                    model vect_type\n",
       "0  0.955157   0.970384      Logistic Regression     Count\n",
       "1  0.965919   0.982499  Multinomial Naive Bayes     Count\n",
       "2  0.965919   0.998654            Random Forest     Count"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results = pd.DataFrame.from_records(results)\n",
    "#results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVozYr-dS_E7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hc-FmCIzyyGt"
   },
   "source": [
    "# Spam Filter - TF-IDF Vectorization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "IPGVmNN3y-sX",
    "outputId": "c3ff2bca-93e2-443b-b46d-864258296c5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcxu089F0GzM"
   },
   "source": [
    "## Vectorize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "kdJnMS86zyTf",
    "outputId": "c3a7c463-8d15-446e-e01b-9ff4c422768d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 7443)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  01223585334  0125698789   02  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "\n",
       "   0207  ...  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada   èn   ú1  〨ud  \n",
       "0   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "1   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "2   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "3   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "4   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 7443 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wai5ltRn0JMH"
   },
   "source": [
    "## Vectorize testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "yZY7QkM50EiN",
    "outputId": "6fe1f40c-9247-4cd4-de73-1080891c944d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 7443)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  01223585334  0125698789   02  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0          0.0         0.0  0.0   \n",
       "\n",
       "   0207  ...  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada   èn   ú1  〨ud  \n",
       "0   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "1   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "2   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "3   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "4   0.0  ...    0.0     0.0  0.0        0.0   0.0   0.0    0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 7443 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vQ4Saxv0WFz"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "_bhI1BPe0N0q",
    "outputId": "dc834747-deb1-4fb1-ed24-783e0a114575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9703836661431456\n",
      "Test Accuracy: 0.9551569506726457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Logistic'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjFCJDnw0X-u"
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "46-ShEu10aVO",
    "outputId": "e05c0147-fea3-4dc9-8b10-fce56c12c11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.982499439084586\n",
      "Test Accuracy: 0.9659192825112107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Naive Bayes'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4FucTNZ0ajz"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "4jn_fmZy0dfa",
    "outputId": "d8a8ae9b-651c-4dc3-b098-f556a9839fc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99798070450976\n",
      "Test Accuracy: 0.9650224215246637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Random Forest'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDVtCrKmUKWG"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gwpDzAAGUP_E",
    "outputId": "7665a535-884e-4197-dc26-6f7724e3ba0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955157</td>\n",
       "      <td>0.970384</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955157</td>\n",
       "      <td>0.970384</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                    model vect_type\n",
       "0  0.955157   0.970384      Logistic Regression     Count\n",
       "1  0.965919   0.982499  Multinomial Naive Bayes     Count\n",
       "2  0.965919   0.997532            Random Forest     Count\n",
       "3  0.955157   0.970384                 Logistic     Tfidf\n",
       "4  0.965919   0.982499              Naive Bayes     Tfidf"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cm4RkbL01uBh"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3P3xtLZELpD"
   },
   "source": [
    "## What is Sentiment Analysis?\n",
    "\n",
    "The objective of sentiment analysis is to take a phrase and based on the text of the phrase determine if its sentiment is: Postive, Neutral, or Negative. \n",
    "\n",
    "Suppose that you wanted to use NLP to classify reviews for your company's products as either positive, neutral, or negative. Maybe you don't trust the star ratings left by the users and you want an additional measure of sentiment from each review - maybe you would use this as a feature generation technique for additional modeling, or to identify disgruntled customers and reach out to them to improve your customer service, etc. Sentiment Analysis has also been used heavily in stock market price estimation by trying to track the sentiment of the tweets of individuals after breaking news comes out about a company.\n",
    "\n",
    "Does every word in each review contribute to its overall sentiment? Not really. Stop words for example don't really tell us much about the overall sentiment of the text, so just like we did before, we will discard them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okxG3HwRWEuc"
   },
   "source": [
    "## NLTK Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "skc_zUROF7mL",
    "outputId": "afa3ca0d-04fa-429f-cbc1-8fdb7949226c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 9.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import movie_reviews\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezXmYriEIRRN"
   },
   "source": [
    "## Check that we have movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GENb7g1pF8AL",
    "outputId": "4d3e8bea-4d2a-4a15-b582-4294ea57c216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 2000\n",
      "Positive reviews: 1000\n",
      "Negative reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "# How many total reviews are there?\n",
    "print(\"Total reviews:\", len(movie_reviews.fileids()))\n",
    "\n",
    "# Total positive reviews\n",
    "print(\"Positive reviews:\", len(movie_reviews.fileids('pos'))) \n",
    " \n",
    "# Total negative reviews\n",
    "print(\"Negative reviews:\", len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "uPXlAORQYY6e",
    "outputId": "71410bc1-77ff-4b2c-b13a-88dd1175e5e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos/cv000_29590.txt',\n",
       " 'pos/cv001_18431.txt',\n",
       " 'pos/cv002_15918.txt',\n",
       " 'pos/cv003_11664.txt',\n",
       " 'pos/cv004_11636.txt',\n",
       " 'pos/cv005_29443.txt',\n",
       " 'pos/cv006_15448.txt',\n",
       " 'pos/cv007_4968.txt',\n",
       " 'pos/cv008_29435.txt',\n",
       " 'pos/cv009_29592.txt']"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids('pos')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erS5mRrJIWuG"
   },
   "source": [
    "## Get Reviews and randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DcOuyfqHoms"
   },
   "outputs": [],
   "source": [
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IX-v-Z8XWLbt"
   },
   "source": [
    "## Understand the format of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "XOm7xzaUIF03",
    "outputId": "c9a5d8d3-52d9-4f1f-e40b-48f6e4b77fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'came', 'to', 'an', 'epiphany', 'while', 'watching', 'the', 'bachelor', ',', 'an', 'innocuous', '-', 'enough', '-', 'on', '-', 'the', '-', 'surface', 'romantic', 'comedy', '.', 'it', \"'\", 's', 'not', 'the', 'sort', 'of', 'film', 'in', 'which', 'one', 'would', 'expect', 'to', 'achieve', 'any', 'moment', 'of', 'clarity', ',', 'but', 'there', 'it', 'was', 'nonetheless', '.', 'i', 'sat', 'there', 'watching', 'this', 'marshmallow', 'of', 'a', 'movie', 'unfold', 'when', 'suddenly', 'i', 'realized', 'what', 'is', 'so', 'ridiculously', 'wrong', 'with', 'the', 'entire', 'romantic', 'comedy', 'genre', 'circa', '1999', '.', 'in', 'a', 'word', ',', 'it', \"'\", 's', 'the', 'same', 'thing', 'that', \"'\", 's', 'wrong', 'with', 'so', 'many', 'movies', 'circa', '1999', ':', 'writing', '.', 'more', 'to', 'the', 'point', ',', 'it', \"'\", 's', 'the', 'refusal', 'to', 'acknowledge', 'that', 'characterizations', 'matter', 'when', 'you', \"'\", 're', 'telling', 'a', 'story', 'about', 'a', 'relationship', '.', 'the', 'bachelor', 'is', 'merely', 'the', 'latest', 'in', 'a', 'long', 'line', 'of', 'films', 'where', 'we', \"'\", 're', 'expected', 'to', 'get', 'dewy', '-', 'eyed', 'over', 'any', 'pairing', 'of', 'attractive', ',', 'pleasant', 'people', 'just', 'because', 'they', \"'\", 're', 'attractive', 'and', 'pleasant', '.', 'in', 'this', 'particular', 'case', ',', 'attractive', 'and', 'pleasant', 'exhibit', 'a', 'is', 'jimmy', 'shannon', '(', 'chris', 'o', \"'\", 'donnell', ')', ',', 'a', 'single', 'guy', 'who', 'has', 'been', 'watching', 'his', 'friends', 'slowly', 'but', 'surely', 'sucked', 'into', 'marriage', '.', 'it', \"'\", 's', 'a', 'scary', 'notion', 'for', 'jimmy', ',', 'even', 'though', 'he', 'dearly', 'loves', 'attractive', 'and', 'pleasant', 'exhibit', 'b', 'anne', '(', 'renee', 'zellweger', ')', ',', 'his', 'girlfriend', 'of', 'three', 'years', '.', 'convinced', 'despite', 'his', 'reservations', 'that', 'it', \"'\", 's', 'time', 'to', '\"', 'sh', '*', 't', 'or', 'get', 'off', 'the', 'pot', ',', '\"', 'jimmy', 'proposes', 'to', 'anne', '--', 'very', 'badly', '.', 'anne', 'refuses', ',', 'which', 'leaves', 'jimmy', 'in', 'a', 'very', 'odd', 'position', 'when', 'his', 'eccentric', 'grandfather', '(', 'peter', 'ustinov', ')', 'dies', 'and', 'leaves', 'a', 'very', 'specific', 'video', 'will', '.', 'jimmy', 'stands', 'to', 'inhereit', '$', '100', 'million', 'if', 'he', 'is', 'married', 'by', '6', ':', '05', 'p', '.', 'm', '.', 'on', 'his', '30th', 'birthday', ',', 'stays', 'married', 'for', '10', 'years', 'and', 'produces', 'a', 'child', '.', 'there', 'are', 'only', 'a', 'couple', 'of', 'minor', 'problems', ':', '1', ')', 'jimmy', \"'\", 's', '30th', 'birthday', 'is', 'the', 'next', 'day', ';', '2', ')', 'anne', 'is', 'nowhere', 'to', 'be', 'found', ',', 'meaning', 'jimmy', 'has', 'to', 'find', 'another', 'willing', 'bride', 'from', 'among', 'his', 'many', 'ex', '-', 'girlfriends', '.', 'it', \"'\", 's', 'a', 'wacky', ',', 'brewster', \"'\", 's', 'millions', '-', 'esque', 'premise', '(', 'acknowledged', 'as', 'such', 'in', 'one', 'of', 'the', 'film', \"'\", 's', 'better', ',', 'more', 'self', '-', 'aware', 'lines', 'of', 'dialogue', ')', ',', 'the', 'kind', 'where', 'a', 'shallow', 'and', 'materialistic', 'guy', 'learns', 'what', 'really', 'matters', '.', 'at', 'least', 'that', 'would', 'be', 'the', 'case', 'if', 'jimmy', 'weren', \"'\", 't', 'already', 'a', 'world', '-', 'class', 'altruist', '.', 'screenwriter', 'steve', 'cohen', 'slides', 'into', 'the', 'story', 'an', 'even', 'more', 'draconian', 'condition', 'in', 'the', 'will', ':', 'if', 'jimmy', 'doesn', \"'\", 't', 'get', 'married', ',', 'not', 'only', 'will', 'he', 'lose', 'all', 'the', 'money', ',', 'but', 'the', 'family', 'billiard', 'table', 'buisness', 'will', 'be', 'sold', 'out', 'from', 'under', 'him', ',', 'costing', 'hudreds', 'of', 'jobs', '.', 'from', 'the', 'outset', ',', 'jimmy', \"'\", 's', 'motivation', 'isn', \"'\", 't', 'cash', ';', 'it', \"'\", 's', 'the', 'livelihoods', 'of', 'his', 'devoted', 'employees', '.', 'it', \"'\", 's', 'almost', 'embarrassing', 'for', 'his', 'marital', 'misgivings', 'to', 'play', 'a', 'role', 'in', 'the', 'bachelor', \"'\", 's', 'plot', 'development', '.', 'by', 'any', 'human', 'standard', ',', 'the', 'guy', 'is', 'impossibly', 'selfless', '.', 'and', 'that', \"'\", 's', 'the', 'essence', 'of', 'the', 'gutlessness', 'endemic', 'to', 'films', 'like', 'the', 'bachelor', '--', 'the', 'fear', 'of', 'giving', 'the', 'characters', 'flaws', 'to', 'overcome', 'on', 'their', 'way', 'to', 'happiness', '.', 'there', \"'\", 's', 'never', 'any', 'tension', 'between', 'the', 'two', 'star', '-', 'crossed', 'lovers', ',', 'because', 'there', \"'\", 's', 'no', 'sense', 'that', 'anything', 'remotely', 'significant', 'is', 'at', 'stake', '.', 'the', 'blandly', 'nice', 'o', \"'\", 'donnell', 'couldn', \"'\", 't', 'pull', 'off', 'a', 'randy', 'cad', 'if', 'he', 'tried', ',', 'so', 'the', 'filmmakers', 'don', \"'\", 't', 'even', 'let', 'him', ';', 'zellweger', \"'\", 's', 'anne', 'may', 'have', 'issues', 'with', 'her', 'sickeningly', 'affectionate', 'parents', 'as', 'an', 'impossible', 'standard', 'to', 'live', 'up', 'to', ',', 'but', 'no', 'one', 'dares', 'make', 'her', 'anything', 'but', 'the', 'woman', '(', 'lightly', ')', 'wronged', '.', 'and', 'forget', 'about', 'seeing', 'enough', 'of', 'jimmy', 'and', 'anne', 'together', 'to', 'feel', 'invested', 'in', 'their', 'potential', 'reconciliation', '.', 'the', 'parade', 'of', 'sit', '-', 'com', 'set', 'pieces', 'had', 'better', 'be', 'damned', 'funny', ',', 'since', 'they', \"'\", 're', 'all', 'that', 'stands', 'between', 'us', 'and', 'a', 'blissfully', 'sweet', 'foregone', 'conclusion', '.', 'i', \"'\", 'll', 'admit', 'a', 'couple', 'of', 'those', 'set', 'pieces', 'are', 'amusing', ',', 'including', 'ustinov', \"'\", 's', 'rantings', 'about', 'procreation', 'and', 'a', 'restaurant', 'full', '-', 'to', '-', 'bursting', 'with', 'men', 'popping', 'questions', 'and', 'champagne', 'corks', '.', 'far', 'more', 'of', 'them', 'are', 'either', 'tedious', 'or', 'downright', 'ghastly', ',', 'like', 'the', 'shudder', '-', 'inducing', 'sight', 'of', 'brooke', 'shields', 'as', 'an', 'icy', 'fortune', '-', 'hunter', 'or', 'the', 'hideous', 'collection', 'of', 'stereotypes', 'when', 'hundreds', 'of', 'potential', 'brides', 'gather', 'in', 'a', 'church', '.', 'you', \"'\", 're', 'never', 'going', 'to', 'get', 'too', 'many', 'raucous', 'belly', 'laughs', 'from', 'a', 'film', 'like', 'the', 'bachelor', ',', 'but', 'that', \"'\", 's', 'not', 'the', 'real', 'problem', '.', 'nor', 'is', 'it', 'the', 'real', 'problem', 'that', 'you', 'know', 'exactly', 'the', 'kind', 'of', 'warm', '-', 'n', '-', 'fuzzy', 'conclusion', 'it', \"'\", 's', 'leading', 'up', 'to', '.', 'the', 'problem', 'are', 'a', 'beginning', 'and', 'middle', 'that', 'are', 'equally', 'warm', '-', 'n', '-', 'fuzzy', '--', 'there', \"'\", 's', 'no', 'spark', ',', 'no', 'energy', ',', 'no', 'humanity', '.', 'it', \"'\", 's', 'an', 'emotional', 'pudding', 'guaranteed', 'not', 'to', 'offend', 'any', 'consumer', \"'\", 's', 'digestions', '.', 'we', \"'\", 've', 'reached', 'a', 'point', 'where', 'our', 'proxies', 'for', 'cinematic', 'romantic', 'wish', 'fulfillment', 'don', \"'\", 't', 'even', 'have', 'a', 'pulse', '.', 'the', 'bachelor', 'is', 'love', 'among', 'the', 'mannequins', '.']\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Print Review Text:\n",
    "print(reviews[0][0])\n",
    "\n",
    "# Print Review Sentiment:\n",
    "print(reviews[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOF0ANVsI1Rt"
   },
   "source": [
    "## Add reviews to a dataframe for kicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "gRO9_tfvIcEM",
    "outputId": "83fb0a8b-557a-4ec2-92a8-e1afb435696f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i came to an epiphany while watching the bachelor , an innocuous - enough - on - the - surface romantic comedy . it ' s not the sort of film in which one would expect to achieve any moment of clar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aspiring broadway composer robert ( aaron williams ) secretly carries a torch for his best friend , struggling actor marc ( michael shawn lucas ) . the problem is , marc only has eyes for \" perfec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there have been bad films in recent years : ' mr . magoo ' was by far the worst ever made , the spectacularly bad ' blue in the face ' , the horrible ' baby genuises ' and now ' i woke up early th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one of the biggest cliches of any serial killer film is also one of the most believable . you know , the one where the detective looks at a wall of pictures and other police information , and sudd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nearly every film tim burton has directed has been an homage to the horror genre -- \" frankenweenie , \" \" beetlejuice , \" \" batman , \" \" edward scissorhands , \" \" ed wood , \" \" mars attacks ! \" --...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "0  i came to an epiphany while watching the bachelor , an innocuous - enough - on - the - surface romantic comedy . it ' s not the sort of film in which one would expect to achieve any moment of clar...   \n",
       "1  aspiring broadway composer robert ( aaron williams ) secretly carries a torch for his best friend , struggling actor marc ( michael shawn lucas ) . the problem is , marc only has eyes for \" perfec...   \n",
       "2  there have been bad films in recent years : ' mr . magoo ' was by far the worst ever made , the spectacularly bad ' blue in the face ' , the horrible ' baby genuises ' and now ' i woke up early th...   \n",
       "3  one of the biggest cliches of any serial killer film is also one of the most believable . you know , the one where the detective looks at a wall of pictures and other police information , and sudd...   \n",
       "4  nearly every film tim burton has directed has been an homage to the horror genre -- \" frankenweenie , \" \" beetlejuice , \" \" batman , \" \" edward scissorhands , \" \" ed wood , \" \" mars attacks ! \" --...   \n",
       "\n",
       "   sentiment  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "sentiments = []\n",
    "\n",
    "for review in reviews:\n",
    "  \n",
    "  # Add sentiment to list\n",
    "  if review[1] == \"pos\":\n",
    "    sentiments.append(1)\n",
    "  else:\n",
    "    sentiments.append(0)\n",
    "  \n",
    "  # Add text to list\n",
    "  review_text = \" \".join(review[0])\n",
    "  documents.append(review_text)\n",
    "  \n",
    "df = pd.DataFrame({\"text\": documents, \"sentiment\": sentiments})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyrzWJo6VVpk"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRpgpiZ4VVAM"
   },
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlH7u0pPaV86"
   },
   "source": [
    "# Sentiment Analysis - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hh_JY67SWXNS"
   },
   "source": [
    "## Generate vocabulary from train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d9hiHBouSvBC",
    "outputId": "d5afde59-cf1a-4f22-830e-712aadd1651a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5h6qB4MbSYJ"
   },
   "source": [
    "## Generate Vectorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "1Ofvut25XgDU",
    "outputId": "d92946f3-57b2-4a3b-e6b3-cce51e43e6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 35995)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>007</th>\n",
       "      <th>00s</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zycie</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35995 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0009f  007  00s  05  10  100  1000  10000  ...  zucker  zuko  \\\n",
       "0   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "1   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "2   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "3   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "4   0    1      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "\n",
       "   zukovsky  zundel  zurg  zweibel  zwick  zwigoff  zycie  zzzzzzz  \n",
       "0         0       0     0        0      0        0      0        0  \n",
       "1         0       0     0        0      0        0      0        0  \n",
       "2         0       0     0        0      0        0      0        0  \n",
       "3         0       0     0        0      0        0      0        0  \n",
       "4         0       0     0        0      0        0      0        0  \n",
       "\n",
       "[5 rows x 35995 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "Ni_sEpWzXp7j",
    "outputId": "741866b7-699b-41de-dc94-6d315fff0925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 35995)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>007</th>\n",
       "      <th>00s</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zycie</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35995 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0009f  007  00s  05  10  100  1000  10000  ...  zucker  zuko  \\\n",
       "0   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "1   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "2   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "3   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "4   0    0      0    0    0   0   0    0     0      0  ...       0     0   \n",
       "\n",
       "   zukovsky  zundel  zurg  zweibel  zwick  zwigoff  zycie  zzzzzzz  \n",
       "0         0       0     0        0      0        0      0        0  \n",
       "1         0       0     0        0      0        0      0        0  \n",
       "2         0       0     0        0      0        0      0        0  \n",
       "3         0       0     0        0      0        0      0        0  \n",
       "4         0       0     0        0      0        0      0        0  \n",
       "\n",
       "[5 rows x 35995 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2e7FAmlMX2e-"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "5NLlkIg_YDUA",
    "outputId": "7c7543d3-3463-4960-b57d-999e2c054ed2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXOXgdBLX2m7"
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ELxvYcwTYELb",
    "outputId": "2e54badb-fc48-45fc-e0f0-83fcc2bd16de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.981875\n",
      "Test Accuracy: 0.8025\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBr2hR_iX2ui"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "UrhtXmB2YEx0",
    "outputId": "3e81a3a0-5700-474b-e643-030b1216fbf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.98875\n",
      "Test Accuracy: 0.6925\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18n4qfR7a6Sk"
   },
   "source": [
    "# Sentiment Analysis - tfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCBDiKvfb6tL"
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FojVcXZTbJQG",
    "outputId": "c4e1d3c5-0081-4a04-b907-26b2e48932c4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1,2),\n",
    "                             min_df = 5, max_df = .80,\n",
    "                             stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfdx7fFnb9xM"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "Yp6MxwlKbZ3s",
    "outputId": "93527670-06fc-44a5-e2b9-99687fe22d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>...</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>years later</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young man</th>\n",
       "      <th>younger</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000   10  100   13        15  1995  1996  1997  1998  1999  ...  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  0.000000  0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  0.000000  0.0  0.0  0.0  0.050565   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  0.000000  0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  0.048029  0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   year old  years  years ago  years later  yes  york  young  young man  \\\n",
       "0  0.000000    0.0        0.0          0.0  0.0   0.0    0.0        0.0   \n",
       "1  0.000000    0.0        0.0          0.0  0.0   0.0    0.0        0.0   \n",
       "2  0.040450    0.0        0.0          0.0  0.0   0.0    0.0        0.0   \n",
       "3  0.000000    0.0        0.0          0.0  0.0   0.0    0.0        0.0   \n",
       "4  0.041431    0.0        0.0          0.0  0.0   0.0    0.0        0.0   \n",
       "\n",
       "   younger  zero  \n",
       "0      0.0   0.0  \n",
       "1      0.0   0.0  \n",
       "2      0.0   0.0  \n",
       "3      0.0   0.0  \n",
       "4      0.0   0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G47FQ9BmcAI5"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "UZ2Py6HJbgVs",
    "outputId": "0d3564ef-57fd-460a-a60e-5882da2a9944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>...</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>years ago</th>\n",
       "      <th>years later</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young man</th>\n",
       "      <th>younger</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071964</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000   10  100   13   15  1995      1996  1997  1998  1999  ...  year old  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   0.0   0.0  ...       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   0.0   0.0  ...       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0   0.0  0.043404   0.0   0.0   0.0  ...       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   0.0   0.0  ...       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0   0.0  0.000000   0.0   0.0   0.0  ...       0.0   \n",
       "\n",
       "      years  years ago  years later       yes      york     young  young man  \\\n",
       "0  0.000000   0.000000          0.0  0.000000  0.000000  0.000000        0.0   \n",
       "1  0.071964   0.065068          0.0  0.000000  0.000000  0.000000        0.0   \n",
       "2  0.000000   0.000000          0.0  0.000000  0.000000  0.047101        0.0   \n",
       "3  0.026499   0.000000          0.0  0.115906  0.000000  0.028267        0.0   \n",
       "4  0.000000   0.000000          0.0  0.000000  0.204767  0.000000        0.0   \n",
       "\n",
       "   younger  zero  \n",
       "0      0.0   0.0  \n",
       "1      0.0   0.0  \n",
       "2      0.0   0.0  \n",
       "3      0.0   0.0  \n",
       "4      0.0   0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQnaB7ZobuTL"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "MxzX6yfXb3-k",
    "outputId": "bfac4200-ea23-4c05-b2ff-2d30f96daad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.939375\n",
      "Test Accuracy: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsnXRciEbunY"
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Q747C2ynb4b6",
    "outputId": "6c20e8ac-b681-428c-b9b2-0c8418a5e6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.89\n",
      "Test Accuracy: 0.8175\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heyQvFDxbu77"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "TLjVqdsHb5Gk",
    "outputId": "7786cab1-1178-47f2-9856-c1394a356eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.985625\n",
      "Test Accuracy: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7iTmvPBY04W"
   },
   "source": [
    "# Using NLTK to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LI4fS7ZUgA0c"
   },
   "source": [
    "## Importing the data fresh to avoid variable collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXl0jxRKgEba"
   },
   "outputs": [],
   "source": [
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(reviews, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "t_QH7TqlgNQ5",
    "outputId": "a32764ca-05fb-476e-d70a-99abb65aecdd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>way of the gun is brimming with surprises , some good , most bad . one of the good ones is ryan phillippe ' s surprisingly halfway decent performance . after the actor gained much attention by pos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i want to correct what i wrote last year in my retrospective of david lean ' s war picture . i still think that \" the bridge on the river kwai \" doesn ' t deserve being the number 13 in the americ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ingredients : man with amnesia who wakes up wanted for murder , dark science fiction city controlled by alien beings with mental powers . synopsis : what if you woke up one day , and suspected you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>note : some may consider portions of the following text to be spoilers . be forewarned . east meets west in mulan , the latest installment in disney ' s parade of annual animated feature films . a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as you should know , this summer has been less than memorable . with a total of 4 decent films , it ' s not a surprise that these big budget failures keep appearing . with that said , you can pret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "0  way of the gun is brimming with surprises , some good , most bad . one of the good ones is ryan phillippe ' s surprisingly halfway decent performance . after the actor gained much attention by pos...   \n",
       "1  i want to correct what i wrote last year in my retrospective of david lean ' s war picture . i still think that \" the bridge on the river kwai \" doesn ' t deserve being the number 13 in the americ...   \n",
       "2  ingredients : man with amnesia who wakes up wanted for murder , dark science fiction city controlled by alien beings with mental powers . synopsis : what if you woke up one day , and suspected you...   \n",
       "3  note : some may consider portions of the following text to be spoilers . be forewarned . east meets west in mulan , the latest installment in disney ' s parade of annual animated feature films . a...   \n",
       "4  as you should know , this summer has been less than memorable . with a total of 4 decent films , it ' s not a surprise that these big budget failures keep appearing . with that said , you can pret...   \n",
       "\n",
       "   sentiment  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "sentiments = []\n",
    "\n",
    "for review in reviews:\n",
    "  \n",
    "  # Add sentiment to list\n",
    "  if review[1] == \"pos\":\n",
    "    sentiments.append(1)\n",
    "  else:\n",
    "    sentiments.append(0)\n",
    "  \n",
    "  # Add text to list\n",
    "  review_text = \" \".join(review[0])\n",
    "  documents.append(review_text)\n",
    "  \n",
    "df = pd.DataFrame({\"text\": documents, \"sentiment\": sentiments})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EX1GfzLxKZhe"
   },
   "source": [
    "## Cleaning function to apply to each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "XX-1OOavJpug",
    "outputId": "039cc6f8-e7b4-49c0-d75d-d2c482f6bd33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[way, gun, brimming, surprises, good, bad, one, good, ones, ryan, phillippe, surprisingly, halfway, decent, performance, actor, gained, much, attention, posing, preening, teen, swill, like, know, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[want, correct, wrote, last, year, retrospective, david, lean, war, picture, still, think, bridge, river, kwai, deserve, number, american, film, institute, list, greatest, american, movies, think,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ingredients, man, amnesia, wakes, wanted, murder, dark, science, fiction, city, controlled, alien, beings, mental, powers, synopsis, woke, one, day, suspected, earth, instead, part, experiment, g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[note, may, consider, portions, following, text, spoilers, forewarned, east, meets, west, mulan, latest, installment, disney, parade, annual, animated, feature, films, odd, fusion, ancient, asian,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[know, summer, less, memorable, total, decent, films, surprise, big, budget, failures, keep, appearing, said, pretty, much, predict, opinion, warrior, film, based, michael, crichton, eaters, dead,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "0  [way, gun, brimming, surprises, good, bad, one, good, ones, ryan, phillippe, surprisingly, halfway, decent, performance, actor, gained, much, attention, posing, preening, teen, swill, like, know, ...   \n",
       "1  [want, correct, wrote, last, year, retrospective, david, lean, war, picture, still, think, bridge, river, kwai, deserve, number, american, film, institute, list, greatest, american, movies, think,...   \n",
       "2  [ingredients, man, amnesia, wakes, wanted, murder, dark, science, fiction, city, controlled, alien, beings, mental, powers, synopsis, woke, one, day, suspected, earth, instead, part, experiment, g...   \n",
       "3  [note, may, consider, portions, following, text, spoilers, forewarned, east, meets, west, mulan, latest, installment, disney, parade, annual, animated, feature, films, odd, fusion, ancient, asian,...   \n",
       "4  [know, summer, less, memorable, total, decent, films, surprise, big, budget, failures, keep, appearing, said, pretty, much, predict, opinion, warrior, film, based, michael, crichton, eaters, dead,...   \n",
       "\n",
       "   sentiment  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "df_nltk = pd.DataFrame()\n",
    "df_nltk['text'] = df.text.apply(clean_doc)\n",
    "df_nltk['sentiment'] = df.sentiment\n",
    "df_nltk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYQ1cyZgg7d8"
   },
   "source": [
    "## Reformat reviews for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "7Or1gRcRhOID",
    "outputId": "8435a535-f26e-4d18-b515-a8f674a2159d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>way gun brimming surprises good bad one good ones ryan phillippe surprisingly halfway decent performance actor gained much attention posing preening teen swill like know last summer hinted bit gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>want correct wrote last year retrospective david lean war picture still think bridge river kwai deserve number american film institute list greatest american movies think angry men witness prosecu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ingredients man amnesia wakes wanted murder dark science fiction city controlled alien beings mental powers synopsis woke one day suspected earth instead part experiment giant space terrarium mani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>note may consider portions following text spoilers forewarned east meets west mulan latest installment disney parade annual animated feature films odd fusion ancient asian traditions disconcerting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know summer less memorable total decent films surprise big budget failures keep appearing said pretty much predict opinion warrior film based michael crichton eaters dead ahmed ibn fahdlan banishe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text  \\\n",
       "0  way gun brimming surprises good bad one good ones ryan phillippe surprisingly halfway decent performance actor gained much attention posing preening teen swill like know last summer hinted bit gro...   \n",
       "1  want correct wrote last year retrospective david lean war picture still think bridge river kwai deserve number american film institute list greatest american movies think angry men witness prosecu...   \n",
       "2  ingredients man amnesia wakes wanted murder dark science fiction city controlled alien beings mental powers synopsis woke one day suspected earth instead part experiment giant space terrarium mani...   \n",
       "3  note may consider portions following text spoilers forewarned east meets west mulan latest installment disney parade annual animated feature films odd fusion ancient asian traditions disconcerting...   \n",
       "4  know summer less memorable total decent films surprise big budget failures keep appearing said pretty much predict opinion warrior film based michael crichton eaters dead ahmed ibn fahdlan banishe...   \n",
       "\n",
       "   sentiment  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for review in df_nltk.text:\n",
    "  review = \" \".join(review)\n",
    "  documents.append(review)\n",
    "  \n",
    "sentiment = list(df_nltk.sentiment)\n",
    "new_df = pd.DataFrame({'text': documents, 'sentiment': sentiment})\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq-ev_ggh9DU"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pfizhRrdGXr"
   },
   "outputs": [],
   "source": [
    "X = new_df.text\n",
    "y = new_df.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAB97oF2LPSy"
   },
   "source": [
    "## Vectorize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aQvOH2JnJNRL",
    "outputId": "7893339c-dc24-4122-f5d7-748e109cf8b8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "IUZR3p7KbHrZ",
    "outputId": "b05d46cb-cba4-4185-846a-9d3598277e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 35288)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaaaah</th>\n",
       "      <th>aaaaaaaahhhh</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aalyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zus</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zycie</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaaaaaaaah  aaaaaaaahhhh  aaaaaah  aaliyah  aalyah  aamir  \\\n",
       "0  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "1  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "2  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "3  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "4  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "\n",
       "   aardman  aaron  ...  zukovsky  zulu  zundel  zurg  zus  zweibel  zwick  \\\n",
       "0      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "1      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "2      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "3      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "4      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "\n",
       "   zwigoff  zycie  zzzzzzz  \n",
       "0      0.0    0.0      0.0  \n",
       "1      0.0    0.0      0.0  \n",
       "2      0.0    0.0      0.0  \n",
       "3      0.0    0.0      0.0  \n",
       "4      0.0    0.0      0.0  \n",
       "\n",
       "[5 rows x 35288 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "GtGOFiU5jIVK",
    "outputId": "28274147-b58b-4dcd-deaa-be0902426e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 35288)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaaaah</th>\n",
       "      <th>aaaaaaaahhhh</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aalyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zus</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zycie</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaaaaaaaah  aaaaaaaahhhh  aaaaaah  aaliyah  aalyah  aamir  \\\n",
       "0  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "1  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "2  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "3  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "4  0.0  0.0         0.0           0.0      0.0      0.0     0.0    0.0   \n",
       "\n",
       "   aardman  aaron  ...  zukovsky  zulu  zundel  zurg  zus  zweibel  zwick  \\\n",
       "0      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "1      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "2      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "3      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "4      0.0    0.0  ...       0.0   0.0     0.0   0.0  0.0      0.0    0.0   \n",
       "\n",
       "   zwigoff  zycie  zzzzzzz  \n",
       "0      0.0    0.0      0.0  \n",
       "1      0.0    0.0      0.0  \n",
       "2      0.0    0.0      0.0  \n",
       "3      0.0    0.0      0.0  \n",
       "4      0.0    0.0      0.0  \n",
       "\n",
       "[5 rows x 35288 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzW-8HmQjSxb"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "FGVMRawTjaZK",
    "outputId": "a2361530-64e7-4435-d183-1729eeb8e842"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.981875\n",
      "Test Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WrEhaJhjS5q"
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FNsAqhv_jscV",
    "outputId": "c751f3ff-f1ae-4bf0-d767-5e1bd8858d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.973125\n",
      "Test Accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f134n-eYjS-l"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Tl6OigFpjKkJ",
    "outputId": "aaeee2d3-9125-41f2-d1ba-61e05693da40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99125\n",
      "Test Accuracy: 0.7275\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "A-6kHJfSjvk0",
    "outputId": "95aea272-4e94-48d0-fc3e-b2a61f62f57f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-8d193ddbaae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "        #hyper params\n",
    "        n_jobs = -1,\n",
    ")\n",
    "\n",
    "clf.fit(X_train_vectorized, y_train_vectorized, eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5KMtpBxHmX9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_423_Document_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "US4-S1-NLP (Python 3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
