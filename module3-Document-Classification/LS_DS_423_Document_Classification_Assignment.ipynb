{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OJHr-tbuSuI"
   },
   "source": [
    "# Now it's your turn!\n",
    "\n",
    "Use the following dataset of scraped \"Data Scientist\" and \"Data Analyst\" job listings to create your own Document Classification Models.\n",
    "\n",
    "<https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv>\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Apply both CountVectorizer and TfidfVectorizer methods to this data and compare results\n",
    "- Use at least two different classification models to compare differences in model accuracy\n",
    "- Try to \"Hyperparameter Tune\" your model by using different n_gram ranges, max_results, and data cleaning methods\n",
    "- Try and get the highest accuracy possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFreAPN3uGgz"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv')\n",
    "jobs = jobs.drop_duplicates(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  b\"<div><div>Job Requirements:</div><ul><li><p>...   \n",
       "1  b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...   \n",
       "2  b'<div><p>As a Data Scientist you will be work...   \n",
       "3  b'<div class=\"jobsearch-JobMetadataHeader icl-...   \n",
       "4  b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...   \n",
       "\n",
       "                          title             job  \n",
       "0               Data scientistÂ   Data Scientist  \n",
       "1              Data Scientist I  Data Scientist  \n",
       "2  Data Scientist - Entry Level  Data Scientist  \n",
       "3                Data Scientist  Data Scientist  \n",
       "4                Data Scientist  Data Scientist  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop nulls, this isn't informative\n",
    "jobs = jobs.dropna(axis=0)\n",
    "jobs.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return re.sub(r'^b\\\"', '', cleantext)\n",
    "\n",
    "def stem_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_text = []\n",
    "    for sentence in sentences:\n",
    "        token_words=word_tokenize(sentence)\n",
    "        lemm_sentence=[]\n",
    "        for word in token_words:\n",
    "            lemm_sentence.append(lemmatizer.lemmatize(unidecode(word)))\n",
    "            lemm_sentence.append(\" \")\n",
    "        lemm_text.append(''.join(lemm_sentence))\n",
    "    return unidecode(''.join(lemm_text))\n",
    "\n",
    "jobs['description'] = jobs['description'].apply(cleanhtml).apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jobs.description\n",
    "y = jobs.job.map({'Data Scientist' : 1, 'Data Analyst' : 0})\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['x83', 'x98', 'xef', 'x83', 'xc2', 'xa8', 'xe2', 'x80', 'nc3',\n",
    "            'x99s', 'xa2'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'countvectorizer__max_features': 100, 'countvectorizer__ngram_range': (1, 2)}\n",
      "CV Score 0.8905325443786982\n",
      "Test Score 0.8588235294117647\n"
     ]
    }
   ],
   "source": [
    "lr_count = make_pipeline(CountVectorizer(stop_words=stop),\n",
    "                        LogisticRegression(solver='lbfgs',\n",
    "                                          max_iter=500))\n",
    "lr_grid_params = [{'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                  'countvectorizer__max_features' : [50, 100, None]}]\n",
    "\n",
    "lr_grid = GridSearchCV(lr_count, lr_grid_params, cv=3)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print ('Best Params', lr_grid.best_params_)\n",
    "print ('CV Score', lr_grid.best_score_)\n",
    "print ('Test Score', lr_grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'countvectorizer__max_features': 100, 'countvectorizer__ngram_range': (1, 2)}\n",
      "CV Score 0.9053254437869822\n",
      "Test Score 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "nb_count = make_pipeline(CountVectorizer(stop_words=stop),\n",
    "                        MultinomialNB())\n",
    "nb_grid_params = [{'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                  'countvectorizer__max_features' : [50, 100, None]}]\n",
    "\n",
    "nb_grid = GridSearchCV(nb_count, nb_grid_params, cv=3)\n",
    "nb_grid.fit(X_train, y_train)\n",
    "print ('Best Params', nb_grid.best_params_)\n",
    "print ('CV Score', nb_grid.best_score_)\n",
    "print ('Test Score', nb_grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'tfidfvectorizer__max_features': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "CV Score 0.8905325443786982\n",
      "Test Score 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf = make_pipeline(TfidfVectorizer(stop_words=stop),\n",
    "                        LogisticRegression(solver='lbfgs',\n",
    "                                          max_iter=500))\n",
    "lr_grid_params = [{'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                  'tfidfvectorizer__max_features' : [50, 100, None]}]\n",
    "lr_grid = GridSearchCV(lr_tfidf, lr_grid_params, cv=3)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print ('Best Params', lr_grid.best_params_)\n",
    "print ('CV Score', lr_grid.best_score_)\n",
    "print ('Test Score', lr_grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tfidf = make_pipeline(TfidfVectorizer(stop_words=stop),\n",
    "                        MultinomialNB())\n",
    "nb_grid_params = [{'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                  'tfidfvectorizer__max_features' : [50, 100, None]}]\n",
    "nb_grid = GridSearchCV(nb_tfidf, nb_grid_params, cv=3)\n",
    "nb_grid.fit(X_train, y_train)\n",
    "print ('Best Params', nb_grid.best_params_)\n",
    "print ('CV Score', nb_grid.best_score_)\n",
    "print ('Test Score', nb_grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlclSdSSveq-"
   },
   "source": [
    "# Stretch Goals\n",
    "\n",
    "- Try some agglomerative clustering using cosine-similarity-distance! (works better with high dimensional spaces) robust clustering - Agglomerative clustering like Ward would be cool. Try and create an awesome Dendrogram of the most important terms from the dataset.\n",
    "\n",
    "- Awesome resource for clustering stretch goals: \n",
    " - Agglomerative Clustering with Scipy: <https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/>\n",
    " - Agglomerative Clustering for NLP: <http://brandonrose.org/clustering>\n",
    " \n",
    "- Use Latent Dirichlet Allocation (LDA) to perform topic modeling on the dataset: \n",
    " - Topic Modeling and LDA in Python: <https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24>\n",
    " - Topic Modeling and LDA using Gensim: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_423_Document_Classification_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
