{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_423_Document_Classification_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donw385/DS-Unit-4-Sprint-2-NLP/blob/master/module3-Document-Classification/LS_DS_423_Document_Classification_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-OJHr-tbuSuI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now it's your turn!\n",
        "\n",
        "Use the following dataset of scraped \"Data Scientist\" and \"Data Analyst\" job listings to create your own Document Classification Models.\n",
        "\n",
        "<https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv>\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- Apply both CountVectorizer and TfidfVectorizer methods to this data and compare results\n",
        "- Use at least two different classification models to compare differences in model accuracy\n",
        "- Try to \"Hyperparameter Tune\" your model by using different n_gram ranges, max_results, and data cleaning methods\n",
        "- Try and get the highest accuracy possible!"
      ]
    },
    {
      "metadata": {
        "id": "SuJFr--7rRGc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "dfa94549-7481-4efa-97bf-be1ffc025661"
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MFreAPN3uGgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "610430b6-1ac0-4530-cc46-3641c6b79b30"
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "QhNzZtsdroEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "jobs = pd.read_csv('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv')\n",
        "jobs = jobs.drop_duplicates(subset='description')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Au1PyrH8r4MN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "806ba88d-d937-439f-c2be-bb61618c8af3"
      },
      "cell_type": "code",
      "source": [
        "jobs = jobs.dropna(axis=0)\n",
        "jobs.isnull().sum().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "AEDrxlScrvv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return re.sub(r'^b\\\"', '', cleantext)\n",
        "\n",
        "def stem_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemm_text = []\n",
        "    for sentence in sentences:\n",
        "        token_words=word_tokenize(sentence)\n",
        "        lemm_sentence=[]\n",
        "        for word in token_words:\n",
        "            lemm_sentence.append(lemmatizer.lemmatize(unidecode(word)))\n",
        "            lemm_sentence.append(\" \")\n",
        "        lemm_text.append(''.join(lemm_sentence))\n",
        "    return unidecode(''.join(lemm_text))\n",
        "\n",
        "jobs['description'] = jobs['description'].apply(cleanhtml).apply(stem_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zdeOKsssKag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = jobs.description\n",
        "y = jobs.job.map({'Data Scientist' : 1, 'Data Analyst' : 0})\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "stop.extend(['x83', 'x98', 'xef', 'x83', 'xc2', 'xa8', 'xe2', 'x80', 'nc3',\n",
        "            'x99s', 'xa2'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                   y,\n",
        "                                                   test_size=0.2,\n",
        "                                                   stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j66BiwtusNMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "8301e726-7cc1-4a0a-8eab-ea118815aedf"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "lr_count = make_pipeline(CountVectorizer(stop_words=stop),\n",
        "                        LogisticRegression(solver='lbfgs',\n",
        "                                          max_iter=500))\n",
        "lr_grid_params = [{'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
        "                  'countvectorizer__max_features' : [50, 100, None]}]\n",
        "\n",
        "lr_grid = GridSearchCV(lr_count, lr_grid_params, cv=3)\n",
        "lr_grid.fit(X_train, y_train)\n",
        "print ('Best Params', lr_grid.best_params_)\n",
        "print ('CV Score', lr_grid.best_score_)\n",
        "print ('Test Score', lr_grid.score(X_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params {'countvectorizer__max_features': None, 'countvectorizer__ngram_range': (1, 2)}\n",
            "CV Score 0.9201183431952663\n",
            "Test Score 0.8470588235294118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CuJp8V4ssRPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a9ce6da7-f0e9-4808-af92-4af5a5a4fb87"
      },
      "cell_type": "code",
      "source": [
        "nb_count = make_pipeline(CountVectorizer(stop_words=stop),\n",
        "                        MultinomialNB())\n",
        "nb_grid_params = [{'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
        "                  'countvectorizer__max_features' : [50, 100, None]}]\n",
        "\n",
        "nb_grid = GridSearchCV(nb_count, nb_grid_params, cv=3)\n",
        "nb_grid.fit(X_train, y_train)\n",
        "print ('Best Params', nb_grid.best_params_)\n",
        "print ('CV Score', nb_grid.best_score_)\n",
        "print ('Test Score', nb_grid.score(X_test, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params {'countvectorizer__max_features': 100, 'countvectorizer__ngram_range': (1, 2)}\n",
            "CV Score 0.9201183431952663\n",
            "Test Score 0.8705882352941177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KGWE_PxxsT5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "876ae4d5-ba01-441c-bd96-4d17c8235fdb"
      },
      "cell_type": "code",
      "source": [
        "lr_tfidf = make_pipeline(TfidfVectorizer(stop_words=stop),\n",
        "                        LogisticRegression(solver='lbfgs',\n",
        "                                          max_iter=500))\n",
        "lr_grid_params = [{'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
        "                  'tfidfvectorizer__max_features' : [50, 100, None]}]\n",
        "lr_grid = GridSearchCV(lr_tfidf, lr_grid_params, cv=3)\n",
        "lr_grid.fit(X_train, y_train)\n",
        "print ('Best Params', lr_grid.best_params_)\n",
        "print ('CV Score', lr_grid.best_score_)\n",
        "print ('Test Score', lr_grid.score(X_test, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Params {'tfidfvectorizer__max_features': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
            "CV Score 0.9142011834319527\n",
            "Test Score 0.8588235294117647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bnrLJJz3sWaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4bbba1a1-16bd-47c8-d10c-14cb54b11faa"
      },
      "cell_type": "code",
      "source": [
        "nb_tfidf = make_pipeline(TfidfVectorizer(stop_words=stop),\n",
        "                        MultinomialNB())\n",
        "nb_grid_params = [{'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
        "                  'tfidfvectorizer__max_features' : [50, 100, None]}]\n",
        "nb_grid = GridSearchCV(nb_tfidf, nb_grid_params, cv=3)\n",
        "nb_grid.fit(X_train, y_train)\n",
        "print ('Best Params', nb_grid.best_params_)\n",
        "print ('CV Score', nb_grid.best_score_)\n",
        "print ('Test Score', nb_grid.score(X_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params {'tfidfvectorizer__max_features': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
            "CV Score 0.9112426035502958\n",
            "Test Score 0.8705882352941177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mwiuo3LBsZSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6c2a5d9-7124-41db-f950-18d4bf25af83"
      },
      "cell_type": "code",
      "source": [
        "nb_pipe = make_pipeline(TfidfVectorizer(stop_words=stop,\n",
        "                                       max_features=100,\n",
        "                                       ngram_range=(1,2)),\n",
        "                       LogisticRegression(solver='lbfgs',\n",
        "                                          max_iter=500))\n",
        "nb_pipe.fit(X_train, y_train)\n",
        "print ('Train Score', nb_pipe.score(X_train, y_train))\n",
        "print ('Test Score', nb_pipe.score(X_test, y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score 0.9378698224852071\n",
            "Test Score 0.8588235294117647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PDzN6K6lsefZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "896657f3-f823-40c0-a9a9-5d8e3eec4773"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "svc_pipe = make_pipeline(TfidfVectorizer(stop_words=stop,\n",
        "                                       max_features=100,\n",
        "                                       ngram_range=(1,2)),\n",
        "                       LinearSVC())\n",
        "# svc_pipe.fit(X_train, y_train)\n",
        "# print ('Train Score', svc_pipe.score(X_train, y_train))\n",
        "# print ('Test Score', svc_pipe.score(X_test, y_test))\n",
        "\n",
        "cross_val_score(svc_pipe, X, y, scoring='roc_auc', cv=5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95016611, 0.96843854, 0.96899225, 0.98014748, 0.95632445])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "vlclSdSSveq-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goals\n",
        "\n",
        "- Try some agglomerative clustering using cosine-similarity-distance! (works better with high dimensional spaces) robust clustering - Agglomerative clustering like Ward would be cool. Try and create an awesome Dendrogram of the most important terms from the dataset.\n",
        "\n",
        "- Awesome resource for clustering stretch goals: \n",
        " - Agglomerative Clustering with Scipy: <https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/>\n",
        " - Agglomerative Clustering for NLP: <http://brandonrose.org/clustering>\n",
        " \n",
        "- Use Latent Dirichlet Allocation (LDA) to perform topic modeling on the dataset: \n",
        " - Topic Modeling and LDA in Python: <https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24>\n",
        " - Topic Modeling and LDA using Gensim: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>\n"
      ]
    }
  ]
}