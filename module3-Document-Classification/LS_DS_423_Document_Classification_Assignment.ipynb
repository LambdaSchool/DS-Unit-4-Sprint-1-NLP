{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OJHr-tbuSuI"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "# Document Classification\n",
    "\n",
    "Use the following dataset of scraped \"Data Scientist\" and \"Data Analyst\" job listings to create your own Document Classification Models.\n",
    "\n",
    "<https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv>\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Apply both CountVectorizer and TfidfVectorizer methods to this data and compare results\n",
    "- Use at least two different classification models to compare differences in model accuracy\n",
    "- Try to \"Hyperparameter Tune\" your model by using different n_gram ranges, max_results, and data cleaning methods\n",
    "- Try and get the highest accuracy possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T00:01:20.451219Z",
     "start_time": "2019-05-02T00:01:17.004724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/superio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/superio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# !pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T02:37:00.940985Z",
     "start_time": "2019-05-02T02:36:59.312342Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MFreAPN3uGgz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  b\"<div><div>Job Requirements:</div><ul><li><p>...   \n",
       "1  b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...   \n",
       "2  b'<div><p>As a Data Scientist you will be work...   \n",
       "3  b'<div class=\"jobsearch-JobMetadataHeader icl-...   \n",
       "4  b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...   \n",
       "\n",
       "                          title             job  \n",
       "0               Data scientist   Data Scientist  \n",
       "1              Data Scientist I  Data Scientist  \n",
       "2  Data Scientist - Entry Level  Data Scientist  \n",
       "3                Data Scientist  Data Scientist  \n",
       "4                Data Scientist  Data Scientist  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T02:41:16.479094Z",
     "start_time": "2019-05-02T02:41:16.421252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>job</th>\n",
       "      <th>tokenized_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Requirements: Conceptual understanding in ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[job, requirements, conceptual, understanding,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Description  As a Data Scientist 1, you wi...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[job, description, data, scientist, help, us, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a Data Scientist you will be working on con...</td>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[data, scientist, working, consulting, side, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$4,969 - $6,756 a monthContractUnder the gener...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[monthcontractunder, general, supervision, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location: USA \\xe2\\x80\\x93 multiple locations ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[location, usa, multiple, locations, years, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Job Requirements: Conceptual understanding in ...   \n",
       "1  Job Description  As a Data Scientist 1, you wi...   \n",
       "2  As a Data Scientist you will be working on con...   \n",
       "3  $4,969 - $6,756 a monthContractUnder the gener...   \n",
       "4  Location: USA \\xe2\\x80\\x93 multiple locations ...   \n",
       "\n",
       "                          title             job  \\\n",
       "0               Data scientist   Data Scientist   \n",
       "1              Data Scientist I  Data Scientist   \n",
       "2  Data Scientist - Entry Level  Data Scientist   \n",
       "3                Data Scientist  Data Scientist   \n",
       "4                Data Scientist  Data Scientist   \n",
       "\n",
       "                               tokenized_description  \n",
       "0  [job, requirements, conceptual, understanding,...  \n",
       "1  [job, description, data, scientist, help, us, ...  \n",
       "2  [data, scientist, working, consulting, side, b...  \n",
       "3  [monthcontractunder, general, supervision, pro...  \n",
       "4  [location, usa, multiple, locations, years, an...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing numbers and comma before description text\n",
    "def clean_columns(columns):\n",
    "    for i in columns:\n",
    "        df[i] = df[i].str.lstrip('1234567890,').str.strip('b\\\"\\'')\n",
    "        df[i] = df[i].str.replace('<[^<]+?>', '')\n",
    "        df[i] = df[i].replace('\\n','', regex=False)\n",
    "        df[i] = df[i].apply(lambda x:str(x).replace('\\\\n', ' '))\n",
    "        df[i] = df[i].apply(lambda x:str(x).replace('\\xe2\\x80\\x93', ' '))\n",
    "clean_columns(['description', 'title'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T02:41:40.499468Z",
     "start_time": "2019-05-02T02:41:36.603415Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>job</th>\n",
       "      <th>tokenized_description</th>\n",
       "      <th>tokenized_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job Requirements: Conceptual understanding in ...</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[job, requirements, conceptual, understanding,...</td>\n",
       "      <td>[data, scientist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Description  As a Data Scientist 1, you wi...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[job, description, data, scientist, help, us, ...</td>\n",
       "      <td>[data, scientist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a Data Scientist you will be working on con...</td>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[data, scientist, working, consulting, side, b...</td>\n",
       "      <td>[data, scientist, entry, level]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$4,969 - $6,756 a monthContractUnder the gener...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[monthcontractunder, general, supervision, pro...</td>\n",
       "      <td>[data, scientist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location: USA \\xe2\\x80\\x93 multiple locations ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[location, usa, multiple, locations, years, an...</td>\n",
       "      <td>[data, scientist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Job Requirements: Conceptual understanding in ...   \n",
       "1  Job Description  As a Data Scientist 1, you wi...   \n",
       "2  As a Data Scientist you will be working on con...   \n",
       "3  $4,969 - $6,756 a monthContractUnder the gener...   \n",
       "4  Location: USA \\xe2\\x80\\x93 multiple locations ...   \n",
       "\n",
       "                          title             job  \\\n",
       "0               Data scientist   Data Scientist   \n",
       "1              Data Scientist I  Data Scientist   \n",
       "2  Data Scientist - Entry Level  Data Scientist   \n",
       "3                Data Scientist  Data Scientist   \n",
       "4                Data Scientist  Data Scientist   \n",
       "\n",
       "                               tokenized_description  \\\n",
       "0  [job, requirements, conceptual, understanding,...   \n",
       "1  [job, description, data, scientist, help, us, ...   \n",
       "2  [data, scientist, working, consulting, side, b...   \n",
       "3  [monthcontractunder, general, supervision, pro...   \n",
       "4  [location, usa, multiple, locations, years, an...   \n",
       "\n",
       "                   tokenized_title  \n",
       "0                [data, scientist]  \n",
       "1                [data, scientist]  \n",
       "2  [data, scientist, entry, level]  \n",
       "3                [data, scientist]  \n",
       "4                [data, scientist]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FROM LECTURE\n",
    "import string\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(texts):\n",
    "    # Tokenize by word\n",
    "    tokens = word_tokenize(texts)\n",
    "#     print(\"Tokens:\", tokens)\n",
    "    # Make all words lowercase\n",
    "    lowercase_tokens = [w.lower() for w in tokens]\n",
    "#     print(\"Lowercase:\", lowercase_tokens)\n",
    "    # Strip punctuation from within words\n",
    "    no_punctuation = [x.translate(table) for x in lowercase_tokens]\n",
    "#     print(\"No Punctuation:\", no_punctuation)\n",
    "    # Remove words that aren't alphabetic\n",
    "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
    "#     print(\"Alphabetic:\", alphabetic)\n",
    "    # Remove stopwords\n",
    "    words = [w for w in alphabetic if not w in stop_words]\n",
    "#     print(\"Cleaned Words:\", words)\n",
    "    return words\n",
    "\n",
    "df['tokenized_description'] = df['description'].apply(tokenize)\n",
    "df['tokenized_title'] = df['title'].apply(tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T03:03:48.263267Z",
     "start_time": "2019-05-02T03:03:48.240240Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) (100,) (400,) (100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X = df.dropna(axis=0).description\n",
    "y = df.dropna(axis=0).job.map({'Data Scientist' : 1, 'Data Analyst' : 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T03:03:54.102428Z",
     "start_time": "2019-05-02T03:03:54.075275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249    Company Overview  Digital Assets Data is a lea...\n",
       "433    Position: Data Analyst  Job Purpose: Na Ali\\xe...\n",
       "19     $70,000 - $100,000 a yearTitle: Data Analyst/J...\n",
       "322    This position will report directly to the Data...\n",
       "332    TemporaryData Analyst Job #: req2358  Organiza...\n",
       "56     Temporary, InternshipDescription:  The Data An...\n",
       "301    ContractJob Description  Job Summary This is a...\n",
       "229    Expedia Do you wish for the opportunity to tra...\n",
       "331    Essential Functions: The Data Visualization An...\n",
       "132    Invitae envisions a world in which genomic seq...\n",
       "137    We\\xe2\\x80\\x99re Elliott Davis, a rapidly grow...\n",
       "423    Responsible for analytic data needs of the bus...\n",
       "335    Trinity Industries is searching for a passiona...\n",
       "25     As a Data Scientist for Ads Measurement in the...\n",
       "464    Provides subject matter expertise to departmen...\n",
       "281    General Summary: You will be responsible for h...\n",
       "247    Position Description you need to have PhD in c...\n",
       "237    Microsoft envisions a world where passionate i...\n",
       "117    Company Details  Founded in 1967, W. R. Berkle...\n",
       "42     Job Description  5-10 years hands-on experienc...\n",
       "220    $78,600 a yearJob Description  Data Analytics ...\n",
       "176    About the Company  Civis Analytics helps leadi...\n",
       "320    Gateway Blend is a St. Louis based online publ...\n",
       "153    The Principal Data Scientist is responsible fo...\n",
       "231    Position Overview Our Oil and Gas division is ...\n",
       "227    Agency and Job Overview  We are OMD, an award-...\n",
       "417    The Data Analyst role at Boss Fight is tightly...\n",
       "203    Proficiency in synthesizing and interpreting d...\n",
       "126    Job description:  Data Scientist positions off...\n",
       "329    OUTFRONT Media\\xe2\\x80\\x99s Information Techno...\n",
       "                             ...                        \n",
       "276    $45,000 a yearGrowing Medical Billing company ...\n",
       "443    NEW YORK, NY  Guardian Debt Relief is looking ...\n",
       "191    At Uber, we ignite opportunity by setting the ...\n",
       "385    Summary Posted: Mar 21, 2019 Role Number: 2000...\n",
       "293    The Role Come help our customers learn more ab...\n",
       "413    InternshipWhat you will do Johnson Controls in...\n",
       "343    POSITION SUMMARY:  Responsible for HIM chart r...\n",
       "257    As a Data Analyst, you will help build TrueMot...\n",
       "308    Abbott is a global healthcare leader that help...\n",
       "149    $27.17 - $36.60 an hourAs a Data Scientist Gra...\n",
       "130    Who You Are -----------  You are a full-stack ...\n",
       "151    The Data Scientist is a critical position with...\n",
       "359    The Research Data Analyst will join a collabor...\n",
       "99     Associate Statistical Modeler conducts statist...\n",
       "372    Morgan Stanley Services Group Inc. seeks an As...\n",
       "87     ContractWE ARE A TRANSFORMATIONAL PARTNER We m...\n",
       "458    Purpose Under general direction, incorporate d...\n",
       "330    Data Analyst for San Francisco, CA to create d...\n",
       "214    About Uber We\\xe2\\x80\\x99re changing the way p...\n",
       "466    InternshipSamsara hardware products are plug a...\n",
       "121    Part-timeThe Nittany Artificial Intelligence (...\n",
       "499    Location: El Segundo, California, United State...\n",
       "20     $45,000 a yearWorking under direction of the A...\n",
       "188    Conducts research using advanced statistical a...\n",
       "71     The Data Scientist will be responsible for mod...\n",
       "106    We are looking for a senior level data scienti...\n",
       "270    Jr. Data Analyst Position Summary: The Data An...\n",
       "348    Part-time, InternshipElement Data is looking f...\n",
       "435    Responsible for data acquisition and ongoing d...\n",
       "102    About the Company  Civis Analytics is a techno...\n",
       "Name: description, Length: 400, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T03:04:04.431800Z",
     "start_time": "2019-05-02T03:04:04.143191Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'abstracting', 'abstraction', 'abstractions', 'abstractly', 'abstracts', 'abundant', 'abuse', 'aca', 'academi', 'academia', 'academic', 'academies', 'accelerate', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accept', 'acceptability', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.get_feature_names()[300:325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T03:14:31.277868Z",
     "start_time": "2019-05-02T03:14:30.965345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 8706)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011236</th>\n",
       "      <th>00079</th>\n",
       "      <th>00805</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02115</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>zetahub</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zogsports</th>\n",
       "      <th>zoho</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zywave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00011236  00079  00805  00am  00pm  01  02115  03  ...  zetahub  \\\n",
       "0   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "1   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "2   0    2         0      0      0     0     0   0      0   0  ...        0   \n",
       "3   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "4   0    1         0      0      0     0     0   0      0   0  ...        0   \n",
       "\n",
       "   zeus  zheng  zillow  zogsports  zoho  zone  zones  zoom  zywave  \n",
       "0     0      0       0          0     0     0      0     0       0  \n",
       "1     0      0       0          0     0     0      0     0       0  \n",
       "2     0      0       0          0     0     0      0     0       0  \n",
       "3     0      0       0          0     0     0      0     0       0  \n",
       "4     0      0       0          0     0     0      0     0       0  \n",
       "\n",
       "[5 rows x 8706 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T03:16:12.004970Z",
     "start_time": "2019-05-02T03:16:11.861468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8706)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011236</th>\n",
       "      <th>00079</th>\n",
       "      <th>00805</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02115</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>zetahub</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zogsports</th>\n",
       "      <th>zoho</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zywave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00011236  00079  00805  00am  00pm  01  02115  03  ...  zetahub  \\\n",
       "0   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "1   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "2   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "3   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "4   0    0         0      0      0     0     0   0      0   0  ...        0   \n",
       "\n",
       "   zeus  zheng  zillow  zogsports  zoho  zone  zones  zoom  zywave  \n",
       "0     0      0       0          0     0     0      0     0       0  \n",
       "1     0      0       0          0     0     0      0     0       0  \n",
       "2     0      0       0          0     0     0      0     0       0  \n",
       "3     0      0       0          0     0     0      0     0       0  \n",
       "4     0      0       0          0     0     0      0     0       0  \n",
       "\n",
       "[5 rows x 8706 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:44:53.717484Z",
     "start_time": "2019-05-02T04:44:53.707391Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:44:56.224701Z",
     "start_time": "2019-05-02T04:44:56.149763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9725\n",
      "Test Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superio/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "columns = ['model', 'acc_train', 'acc_test', 'vect']\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Logistic Regression'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:44:58.971918Z",
     "start_time": "2019-05-02T04:44:58.825785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.965\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Multinomial Naive Bayes'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:02.503677Z",
     "start_time": "2019-05-02T04:45:02.286097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99\n",
      "Test Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superio/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Random Forest'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:04.874268Z",
     "start_time": "2019-05-02T04:45:04.848945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                    model vect_type\n",
       "0      0.88     0.9725      Logistic Regression     Count\n",
       "1      0.86     0.9650  Multinomial Naive Bayes     Count\n",
       "2      0.82     0.9900            Random Forest     Count"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:41:55.487049Z",
     "start_time": "2019-05-02T04:41:55.479489Z"
    }
   },
   "source": [
    "# TF-IDF Vectorization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:11.040001Z",
     "start_time": "2019-05-02T04:45:10.730737Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:43:14.868292Z",
     "start_time": "2019-05-02T04:43:14.864580Z"
    }
   },
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:13.753317Z",
     "start_time": "2019-05-02T04:45:13.407964Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 8706)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011236</th>\n",
       "      <th>00079</th>\n",
       "      <th>00805</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02115</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>zetahub</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zogsports</th>\n",
       "      <th>zoho</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zywave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000  00011236  00079  00805  00am  00pm   01  02115   03  ...  \\\n",
       "0  0.0  0.000000       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "1  0.0  0.000000       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "2  0.0  0.253112       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "3  0.0  0.000000       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "4  0.0  0.015946       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "\n",
       "   zetahub  zeus  zheng  zillow  zogsports  zoho  zone  zones  zoom  zywave  \n",
       "0      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "1      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "2      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "3      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "4      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 8706 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:17.484367Z",
     "start_time": "2019-05-02T04:45:17.324083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8706)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00011236</th>\n",
       "      <th>00079</th>\n",
       "      <th>00805</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02115</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>zetahub</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zogsports</th>\n",
       "      <th>zoho</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zywave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00011236  00079  00805  00am  00pm   01  02115   03  ...  \\\n",
       "0  0.0  0.0       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "1  0.0  0.0       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "2  0.0  0.0       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "3  0.0  0.0       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "4  0.0  0.0       0.0    0.0    0.0   0.0   0.0  0.0    0.0  0.0  ...   \n",
       "\n",
       "   zetahub  zeus  zheng  zillow  zogsports  zoho  zone  zones  zoom  zywave  \n",
       "0      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "1      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "2      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "3      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "4      0.0   0.0    0.0     0.0        0.0   0.0   0.0    0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 8706 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:22.791479Z",
     "start_time": "2019-05-02T04:45:22.674958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9725\n",
      "Test Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superio/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Logistic'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:45:57.648235Z",
     "start_time": "2019-05-02T04:45:57.514371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.965\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Naive Bayes'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:46:44.294483Z",
     "start_time": "2019-05-02T04:46:44.053182Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superio/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99\n",
      "Test Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Random Forest'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:50:12.992790Z",
     "start_time": "2019-05-02T04:50:04.537217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9925\n",
      "Test Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "XGB = XGBClassifier(n_jobs = -1).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = XGB.predict(X_train_vectorized)\n",
    "test_predictions = XGB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Xgboost'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Tfidf'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T04:50:26.331031Z",
     "start_time": "2019-05-02T04:50:26.302980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>Xgboost</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                    model vect_type\n",
       "0      0.88     0.9725      Logistic Regression     Count\n",
       "1      0.86     0.9650  Multinomial Naive Bayes     Count\n",
       "2      0.82     0.9900            Random Forest     Count\n",
       "3      0.88     0.9725                 Logistic     Tfidf\n",
       "4      0.86     0.9650              Naive Bayes     Tfidf\n",
       "5      0.79     0.9900            Random Forest     Tfidf\n",
       "6      0.93     0.9925                  Xgboost     Tfidf"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlclSdSSveq-"
   },
   "source": [
    "# Stretch Goals\n",
    "\n",
    "- Try some agglomerative clustering using cosine-similarity-distance! (works better with high dimensional spaces) robust clustering - Agglomerative clustering like Ward would be cool. Try and create an awesome Dendrogram of the most important terms from the dataset.\n",
    "\n",
    "- Awesome resource for clustering stretch goals: \n",
    " - Agglomerative Clustering with Scipy: <https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/>\n",
    " - Agglomerative Clustering for NLP: <http://brandonrose.org/clustering>\n",
    " \n",
    "- Use Latent Dirichlet Allocation (LDA) to perform topic modeling on the dataset: \n",
    " - Topic Modeling and LDA in Python: <https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24>\n",
    " - Topic Modeling and LDA using Gensim: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_423_Document_Classification_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
