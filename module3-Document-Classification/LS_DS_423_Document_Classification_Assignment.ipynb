{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_423_Document_Classification_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brit228/DS-Unit-4-Sprint-2-NLP/blob/master/module3-Document-Classification/LS_DS_423_Document_Classification_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-OJHr-tbuSuI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now it's your turn!\n",
        "\n",
        "Use the following dataset of scraped \"Data Scientist\" and \"Data Analyst\" job listings to create your own Document Classification Models.\n",
        "\n",
        "<https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv>\n",
        "\n",
        "Requirements:\n",
        "\n",
        "- Apply both CountVectorizer and TfidfVectorizer methods to this data and compare results\n",
        "- Use at least two different classification models to compare differences in model accuracy\n",
        "- Try to \"Hyperparameter Tune\" your model by using different n_gram ranges, max_results, and data cleaning methods\n",
        "- Try and get the highest accuracy possible!"
      ]
    },
    {
      "metadata": {
        "id": "MFreAPN3uGgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ebde0ae7-5600-4afd-8097-dac732dec1df"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import numpy as np\n",
        "\n",
        "def text_with_newlines(elem):\n",
        "  text = ''\n",
        "  for e in elem.descendants:\n",
        "    if isinstance(e, str):\n",
        "      text += e.strip()\n",
        "    elif e.name == 'br' or e.name == 'p':\n",
        "      text += '\\n'\n",
        "  return text.replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "def get_text(v):\n",
        "  soup = BeautifulSoup(v, 'html.parser')\n",
        "  if soup.find() == None:\n",
        "    return v.replace(\"\\\\n\", \"\\n\")\n",
        "  return \"\\n\".join([text_with_newlines(c) for c in soup.find_all(recursive=False)])\n",
        "\n",
        "def split_lemma(v):\n",
        "  stop_words = stopwords.words('english')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return [lemmatizer.lemmatize(w).lower() for w in word_tokenize(get_text(v)) if w.isalpha() and w not in stop_words and len(w) > 1]\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv\")\n",
        "df[\"description\"] = df[\"description\"].apply(lambda v: split_lemma(v) if v is not np.nan else '')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n",
            "Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5kO7Wo4i_3Fl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=df.sample(frac=0.8,random_state=200)\n",
        "test=df.drop(train.index)\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "bag_of_words = tfidf.fit_transform([\" \".join(v) for v in train[\"description\"].values])\n",
        "bag_of_words_test = tfidf.transform([\" \".join(v) for v in test[\"description\"].values])\n",
        "\n",
        "train_vec = pd.DataFrame(bag_of_words.toarray(), columns=tfidf.get_feature_names(), index=train.index)\n",
        "train_vec[\"DataRole\"] = train[\"job\"]\n",
        "\n",
        "test_vec = pd.DataFrame(bag_of_words_test.toarray(), columns=tfidf.get_feature_names(), index=test.index)\n",
        "test_vec[\"DataRole\"] = test[\"job\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7--DXyacNqVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqH9NudsFKCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c4e947be-c524-414f-8a72-35bc7dfd4b88"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression(random_state=42).fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
        "print(LR.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
        "print(roc_auc_score(test_vec[\"DataRole\"], LR.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.89\n",
            "0.9483173076923077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rlEof5jdNbao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8885697-564e-43f4-ea7a-af2cf47fe777"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB().fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
        "print(mnb.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
        "print(roc_auc_score(test_vec[\"DataRole\"], mnb.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.89\n",
            "0.939903846153846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DeqGWy8BPhSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "128533b0-d9c5-4bd5-c5ad-64d2995720b5"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RFC = RandomForestClassifier(n_estimators=200).fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
        "print(RFC.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
        "print(roc_auc_score(test_vec[\"DataRole\"], RFC.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.91\n",
            "0.9651442307692308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vlclSdSSveq-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stretch Goals\n",
        "\n",
        "- Try some agglomerative clustering using cosine-similarity-distance! (works better with high dimensional spaces) robust clustering - Agglomerative clustering like Ward would be cool. Try and create an awesome Dendrogram of the most important terms from the dataset.\n",
        "\n",
        "- Awesome resource for clustering stretch goals: \n",
        " - Agglomerative Clustering with Scipy: <https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/>\n",
        " - Agglomerative Clustering for NLP: <http://brandonrose.org/clustering>\n",
        " \n",
        "- Use Latent Dirichlet Allocation (LDA) to perform topic modeling on the dataset: \n",
        " - Topic Modeling and LDA in Python: <https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24>\n",
        " - Topic Modeling and LDA using Gensim: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>\n"
      ]
    },
    {
      "metadata": {
        "id": "5J8pYxx57EmS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}