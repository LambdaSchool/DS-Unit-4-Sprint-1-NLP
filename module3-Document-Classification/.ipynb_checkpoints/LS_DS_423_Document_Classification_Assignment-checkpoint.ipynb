{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brit228/DS-Unit-4-Sprint-2-NLP/blob/master/module3-Document-Classification/LS_DS_423_Document_Classification_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OJHr-tbuSuI"
   },
   "source": [
    "# Now it's your turn!\n",
    "\n",
    "Use the following dataset of scraped \"Data Scientist\" and \"Data Analyst\" job listings to create your own Document Classification Models.\n",
    "\n",
    "<https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv>\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Apply both CountVectorizer and TfidfVectorizer methods to this data and compare results\n",
    "- Use at least two different classification models to compare differences in model accuracy\n",
    "- Try to \"Hyperparameter Tune\" your model by using different n_gram ranges, max_results, and data cleaning methods\n",
    "- Try and get the highest accuracy possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "MFreAPN3uGgz",
    "outputId": "ebde0ae7-5600-4afd-8097-dac732dec1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import numpy as np\n",
    "\n",
    "def text_with_newlines(elem):\n",
    "  text = ''\n",
    "  for e in elem.descendants:\n",
    "    if isinstance(e, str):\n",
    "      text += e.strip()\n",
    "    elif e.name == 'br' or e.name == 'p':\n",
    "      text += '\\n'\n",
    "  return text.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "def get_text(v):\n",
    "  soup = BeautifulSoup(v, 'html.parser')\n",
    "  if soup.find() == None:\n",
    "    return v.replace(\"\\\\n\", \"\\n\")\n",
    "  return \"\\n\".join([text_with_newlines(c) for c in soup.find_all(recursive=False)])\n",
    "\n",
    "def split_lemma(v):\n",
    "  stop_words = stopwords.words('english')\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return [lemmatizer.lemmatize(w).lower() for w in word_tokenize(get_text(v)) if w.isalpha() and w not in stop_words and len(w) > 1]\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module3-Document-Classification/job_listings.csv\")\n",
    "df[\"description\"] = df[\"description\"].apply(lambda v: split_lemma(v) if v is not np.nan else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kO7Wo4i_3Fl"
   },
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.8,random_state=200)\n",
    "test=df.drop(train.index)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "bag_of_words = tfidf.fit_transform([\" \".join(v) for v in train[\"description\"].values])\n",
    "bag_of_words_test = tfidf.transform([\" \".join(v) for v in test[\"description\"].values])\n",
    "\n",
    "train_vec = pd.DataFrame(bag_of_words.toarray(), columns=tfidf.get_feature_names(), index=train.index)\n",
    "train_vec[\"DataRole\"] = train[\"job\"]\n",
    "\n",
    "test_vec = pd.DataFrame(bag_of_words_test.toarray(), columns=tfidf.get_feature_names(), index=test.index)\n",
    "test_vec[\"DataRole\"] = test[\"job\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7--DXyacNqVf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "kqH9NudsFKCc",
    "outputId": "c4e947be-c524-414f-8a72-35bc7dfd4b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n",
      "0.9483173076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
    "print(LR.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
    "print(roc_auc_score(test_vec[\"DataRole\"], LR.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rlEof5jdNbao",
    "outputId": "c8885697-564e-43f4-ea7a-af2cf47fe777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n",
      "0.939903846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB().fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
    "print(mnb.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
    "print(roc_auc_score(test_vec[\"DataRole\"], mnb.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DeqGWy8BPhSC",
    "outputId": "128533b0-d9c5-4bd5-c5ad-64d2995720b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "0.9651442307692308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=200).fit(train_vec[[c for c in train_vec if c is not \"DataRole\"]], train_vec[\"DataRole\"])\n",
    "print(RFC.score(test_vec[[c for c in test_vec if c is not \"DataRole\"]], test_vec[\"DataRole\"]))\n",
    "print(roc_auc_score(test_vec[\"DataRole\"], RFC.predict_proba(test_vec[[c for c in test_vec if c is not \"DataRole\"]])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlclSdSSveq-"
   },
   "source": [
    "# Stretch Goals\n",
    "\n",
    "- Try some agglomerative clustering using cosine-similarity-distance! (works better with high dimensional spaces) robust clustering - Agglomerative clustering like Ward would be cool. Try and create an awesome Dendrogram of the most important terms from the dataset.\n",
    "\n",
    "- Awesome resource for clustering stretch goals: \n",
    " - Agglomerative Clustering with Scipy: <https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/>\n",
    " - Agglomerative Clustering for NLP: <http://brandonrose.org/clustering>\n",
    " \n",
    "- Use Latent Dirichlet Allocation (LDA) to perform topic modeling on the dataset: \n",
    " - Topic Modeling and LDA in Python: <https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24>\n",
    " - Topic Modeling and LDA using Gensim: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5J8pYxx57EmS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LS_DS_423_Document_Classification_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
