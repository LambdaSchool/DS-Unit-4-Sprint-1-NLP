{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    " ##### Your Code Here #####\n",
    "    \n",
    "reg_space_string = \" \".join(whitespace_string.split())\n",
    "\n",
    "print(reg_space_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string = \"\"\"\n",
    "March 8, 2015\n",
    "March 15, 2015\n",
    "March 22, 2015\n",
    "March 29, 2015\n",
    "April 5, 2015\n",
    "April 12, 2015\n",
    "April 19, 2015\n",
    "April 26, 2015\n",
    "May 3, 2015\n",
    "May 10, 2015\n",
    "May 17, 2015\n",
    "May 24, 2015\n",
    "May 31, 2015\n",
    "June 7, 2015\n",
    "June 14, 2015\n",
    "June 21, 2015\n",
    "June 28, 2015\n",
    "July 5, 2015\n",
    "July 12, 2015\n",
    "July 19, 2015\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('March', '8', ', ', '2015'), ('March', '15', ', ', '2015'), ('March', '22', ', ', '2015'), ('March', '29', ', ', '2015'), ('April', '5', ', ', '2015'), ('April', '12', ', ', '2015'), ('April', '19', ', ', '2015'), ('April', '26', ', ', '2015'), ('May', '3', ', ', '2015'), ('May', '10', ', ', '2015'), ('May', '17', ', ', '2015'), ('May', '24', ', ', '2015'), ('May', '31', ', ', '2015'), ('June', '7', ', ', '2015'), ('June', '14', ', ', '2015'), ('June', '21', ', ', '2015'), ('June', '28', ', ', '2015'), ('July', '5', ', ', '2015'), ('July', '12', ', ', '2015'), ('July', '19', ', ', '2015')]\n"
     ]
    }
   ],
   "source": [
    "regex = r\"([a-zA-Z]+) (\\d+)(..)(\\d+)\" \n",
    "search_result = re.findall(regex, string)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month Day  Year\n",
       "0  March   8  2015\n",
       "1  March  15  2015\n",
       "2  March  22  2015\n",
       "3  March  29  2015\n",
       "4  April   5  2015"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search_result, columns=['Month', 'Day', 'comma', 'Year'])\n",
    "df = df.drop(columns = 'comma')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(x):\n",
    "    try:\n",
    "        # remove spaces inside and outside of string\n",
    "        x = \" \".join(x.split())\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SentimentText = df.SentimentText.apply(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo im gunna CRy. I've been at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!! T_T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0           is so sad for my APL friend.............\n",
       "1          0                   I missed the New Moon trailer...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0  .. Omgaga. Im sooo im gunna CRy. I've been at ...\n",
       "4          0             i think mi bf is cheating on me!!! T_T"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SentimentText = df.SentimentText.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. omgaga. im sooo im gunna cry. i've been at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!! t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0           is so sad for my apl friend.............\n",
       "1          0                   i missed the new moon trailer...\n",
       "2          1                            omg its already 7:30 :o\n",
       "3          0  .. omgaga. im sooo im gunna cry. i've been at ...\n",
       "4          0             i think mi bf is cheating on me!!! t_t"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SentimentText = df.SentimentText.apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry ive been at this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me tt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                        is so sad for my apl friend\n",
       "1          0                      i missed the new moon trailer\n",
       "2          1                              omg its already 730 o\n",
       "3          0   omgaga im sooo im gunna cry ive been at this ...\n",
       "4          0                 i think mi bf is cheating on me tt"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText'] = df.apply(lambda row: nltk.word_tokenize(row['SentimentText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[is, so, sad, for, my, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, missed, the, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, its, already, 730, o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, been, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                [is, so, sad, for, my, apl, friend]\n",
       "1          0               [i, missed, the, new, moon, trailer]\n",
       "2          1                        [omg, its, already, 730, o]\n",
       "3          0  [omgaga, im, sooo, im, gunna, cry, ive, been, ...\n",
       "4          0       [i, think, mi, bf, is, cheating, on, me, tt]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText'] = df['SentimentText'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[worry, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[handed, uniform, today, miss, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[hmmmm, wonder, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[must, think, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, haters, face, day, 112102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[weekend, sucked, far]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[jb, isnt, showing, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, thats, win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, way, feel, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[awhhe, man, im, completely, useless, rt, funn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, strangely, fine, im, gon, na, go, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[huge, roll, thunder, nowso, scary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[cut, beard, growing, well, year, im, gon, na,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText\n",
       "0           0                                 [sad, apl, friend]\n",
       "1           0                       [missed, new, moon, trailer]\n",
       "2           1                                [omg, already, 730]\n",
       "3           0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...\n",
       "4           0                      [think, mi, bf, cheating, tt]\n",
       "5           0                                      [worry, much]\n",
       "6           1                [juuuuuuuuuuuuuuuuussssst, chillin]\n",
       "7           0               [sunny, work, tomorrow, tv, tonight]\n",
       "8           1            [handed, uniform, today, miss, already]\n",
       "9           1                            [hmmmm, wonder, number]\n",
       "10          0                            [must, think, positive]\n",
       "11          1                [thanks, haters, face, day, 112102]\n",
       "12          0                             [weekend, sucked, far]\n",
       "13          0                     [jb, isnt, showing, australia]\n",
       "14          0                                   [ok, thats, win]\n",
       "15          0                             [lt, way, feel, right]\n",
       "16          0  [awhhe, man, im, completely, useless, rt, funn...\n",
       "17          1  [feeling, strangely, fine, im, gon, na, go, li...\n",
       "18          0                [huge, roll, thunder, nowso, scary]\n",
       "19          0  [cut, beard, growing, well, year, im, gon, na,..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xailus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                                 [sad, apl, friend]\n",
       "1          0                       [missed, new, moon, trailer]\n",
       "2          1                                [omg, already, 730]\n",
       "3          0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...\n",
       "4          0                      [think, mi, bf, cheating, tt]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df['SentimentText'] = df['SentimentText'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x ])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[worry, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[handed, uniform, today, miss, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[hmmmm, wonder, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[must, think, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, hater, face, day, 112102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[weekend, sucked, far]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[jb, isnt, showing, australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, thats, win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, way, feel, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[awhhe, man, im, completely, useless, rt, funn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, strangely, fine, im, gon, na, go, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[huge, roll, thunder, nowso, scary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[cut, beard, growing, well, year, im, gon, na,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText\n",
       "0           0                                 [sad, apl, friend]\n",
       "1           0                       [missed, new, moon, trailer]\n",
       "2           1                                [omg, already, 730]\n",
       "3           0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...\n",
       "4           0                      [think, mi, bf, cheating, tt]\n",
       "5           0                                      [worry, much]\n",
       "6           1                [juuuuuuuuuuuuuuuuussssst, chillin]\n",
       "7           0               [sunny, work, tomorrow, tv, tonight]\n",
       "8           1            [handed, uniform, today, miss, already]\n",
       "9           1                            [hmmmm, wonder, number]\n",
       "10          0                            [must, think, positive]\n",
       "11          1                 [thanks, hater, face, day, 112102]\n",
       "12          0                             [weekend, sucked, far]\n",
       "13          0                     [jb, isnt, showing, australia]\n",
       "14          0                                   [ok, thats, win]\n",
       "15          0                             [lt, way, feel, right]\n",
       "16          0  [awhhe, man, im, completely, useless, rt, funn...\n",
       "17          1  [feeling, strangely, fine, im, gon, na, go, li...\n",
       "18          0                [huge, roll, thunder, nowso, scary]\n",
       "19          0  [cut, beard, growing, well, year, im, gon, na,..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "TF-IDF scores are calculated across a set of documents. TFIDF finds words that appear more frequently in a given document than in other documents in the set. TFIDF can be used as a vectorization method - it embeds the importance of specific words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText_notoken'] = df['SentimentText'].apply(lambda x: [' '.join(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentText_notoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>[sad apl friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>[missed new moon trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "      <td>[omg already 730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "      <td>[omgaga im sooo im gunna cry ive dentist since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>[think mi bf cheating tt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[worry, much]</td>\n",
       "      <td>[worry much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst chillin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
       "      <td>[sunny work tomorrow tv tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[handed, uniform, today, miss, already]</td>\n",
       "      <td>[handed uniform today miss already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[hmmmm, wonder, number]</td>\n",
       "      <td>[hmmmm wonder number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[must, think, positive]</td>\n",
       "      <td>[must think positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, hater, face, day, 112102]</td>\n",
       "      <td>[thanks hater face day 112102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[weekend, sucked, far]</td>\n",
       "      <td>[weekend sucked far]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[jb, isnt, showing, australia]</td>\n",
       "      <td>[jb isnt showing australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, thats, win]</td>\n",
       "      <td>[ok thats win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, way, feel, right]</td>\n",
       "      <td>[lt way feel right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[awhhe, man, im, completely, useless, rt, funn...</td>\n",
       "      <td>[awhhe man im completely useless rt funny twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, strangely, fine, im, gon, na, go, li...</td>\n",
       "      <td>[feeling strangely fine im gon na go listen se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[huge, roll, thunder, nowso, scary]</td>\n",
       "      <td>[huge roll thunder nowso scary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[cut, beard, growing, well, year, im, gon, na,...</td>\n",
       "      <td>[cut beard growing well year im gon na start s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText  \\\n",
       "0           0                                 [sad, apl, friend]   \n",
       "1           0                       [missed, new, moon, trailer]   \n",
       "2           1                                [omg, already, 730]   \n",
       "3           0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...   \n",
       "4           0                      [think, mi, bf, cheating, tt]   \n",
       "5           0                                      [worry, much]   \n",
       "6           1                [juuuuuuuuuuuuuuuuussssst, chillin]   \n",
       "7           0               [sunny, work, tomorrow, tv, tonight]   \n",
       "8           1            [handed, uniform, today, miss, already]   \n",
       "9           1                            [hmmmm, wonder, number]   \n",
       "10          0                            [must, think, positive]   \n",
       "11          1                 [thanks, hater, face, day, 112102]   \n",
       "12          0                             [weekend, sucked, far]   \n",
       "13          0                     [jb, isnt, showing, australia]   \n",
       "14          0                                   [ok, thats, win]   \n",
       "15          0                             [lt, way, feel, right]   \n",
       "16          0  [awhhe, man, im, completely, useless, rt, funn...   \n",
       "17          1  [feeling, strangely, fine, im, gon, na, go, li...   \n",
       "18          0                [huge, roll, thunder, nowso, scary]   \n",
       "19          0  [cut, beard, growing, well, year, im, gon, na,...   \n",
       "\n",
       "                                SentimentText_notoken  \n",
       "0                                    [sad apl friend]  \n",
       "1                           [missed new moon trailer]  \n",
       "2                                   [omg already 730]  \n",
       "3   [omgaga im sooo im gunna cry ive dentist since...  \n",
       "4                           [think mi bf cheating tt]  \n",
       "5                                        [worry much]  \n",
       "6                  [juuuuuuuuuuuuuuuuussssst chillin]  \n",
       "7                    [sunny work tomorrow tv tonight]  \n",
       "8                 [handed uniform today miss already]  \n",
       "9                               [hmmmm wonder number]  \n",
       "10                              [must think positive]  \n",
       "11                     [thanks hater face day 112102]  \n",
       "12                               [weekend sucked far]  \n",
       "13                        [jb isnt showing australia]  \n",
       "14                                     [ok thats win]  \n",
       "15                                [lt way feel right]  \n",
       "16  [awhhe man im completely useless rt funny twit...  \n",
       "17  [feeling strangely fine im gon na go listen se...  \n",
       "18                    [huge roll thunder nowso scary]  \n",
       "19  [cut beard growing well year im gon na start s...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment                 int64\n",
       "SentimentText            object\n",
       "SentimentText_notoken    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText_notoken'] = df['SentimentText_notoken'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentText_notoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>['sad apl friend']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>['missed new moon trailer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, 730]</td>\n",
       "      <td>['omg already 730']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
       "      <td>['omgaga im sooo im gunna cry ive dentist sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>['think mi bf cheating tt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[worry, much]</td>\n",
       "      <td>['worry much']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "      <td>['juuuuuuuuuuuuuuuuussssst chillin']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
       "      <td>['sunny work tomorrow tv tonight']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[handed, uniform, today, miss, already]</td>\n",
       "      <td>['handed uniform today miss already']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[hmmmm, wonder, number]</td>\n",
       "      <td>['hmmmm wonder number']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[must, think, positive]</td>\n",
       "      <td>['must think positive']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, hater, face, day, 112102]</td>\n",
       "      <td>['thanks hater face day 112102']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[weekend, sucked, far]</td>\n",
       "      <td>['weekend sucked far']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[jb, isnt, showing, australia]</td>\n",
       "      <td>['jb isnt showing australia']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, thats, win]</td>\n",
       "      <td>['ok thats win']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, way, feel, right]</td>\n",
       "      <td>['lt way feel right']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[awhhe, man, im, completely, useless, rt, funn...</td>\n",
       "      <td>['awhhe man im completely useless rt funny twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, strangely, fine, im, gon, na, go, li...</td>\n",
       "      <td>['feeling strangely fine im gon na go listen s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[huge, roll, thunder, nowso, scary]</td>\n",
       "      <td>['huge roll thunder nowso scary']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[cut, beard, growing, well, year, im, gon, na,...</td>\n",
       "      <td>['cut beard growing well year im gon na start ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText  \\\n",
       "0           0                                 [sad, apl, friend]   \n",
       "1           0                       [missed, new, moon, trailer]   \n",
       "2           1                                [omg, already, 730]   \n",
       "3           0  [omgaga, im, sooo, im, gunna, cry, ive, dentis...   \n",
       "4           0                      [think, mi, bf, cheating, tt]   \n",
       "5           0                                      [worry, much]   \n",
       "6           1                [juuuuuuuuuuuuuuuuussssst, chillin]   \n",
       "7           0               [sunny, work, tomorrow, tv, tonight]   \n",
       "8           1            [handed, uniform, today, miss, already]   \n",
       "9           1                            [hmmmm, wonder, number]   \n",
       "10          0                            [must, think, positive]   \n",
       "11          1                 [thanks, hater, face, day, 112102]   \n",
       "12          0                             [weekend, sucked, far]   \n",
       "13          0                     [jb, isnt, showing, australia]   \n",
       "14          0                                   [ok, thats, win]   \n",
       "15          0                             [lt, way, feel, right]   \n",
       "16          0  [awhhe, man, im, completely, useless, rt, funn...   \n",
       "17          1  [feeling, strangely, fine, im, gon, na, go, li...   \n",
       "18          0                [huge, roll, thunder, nowso, scary]   \n",
       "19          0  [cut, beard, growing, well, year, im, gon, na,...   \n",
       "\n",
       "                                SentimentText_notoken  \n",
       "0                                  ['sad apl friend']  \n",
       "1                         ['missed new moon trailer']  \n",
       "2                                 ['omg already 730']  \n",
       "3   ['omgaga im sooo im gunna cry ive dentist sinc...  \n",
       "4                         ['think mi bf cheating tt']  \n",
       "5                                      ['worry much']  \n",
       "6                ['juuuuuuuuuuuuuuuuussssst chillin']  \n",
       "7                  ['sunny work tomorrow tv tonight']  \n",
       "8               ['handed uniform today miss already']  \n",
       "9                             ['hmmmm wonder number']  \n",
       "10                            ['must think positive']  \n",
       "11                   ['thanks hater face day 112102']  \n",
       "12                             ['weekend sucked far']  \n",
       "13                      ['jb isnt showing australia']  \n",
       "14                                   ['ok thats win']  \n",
       "15                              ['lt way feel right']  \n",
       "16  ['awhhe man im completely useless rt funny twi...  \n",
       "17  ['feeling strangely fine im gon na go listen s...  \n",
       "18                  ['huge roll thunder nowso scary']  \n",
       "19  ['cut beard growing well year im gon na start ...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment                 int64\n",
       "SentimentText            object\n",
       "SentimentText_notoken    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.SentimentText_notoken\n",
    "y = df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991,)\n",
      "(19998,)\n",
      "(79991,)\n",
      "(19998,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=None, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "#print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79991, 98395)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19998, 98395)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(train_word_counts, y_train)\n",
    "\n",
    "train_predictions = LR.predict(train_word_counts)\n",
    "test_predictions = LR.predict(test_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9014639146904027\n",
      "Test Accuracy: 0.7495749574957495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/22/18d108180fb6d9408a7c7d3c47e1a7c7a4e0d348420be27faa9a22f57117/gensim-3.7.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 1.9MB/s \n",
      "\u001b[?25hCollecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 6.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/xailus/anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/xailus/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/xailus/anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/xailus/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/xailus/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/19/97d22014b97783aa9986c5c9fc9361f2539bd50a5532a7c3f13e7f13dc34/boto3-1.9.124-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 33.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/xailus/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/xailus/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/xailus/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/xailus/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.124 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/bd/2e5b84f477389442a349511404d269aed9f98d26a4f5fa240229280a3d01/botocore-1.12.124-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 11.0MB/s \n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 5.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/xailus/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.124->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /home/xailus/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.124->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/xailus/.cache/pip/wheels/f7/a6/ff/9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/xailus/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.124 botocore-1.12.124 bz2file-0.98 gensim-3.7.1 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(df['SentimentText'], min_count=5, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=12165, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "#print(list(model.wv.vocab))\n",
    "#print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dm', 0.8501983880996704),\n",
       " ('facebook', 0.8293087482452393),\n",
       " ('following', 0.8256533741950989),\n",
       " ('link', 0.8233585357666016),\n",
       " ('blog', 0.8219295144081116),\n",
       " ('email', 0.8137913942337036),\n",
       " ('list', 0.8130325078964233),\n",
       " ('update', 0.8097699880599976),\n",
       " ('follow', 0.8063111901283264),\n",
       " ('message', 0.805451512336731)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('twitter')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
