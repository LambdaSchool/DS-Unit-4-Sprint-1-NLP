{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Sprint Challenge*\n",
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "**Part 1 - Working with Text Data**\n",
    "Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:06:20.777681Z",
     "start_time": "2019-05-03T15:06:20.763650Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:06:21.526167Z",
     "start_time": "2019-05-03T15:06:21.475905Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has a lot of extra whitespace.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(whitespace_string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:06:41.905166Z",
     "start_time": "2019-05-03T15:06:41.881810Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'March 8, 2015\\nMarch 15, 2015\\nMarch 22, 2015\\nMarch 29, 2015\\nApril 5, 2015\\nApril 12, 2015\\nApril 19, 2015\\nApril 26, 2015\\nMay 3, 2015\\nMay 10, 2015\\nMay 17, 2015\\nMay 24, 2015\\nMay 31, 2015\\nJune 7, 2015\\nJune 14, 2015\\nJune 21, 2015\\nJune 28, 2015\\nJuly 5, 2015\\nJuly 12, 2015\\nJuly 19, 2015'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dates.txt', 'r', encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  \n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:07:12.878613Z",
     "start_time": "2019-05-03T15:07:12.858597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('March', '8', '2015'),\n",
       " ('March', '15', '2015'),\n",
       " ('March', '22', '2015'),\n",
       " ('March', '29', '2015'),\n",
       " ('April', '5', '2015'),\n",
       " ('April', '12', '2015'),\n",
       " ('April', '19', '2015'),\n",
       " ('April', '26', '2015'),\n",
       " ('May', '3', '2015'),\n",
       " ('May', '10', '2015'),\n",
       " ('May', '17', '2015'),\n",
       " ('May', '24', '2015'),\n",
       " ('May', '31', '2015'),\n",
       " ('June', '7', '2015'),\n",
       " ('June', '14', '2015'),\n",
       " ('June', '21', '2015'),\n",
       " ('June', '28', '2015'),\n",
       " ('July', '5', '2015'),\n",
       " ('July', '12', '2015'),\n",
       " ('July', '19', '2015')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "dates = re.findall(r\"([a-zA-Z]+) ([0-9]+), ([0-9]+)\", contents)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:07:39.402282Z",
     "start_time": "2019-05-03T15:07:38.614921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dates, columns=['Month', 'Day','Year'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:08:53.608176Z",
     "start_time": "2019-05-03T15:08:51.326125Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "# !pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:09:31.012106Z",
     "start_time": "2019-05-03T15:09:27.101090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')\n",
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:09:54.912063Z",
     "start_time": "2019-05-03T15:09:54.694558Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.564632\n",
      "0    0.435368\n",
      "Name: Sentiment, dtype: float64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "99989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99989, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(twit_df.Sentiment.value_counts(normalize=True))\n",
    "print(\"-\"*100)\n",
    "print(twit_df.SentimentText.value_counts().sum())\n",
    "twit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:11:22.940990Z",
     "start_time": "2019-05-03T15:10:35.917366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokenized_SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg already</td>\n",
       "      <td>[omg, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>think mi bf cheating tt</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                                     sad apl friend   \n",
       "1          0                            missed new moon trailer   \n",
       "2          1                                        omg already   \n",
       "3          0  omgaga im sooo im gunna cry dentist since supo...   \n",
       "4          0                            think mi bf cheating tt   \n",
       "\n",
       "                             tokenized_SentimentText  \n",
       "0                                 [sad, apl, friend]  \n",
       "1                       [missed, new, moon, trailer]  \n",
       "2                                     [omg, already]  \n",
       "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...  \n",
       "4                      [think, mi, bf, cheating, tt]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = str.maketrans('','', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(texts):\n",
    "    # Tokenize by word\n",
    "    tokens = word_tokenize(texts)\n",
    "\n",
    "    # Make all words lowercase\n",
    "    lowercase_tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    # Strip punctuation from within words\n",
    "    no_punctuation = [x.translate(table) for x in lowercase_tokens]\n",
    "\n",
    "    # Remove words that aren't alphabetic\n",
    "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [w for w in alphabetic if not w in stop_words]\n",
    "    return words\n",
    "\n",
    "twit_df['tokenized_SentimentText'] = twit_df['SentimentText'].apply(tokenize)\n",
    "twit_df.SentimentText = twit_df.tokenized_SentimentText.apply(lambda x:' '.join(x))\n",
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "* TF-IDF is short for term frequency-inverse document frequency. It evaluate a keyword within a document in a corpus to score and rank the importance of such keyword based on how many times it appears.  Each word is given a  specific TF-IDF score.  Higher TF-IDF score means the word is less common and word with lower TF-IDF score is more common.\n",
    "* TF = word frequency / total words of a document.  IDF = log(number of documents/ word frequency in all documents).  TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "X = twit_df.SentimentText\n",
    "y = twit_df.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'application', 'applied', 'apply', 'appointment', 'appreciate', 'appreciated', 'apprentice', 'appropriate', 'approval', 'approve', 'approved', 'apps', 'appt', 'april', 'aprilyim', 'apt', 'ar', 'aral', 'arancinibaby', 'aravindkumar', 'arbonneteam', 'archie', 'archuphils', 'area']\n",
      "(79991, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aafreen</th>\n",
       "      <th>aah</th>\n",
       "      <th>aalaap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcdefglynis</th>\n",
       "      <th>abduzeedo</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>yur</th>\n",
       "      <th>yw</th>\n",
       "      <th>zac</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aafreen  aah  aalaap  aaron  abby  abc  abcdefglynis  abduzeedo  ability  \\\n",
       "0        0    0       0      0     0    0             0          0        0   \n",
       "1        0    0       0      0     0    0             0          0        0   \n",
       "2        0    0       0      0     0    0             0          0        0   \n",
       "3        0    0       0      0     0    0             0          0        0   \n",
       "4        0    0       0      0     0    0             0          0        0   \n",
       "\n",
       "   abit ...   yummy  yup  yur  yw  zac  zealand  zero  zombie  zone  zoo  \n",
       "0     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "1     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "2     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "3     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "4     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(X_train)\n",
    "print(vectorizer.get_feature_names()[300:325])\n",
    "\n",
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aafreen</th>\n",
       "      <th>aah</th>\n",
       "      <th>aalaap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcdefglynis</th>\n",
       "      <th>abduzeedo</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>yur</th>\n",
       "      <th>yw</th>\n",
       "      <th>zac</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aafreen  aah  aalaap  aaron  abby  abc  abcdefglynis  abduzeedo  ability  \\\n",
       "0        0    0       0      0     0    0             0          0        0   \n",
       "1        0    0       0      0     0    0             0          0        0   \n",
       "2        0    0       0      0     0    0             0          0        0   \n",
       "3        0    0       0      0     0    0             0          0        0   \n",
       "4        0    0       0      0     0    0             0          0        0   \n",
       "\n",
       "   abit ...   yummy  yup  yur  yw  zac  zealand  zero  zombie  zone  zoo  \n",
       "0     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "1     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "2     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "3     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "4     0 ...       0    0    0   0    0        0     0       0     0    0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7795001937717994\n",
      "Test Accuracy: 0.7498249824982498\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "columns = ['model', 'acc_train', 'acc_test', 'vect']\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Logistic Regression'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7660736832893701\n",
      "Test Accuracy: 0.7465746574657466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Multinomial Naive Bayes'\n",
    "result['acc_train'] = accuracy_score(y_train, train_predictions)\n",
    "result['acc_test'] = accuracy_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.749825</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.746575</td>\n",
       "      <td>0.766074</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_test  acc_train                    model vect_type\n",
       "0  0.749825   0.779500      Logistic Regression     Count\n",
       "1  0.746575   0.766074  Multinomial Naive Bayes     Count"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy results\n",
    "pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.7679205350669274\n",
      "Test ROC AUC Score: 0.7383023978866086\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_predictions)}')\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, test_predictions)}')\n",
    "\n",
    "columns = ['model', 'auc_train', 'auc_test', 'vect']\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Logistic Regression'\n",
    "result['auc_train'] = roc_auc_score(y_train, train_predictions)\n",
    "result['auc_test'] = roc_auc_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7584726357183148\n",
      "Test Accuracy: 0.7387886710018289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {roc_auc_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {roc_auc_score(y_test, test_predictions)}')\n",
    "\n",
    "result = {}\n",
    "result['model'] = 'Multinomial Naive Bayes'\n",
    "result['auc_train'] = roc_auc_score(y_train, train_predictions)\n",
    "result['auc_test'] = roc_auc_score(y_test, test_predictions)\n",
    "result['vect_type'] = 'Count'\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc_test</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>model</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.738302</td>\n",
       "      <td>0.767921</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.738789</td>\n",
       "      <td>0.758473</td>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auc_test  auc_train                    model vect_type\n",
       "0  0.738302   0.767921      Logistic Regression     Count\n",
       "1  0.738789   0.758473  Multinomial Naive Bayes     Count"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roc_auc_score results\n",
    "pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.7.2)\n",
      "Requirement not upgraded as not directly required: smart-open>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.8.3)\n",
      "Requirement not upgraded as not directly required: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement not upgraded as not directly required: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.15.4)\n",
      "Requirement not upgraded as not directly required: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement not upgraded as not directly required: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.20.1)\n",
      "Requirement not upgraded as not directly required: boto>=2.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Requirement not upgraded as not directly required: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (1.9.138)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement not upgraded as not directly required: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.23)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement not upgraded as not directly required: botocore<1.13.0,>=1.12.138 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.138)\n",
      "Requirement not upgraded as not directly required: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.138->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.138->boto3->smart-open>=1.7.0->gensim) (2.7.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harley', 0.9986058473587036),\n",
       " ('precooked', 0.9978617429733276),\n",
       " ('aceitonoequal', 0.9976887106895447),\n",
       " ('strategy', 0.9974822998046875),\n",
       " ('brandipowers', 0.9972800016403198),\n",
       " ('juggernaut', 0.9965915679931641),\n",
       " ('bradwilson', 0.9965680837631226),\n",
       " ('dizzy', 0.9963905811309814),\n",
       " ('bids', 0.9963849782943726),\n",
       " ('cwilso', 0.9961029291152954)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(twit_df.tokenized_SentimentText, min_count=1, size=5)\n",
    "model.wv.most_similar('twitter', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ahoova', 0.9993448257446289),\n",
       " ('dodgy', 0.9993407726287842),\n",
       " ('surreal', 0.9991070628166199),\n",
       " ('awful', 0.9985026121139526),\n",
       " ('aalaap', 0.998380184173584),\n",
       " ('busted', 0.9982904195785522),\n",
       " ('embrace', 0.9980837106704712),\n",
       " ('edward', 0.9979701638221741),\n",
       " ('celebrity', 0.9979403018951416),\n",
       " ('shy', 0.9979146718978882)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('funny', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " 'accuracy',\n",
       " 'add',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'get_keras_embedding',\n",
       " 'get_vector',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'init_sims',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'rank',\n",
       " 'relative_cosine_similarity',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_matrix',\n",
       " 'syn0',\n",
       " 'syn0norm',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than',\n",
       " 'wv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96546155"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('twitter','funny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
