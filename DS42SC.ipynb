{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS42SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QpPcbYew_ttN",
        "Vg1-d2aAsXLn",
        "Q764vszGqiUh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ed-chin-git/DS-Unit-4-Sprint-2-NLP/blob/master/DS42SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WVwRgvXMnfI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QpPcbYew_ttN"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Working with Text Data\n",
        "\n",
        "### Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dtotEnsStY5o",
        "outputId": "7750f7f5-0bf4-4e1c-b8b5-0030d45c1153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "\n",
        "print(whitespace_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G9-MkBwasXx8",
        "outputId": "127cedaf-293e-41e4-995f-ea798fd754bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\" \".join(whitespace_string.split()))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a string that has a lot of extra whitespace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Vg1-d2aAsXLn"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KWDiN4C9_0sq",
        "outputId": "4e591d44-1574-4866-f7ea-e87ee8b270e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "cell_type": "code",
      "source": [
        "with open('dates.txt', 'r', encoding='utf-8') as f:\n",
        "  contents = f.read()\n",
        "  \n",
        "contents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0b378becef90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dates.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dates.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rX2OZQ65nfJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regex = r\"([a-zA-Z]+) (\\d+), (\\d\\d\\d\\d)\" \n",
        "\n",
        "search_result = re.findall(regex, contents)\n",
        "\n",
        "for match in search_result:\n",
        "  print(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-v1xQklnfJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dfDate = pd.DataFrame(search_result, columns=['Month', 'Day','Year'])\n",
        "dfDate.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s4Q0dgoe_uBW"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "metadata": {
        "id": "-WvC6MKHnfJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1xzdhyTS_3F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3e8033ee-4960-4ea7-b881-b1cc901ccd51"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 200)\n",
        "url='https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv'\n",
        "df=pd.read_csv(url)\n",
        "\n",
        "# Create punctuation table\n",
        "table = str.maketrans('','', string.punctuation)\n",
        "\n",
        "# Create Stop word list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df2=df.copy()\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL friend.............</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trailer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment  \\\n",
              "0          0   \n",
              "1          0   \n",
              "2          1   \n",
              "3          0   \n",
              "4          0   \n",
              "\n",
              "                                                                                                                          SentimentText  \n",
              "0                                                                                              is so sad for my APL friend.............  \n",
              "1                                                                                                      I missed the New Moon trailer...  \n",
              "2                                                                                                               omg its already 7:30 :O  \n",
              "3            .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...  \n",
              "4                                                                                          i think mi bf is cheating on me!!!       T_T  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "O0pg3j5gnfJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df=df2.copy()\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = word_tokenize(doc)\n",
        "    #  lowercase\n",
        "    lowercase_tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each token\n",
        "    no_punctuation = [w.translate(table) for w in lowercase_tokens]\n",
        "    # Remove words that aren't alphabetic\n",
        "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    words = [w for w in alphabetic if not w in stop_words]\n",
        "\t  # filter out short tokens\n",
        "    tokens = [w for w in words if len(w)>1]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "df['SentimentText'] = df['SentimentText'].apply(clean_doc)\n",
        "\n",
        "# transform word list into word string\n",
        "df['SentimentText'] = df.SentimentText.apply(' '.join) # remove commas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q764vszGqiUh"
      },
      "cell_type": "markdown",
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "e2Ji7BMhqs3M"
      },
      "cell_type": "markdown",
      "source": [
        "#### Term Frequency / Inverse Document Frequency  #####\n",
        "\n",
        "Term Frequency: Percentage of words in document for each word\n",
        "  TF = num of times word occurs (frequency) divided by the total num of word in   the document\n",
        "\n",
        "Inverse Document Frequency: A penalty for the word existing in a high number of documents.\n",
        "    IDF = log2(total-#-documents/ #-documents-including-word )\n",
        "\n",
        "TF-IDF weighs a keyword in any content and assigns the importance to that keyword based on the number of times it appears in the document. It also checks how relevant the keyword is. Each word has its respective TF and IDF score. The product of the TF and IDF scores of a term is called the TF-IDF weight of that term.\n",
        "\n",
        "Put simply, the higher the TF-IDF score (weight), the rarer the term and vice versa.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3iUeBKtG_uEK"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TX8OEgUP_3ee",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.SentimentText.tolist()\n",
        "y = df.Sentiment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fC7xdLN1jmu5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f4b8bd30-b84e-4df3-a7e5-46f31c269b41"
      },
      "cell_type": "code",
      "source": [
        "#vectorizer = CountVectorizer(max_features=9999, ngram_range=(1,1), stop_words='english')\n",
        "vectorizer = TfidfVectorizer(max_features=9999, ngram_range=(1,1), stop_words='english')\n",
        "vectorizer.fit(X_train)\n",
        "# print(vectorizer.vocabulary_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=9999, min_df=1,\n",
              "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
              "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
              "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "        vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8ZVtIrr4nfJ6",
        "colab_type": "code",
        "outputId": "eb0f7d2a-584f-47d7-a768-5deaa5591913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "cell_type": "code",
      "source": [
        "train_word_counts = vectorizer.transform(X_train)\n",
        "\n",
        "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "#vectorize  X_test, uses ame vocabulary as the training dataset so  just call .transform() on X_test\n",
        "test_word_counts = vectorizer.transform(X_test)\n",
        "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(X_test_vectorized.shape)\n",
        "print(X_test_vectorized.head())\n",
        "print(X_train_vectorized.shape)\n",
        "print(X_train_vectorized.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24998, 9999)\n",
            "    aa  aaa  aaah  aaahh  aaaww  aafreen  aah  aahh  aahhh  aakomas ...   \\\n",
            "0  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "1  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "2  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "3  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "4  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "\n",
            "   zombie  zombies  zomg  zone  zones  zoo  zoom  zune   ðµ  ðµñ  \n",
            "0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "1     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "2     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "3     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "4     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 9999 columns]\n",
            "(74991, 9999)\n",
            "    aa  aaa  aaah  aaahh  aaaww  aafreen  aah  aahh  aahhh  aakomas ...   \\\n",
            "0  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "1  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "2  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "3  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "4  0.0  0.0   0.0    0.0    0.0      0.0  0.0   0.0    0.0      0.0 ...    \n",
            "\n",
            "   zombie  zombies  zomg  zone  zones  zoo  zoom  zune   ðµ  ðµñ  \n",
            "0     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "1     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "2     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "3     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "4     0.0      0.0   0.0   0.0    0.0  0.0   0.0   0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 9999 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "35kksfBZnfKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "31a1f265-a02a-4f5c-9dec-ec10be523db8"
      },
      "cell_type": "code",
      "source": [
        "RFC = RandomForestClassifier().fit(X_train_vectorized, y_train)\n",
        "\n",
        "train_predictions = RFC.predict(X_train_vectorized)\n",
        "test_predictions = RFC.predict(X_test_vectorized)\n",
        "\n",
        "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
        "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.9668760251230147\n",
            "Test Accuracy: 0.7163773101848148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXUmtuDgv1Xq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = XGBClassifier(n_jobs = -1).fit(X_train_vectorized, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sorF95UO_uGx"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DYno4d4N-LHR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(df2.SentimentText, min_count=20, window=3, size=300, negative=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6jwYvb6nfKJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('twitter', topn=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}