{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "75777175-2790-43b6-e986-f269c0ba8cbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# !pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4thPjvRikUDA"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "# 1) (optional) Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [],
   "source": [
    "job_listings = []\n",
    "\n",
    "target_url = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module2-Bag-of-Words/job_listings.csv'\n",
    "\n",
    "html_page = urlopen(target_url)\n",
    "for html_doc in html_page:\n",
    "    #html_doc = line\n",
    "    bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "    job_listings.append(bs.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another way to upload the data -- it's simpler and it has a few variations on how the strings render\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-NLP/master/module2-Bag-of-Words/job_listings.csv\"\n",
    "# df = pd.read_csv(url, index_col=0)\n",
    "# df.head()\n",
    "# df.description[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,\"b\"\"Job Requirements:\\\\nConceptual understanding in Machine Learning models like Nai\\\\xc2\\\\xa8ve Bayes, K-Means, SVM, Apriori, Linear/ Logistic Regression, Neural, Random Forests, Decision Trees, K-NN along with hands-on experience in at least 2 of them\\\\nIntermediate to expert level coding skills in Python/R. (Ability to write functions, clean and efficient data manipulation are mandatory for this role)\\\\nExposure to packages like NumPy, SciPy, Pandas, Matplotlib etc in Python or GGPlot2, dplyr, tidyR in R\\\\nAbility to communicate Model findings to both Technical and Non-Technical stake holders\\\\nHands on experience in SQL/Hive or similar programming language\\\\nMust show past work via GitHub, Kaggle or any other published article\\\\nMaster\\'s degree in Statistics/Mathematics/Computer Science or any other quant specific field.\\\\nApply Now\"\"\",Data scientist√Ç\\xa0\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_listings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H-mr3PIVb7M1",
    "outputId": "73c0f492-9fa2-40b8-ae8e-22b5bb34a1a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',description,title\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting first item in list\n",
    "job_listings.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_4cp_osVykS"
   },
   "outputs": [],
   "source": [
    "# Dictionary of job descriptions and job titles\n",
    "post_dict = {'description': [], 'title': []}\n",
    "for posting in job_listings:\n",
    "    # Spliting at quote tickmakrs and comma\n",
    "    description = re.split(r'\",|\\',', posting)[0]\n",
    "    # Converting `\\\\n` into space and joining\n",
    "    description = (' ').join(description.split('\\\\n'))\n",
    "    # Convering `/` into spaces\n",
    "    description = (' ').join(description.split('/'))\n",
    "    post_dict['description'].append(description)\n",
    "    \n",
    "    title = re.split(r'\",|\\',', posting)[1]\n",
    "    title = title.rstrip('\\n')\n",
    "    post_dict['title'].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "# 2) Use NLTK to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers and comma before description text\n",
    "df.description = df.description.str.lstrip('1234567890,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'b's and quote tickmarks before description text\n",
    "df.description = df.description.str.strip('b\\\"\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the 3-argument version of str.maketrans\n",
    "# with arguments (x, y, z) where 'x' and 'y'\n",
    "# must be equal-length strings and characters in 'x'\n",
    "# are replaced by characters in 'y'. 'z'\n",
    "# is a string (string.punctuation here)\n",
    "# where each character in the string is mapped\n",
    "# to None\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# stopwords set from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Function to Tokenizing by word\n",
    "def tokenize(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    \n",
    "    # making all words lowercase\n",
    "    lowercase_tokens = [w.lower() for w in tokens]\n",
    "    # print('Lowercase w:', lowercase_tokens)\n",
    "    \n",
    "    # Removing punctuation within words using `translator`\n",
    "    no_punctuation = [x.translate(translator) for x in lowercase_tokens]\n",
    "    #print(\"No Punctuation tk:\", no_punctuation)\n",
    "    \n",
    "    # Keeping only alphabetic words (no non-alphabetic)\n",
    "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
    "    # print('Alphabetic words:', alphabetic)\n",
    "    \n",
    "    # Removing stopwords\n",
    "    words = [w for w in alphabetic if not w in stop_words]\n",
    "    # print('Cleaned words:', words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title = df.title[2]\n",
    "tokenize(sample_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_description'] = df['description'].apply(tokenize)\n",
    "df['tokenized_title'] = df['title'].apply(tokenize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "# 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
   "outputs": [],
   "source": [
    "# Count Vectorizer as \"word word word\" NOT AS \"word\", \"word\", \"word\"\n",
    "def word_join(input_string, output_lst):\n",
    "    joined_string = (\" \").join(input_string)\n",
    "    output_lst.append(joined_string)\n",
    "    \n",
    "description_for_countvectorizer = []\n",
    "df.tokenized_description.apply(lambda x: word_join(x, description_for_countvectorizer))\n",
    "\n",
    "title_for_countvectorizer = []\n",
    "df.tokenized_title.apply(lambda x: word_join(x, title_for_countvectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_for_countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorize_df(string):\n",
    "    # Instantiate vectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # Create a vocabulary and get word counts per document\n",
    "    bag_of_words = vectorizer.fit_transform(string)\n",
    "    # print(bag_of_words.toarray())\n",
    "    \n",
    "    # Get feature names to use as df column headers\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return pd.DataFrame(bag_of_words.toarray(),\n",
    "                        columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description_vectorized = get_vectorize_df(\n",
    "                            description_for_countvectorizer)\n",
    "\n",
    "df_title_vectorized = get_vectorize_df(\n",
    "                      title_for_countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "# 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
   "outputs": [],
   "source": [
    "# Combining words into 1 list\n",
    "def data_for_viz(string, output_lst):\n",
    "    for word in string:\n",
    "        output_lst.append(word)\n",
    "        \n",
    "desc_for_viz = []\n",
    "df['tokenized_description'].apply(\n",
    "                            lambda x: data_for_viz(x, desc_for_viz))\n",
    "\n",
    "title_for_viz = []\n",
    "df['tokenized_title'].apply(\n",
    "                            lambda x: data_for_viz(x, title_for_viz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing most common wordsf\n",
    "def viz_common_words(string, count):\n",
    "    freq_dist = FreqDist(string)\n",
    "    print(freq_dist)\n",
    "    print(freq_dist.most_common(count))\n",
    "    \n",
    "    freq_dist.plot(count, cumulative=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_common_words(desc_for_viz, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_common_words(title_for_viz, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    " # 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TF-IDF` for `Descriptions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of document strings as text\n",
    "text = description_for_countvectorizer\n",
    "\n",
    "# Instatiate vectorizer object\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,1), max_features=20)\n",
    "\n",
    "# Create a vocabulary and get word counts per doc\n",
    "feature_matrix = tf_idf.fit_transform(text)\n",
    "# print(feature_matrix.toarray())\n",
    "\n",
    "# Get feature names to uas as df column headers\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "\n",
    "desc_tf_idf = pd.DataFrame(feature_matrix.toarray(0),\n",
    "                 columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tf_idf.shape)\n",
    "desc_tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TF-IDF` for `Descriptions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of document strings as text\n",
    "text_title = title_for_countvectorizer\n",
    "\n",
    "# Instatiate vectorizer object\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,1), max_features=20)\n",
    "\n",
    "# Create a vocabulary and get word counts per doc\n",
    "feature_matrix = tf_idf.fit_transform(text_title)\n",
    "# print(feature_matrix.toarray())\n",
    "\n",
    "# Get feature names to uas as df column headers\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "\n",
    "title_tf_idf = pd.DataFrame(feature_matrix.toarray(0),\n",
    "                 columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tf_idf.shape)\n",
    "title_tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
