{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "5dd045fe-6e4c-458c-e2fc-253c3da9c805"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "# 1) (optional) Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?as_and=tensorflow&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&sr=directhire&as_src=&salary=&radius=50&l=21044&fromage=any&limit=10&sort=&psf=advsrch\n"
     ]
    }
   ],
   "source": [
    "def query_generator(keyword, location, radius=50):\n",
    "\n",
    "    '''A function that takes a search keyword (or keywords) and a city and returns the resulting query url'''\n",
    "\n",
    "    if ' ' in keyword:\n",
    "        keyword.replace(' ', '+')\n",
    "    \n",
    "    \n",
    "    string1 = f\"https://www.indeed.com/jobs?as_and={keyword}&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&\"\n",
    "    string2 = f\"sr=directhire&as_src=&salary=&radius=50&l={location}&fromage=any&limit=10&sort=&psf=advsrch\"\n",
    "    query = string1+string2\n",
    "\n",
    "    return query\n",
    "\n",
    "test_query = query_generator(\"tensorflow\", \"21044\")\n",
    "print(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def num_search_results(query):\n",
    "\n",
    "    '''A function that takes a search query url and returns the number of search results'''\n",
    "\n",
    "    only_search_count = SoupStrainer(id=\"searchCount\")\n",
    "    page = requests.get(query)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\", parse_only=only_search_count)\n",
    "    s = soup.get_text()\n",
    "    num_search_results = [int(item) for item in s.split(' ') if item.isdigit()][-1]\n",
    "    return num_search_results\n",
    "\n",
    "test_num_search_results = num_search_results(test_query)\n",
    "# print(test_num_search_results)\n",
    "\n",
    "def soup_generator(query, parser='html.parser'):\n",
    "\n",
    "    '''A function that takes a query url and returns a BeautifulSoup object. html.parser is passed in as default parser'''\n",
    "\n",
    "    page = requests.get(query)\n",
    "    soup = BeautifulSoup(page.text, parser)\n",
    "\n",
    "    return soup\n",
    "\n",
    "test_soup = soup_generator(test_query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_page_generator(query, num_search_results, limit=50):\n",
    "    '''A function that takes a query url and the number of search results corresponding that query, and returns a list of urls to be scraped.'''\n",
    "\n",
    "    urls_to_scrape = []\n",
    "    i = int(num_search_results/50)\n",
    "    for page_number in range(i + 1):\n",
    "        url_suffix = f'&limit={limit}&start={str(page_number * 50)}'\n",
    "        url = f'{query}{url_suffix}'\n",
    "        urls_to_scrape.append(url)\n",
    "\n",
    "    return urls_to_scrape\n",
    "\n",
    "urls_to_scrape = search_page_generator(test_query, test_num_search_results)\n",
    "\n",
    "# Plug each of the items in urls_to_scrape into soup_generator()\n",
    "def extract_job_postings(url_list):\n",
    "    '''A function that takes a list of urls resulting from a specific query and returns a list of BeautifulSoup objects corresponding to each job posting in the list of urls'''\n",
    "\n",
    "    job_postings = []\n",
    "    for u in url_list:\n",
    "        soup = soup_generator(u)\n",
    "        for result in soup.find_all('div', attrs={'data-tn-component': 'organicJob'}):\n",
    "            job_postings.append(result)\n",
    "    return job_postings\n",
    "\n",
    "jobs = extract_job_postings(urls_to_scrape)\n",
    "print(len(jobs))\n",
    "job = jobs[200]\n",
    "\n",
    "\n",
    "# extract all html elements we want to scrape\n",
    "\n",
    "def extract_elements(job):\n",
    "\n",
    "    ''' A function that takes a scraped job posting and returns a dict representing each separate element (job_title, company, location, etc)'''\n",
    "\n",
    "    pass\n",
    "\n",
    "# * job_id\n",
    "\n",
    "job_id = job.find('h2', attrs={\"class\": \"jobtitle\"})['id']\n",
    "print(job_id)\n",
    "\n",
    "# * timestamp (date & time scraped - UTC)\n",
    "\n",
    "\n",
    "timestamp = dt.datetime.utcnow()\n",
    "format = \"%d-%m-%Y %-H:%M\"\n",
    "timestamp_formatted = timestamp.strftime(format)\n",
    "print(timestamp_formatted)\n",
    "# * job_title\n",
    "\n",
    "job_title = job.find('a', attrs={'data-tn-element':\"jobTitle\"}).text.strip().capitalize()\n",
    "\n",
    "print(job_title)\n",
    "\n",
    "# * company\n",
    "\n",
    "company = job.find('span', class_='company').text.strip()\n",
    "\n",
    "print(company)\n",
    "\n",
    "# * location\n",
    "\n",
    "location = job.find('span', class_='location').get_text()\n",
    "print(location)\n",
    "# * date_posting\n",
    "\n",
    "post_date = job.find('span', class_='date').get_text()\n",
    "\n",
    "\n",
    "def format_post_date(post_date, timestamp):\n",
    "\n",
    "    '''A function that takes in a timestamp indicating when a job posting was scraped and a string such as \"Publi√©e il y a 2 jours\", and returns a formatted date string such as \"03-05-2018\"'''\n",
    "\n",
    "    date = post_date.split(' ')\n",
    "\n",
    "    if \"l'instant\" in date or \"Aujourd'hui\" in date:\n",
    "        days_ago = 0\n",
    "    elif \"jour\" in date:\n",
    "        days_ago = 1\n",
    "    elif \"jours\" in date and \"30+\" not in date:\n",
    "        days_ago = int(date[-2])\n",
    "    else:\n",
    "        days_ago = None\n",
    "\n",
    "    if not days_ago:\n",
    "        return \"30+ days\"\n",
    "\n",
    "    delta = dt.timedelta(days=days_ago)\n",
    "\n",
    "    format = \"%d-%m-%Y\"\n",
    "    date_scraped = timestamp.date()\n",
    "    date_posted = (date_scraped - delta).strftime(format)\n",
    "    return date_posted\n",
    "\n",
    "\n",
    "formatted_post_date = format_post_date(post_date, timestamp)\n",
    "print(formatted_post_date)\n",
    "\n",
    "# * salary_range\n",
    "\n",
    "try:\n",
    "    salary = job.find('span', class_='salary no-wrap').text.strip()\n",
    "    print(salary)\n",
    "except AttributeError:\n",
    "    print('NA')\n",
    "\n",
    "# * job_summary\n",
    "\n",
    "summary = job.find('span', class_='summary').text.strip()\n",
    "print(summary)\n",
    "\n",
    "# * job_link\n",
    "\n",
    "job_link = \"https://www.indeed.com\" + job.find('h2', attrs={\"class\": \"jobtitle\"}).find('a')['href']\n",
    "print(job_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "## 2) Use NLTK to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhUHuMr-X-II"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "# 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "# 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    " # 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
