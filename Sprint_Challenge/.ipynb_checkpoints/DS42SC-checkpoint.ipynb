{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(whitespace_string.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Primary/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month  Day  Year\n",
       "0   March    8  2015\n",
       "1   March   15  2015\n",
       "2   March   22  2015\n",
       "3   March   29  2015\n",
       "4   April    5  2015\n",
       "5   April   12  2015\n",
       "6   April   19  2015\n",
       "7   April   26  2015\n",
       "8     May    3  2015\n",
       "9     May   10  2015\n",
       "10    May   17  2015\n",
       "11    May   24  2015\n",
       "12    May   31  2015\n",
       "13   June    7  2015\n",
       "14   June   14  2015\n",
       "15   June   21  2015\n",
       "16   June   28  2015\n",
       "17   July    5  2015\n",
       "18   July   12  2015\n",
       "19   July   19  2015"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "regex = r\"(?:,|\\s)\\s*\"\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt\", sep = regex, header = None)\n",
    "df.columns = ['Month', 'Day', 'Year']\n",
    "df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df['SentimentText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in df_tweets:\n",
    "  # Tokenize by word\n",
    "  tokens = word_tokenize(tweet)\n",
    "  #print(\"Tokens:\", tokens)\n",
    "  # Make all words lowercase\n",
    "  lowercase_tokens = [w.lower() for w in tokens]\n",
    "  #print(\"\\nLowercase:\", lowercase_tokens)\n",
    "  # Strip punctuation from within words\n",
    "  no_punctuation = [x.translate(table) for x in lowercase_tokens]\n",
    "  #print(\"\\nNo Punctuation:\", no_punctuation)\n",
    "  # Remove words that aren't alphabetic\n",
    "  #alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
    "  #print(\"\\nAlphabetic:\", alphabetic)\n",
    "  # Remove stopwords\n",
    "  words = [w for w in no_punctuation if not w in stop_words]\n",
    "  #print(\"\\nCleaned Words:\", words)\n",
    "  #print(\"--------------------------------\")\n",
    "  # Append to list\n",
    "  cleaned_tweets.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentimentText_tokenized'] = cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentText_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[sad, apl, friend, , , , , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[missed, new, moon, trailer, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, already, 730, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[, omgaga, , im, sooo, im, gunna, cry, , denti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[think, mi, bf, cheating, , , , tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                             SentimentText_tokenized  \n",
       "0                       [sad, apl, friend, , , , , ]  \n",
       "1                     [missed, new, moon, trailer, ]  \n",
       "2                              [omg, already, 730, ]  \n",
       "3  [, omgaga, , im, sooo, im, gunna, cry, , denti...  \n",
       "4                [think, mi, bf, cheating, , , , tt]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "TF-IDF weighs the term frequency against the inverse of its document frequency. This will give the value of specific content and determine if it is undervalued or overvalued. Common words will have low scores (<1) and words with more impact or rarity will have a higher score ( closer to 1).\n",
    "\n",
    "TF-IDF is calculated as:\n",
    "\n",
    "TF = # times a word appears / total words\n",
    "IDF = total # documents / # docs containing the word\n",
    "\n",
    "**TF-IDF = TF * IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['SentimentText']\n",
    "y = df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991,)\n",
      "(19998,)\n",
      "(79991,)\n",
      "(19998,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sleep': 290, 'watching': 366, 'quot': 259, 'school': 280, 'haha': 131, 'just': 171, 'forgot': 103, 'la': 177, 'love': 200, 'music': 227, 'little': 188, 'gotta': 124, 'dont': 78, 'know': 176, 'coffee': 52, 'lt': 203, 'gt': 126, 'twitpic': 348, 'working': 382, 'wanna': 360, 'hear': 143, 'bet': 29, 'cute': 62, 'like': 184, 'friend': 107, 'told': 334, 'got': 123, 'video': 357, 'll': 190, '10': 0, '30': 2, 'week': 369, 'ago': 6, 'http': 160, 'com': 54, 'time': 329, 'long': 193, 'party': 241, 'kinda': 174, 'thing': 320, 'good': 122, 'night': 232, 'sun': 308, 'way': 367, 'did': 70, 'today': 333, 'yes': 398, 'sigh': 288, 'cool': 58, 'things': 321, 'hot': 156, 'day': 66, 'right': 270, 'lol': 192, 'new': 229, 'nice': 231, 'meet': 216, 'happy': 137, 'sad': 274, 'come': 55, 'makes': 210, 'mad': 208, 'yeah': 394, 'remember': 267, 'yep': 397, 'does': 74, 'doesn': 75, 'think': 322, 'old': 237, 'far': 91, 'im': 165, 'car': 46, 'having': 141, 'iphone': 167, 'really': 265, 'want': 361, 'don': 77, 'bad': 24, 'ones': 239, 'miss': 218, 'favorite': 92, 'person': 244, 'won': 376, 'help': 149, 'following': 101, 'amp': 11, 'twitter': 349, 'looks': 196, 'gonna': 121, 'wont': 378, 'let': 182, 'thinking': 323, 'oh': 234, 'didn': 71, 'www': 388, 'congrats': 57, 'loved': 201, 'ok': 235, 'beautiful': 25, 'rock': 271, 'thanks': 318, 'bit': 33, 'ly': 207, 'said': 275, 'work': 381, 'year': 395, 'read': 261, 'talk': 314, 'tomorrow': 335, 'awww': 22, 'poor': 252, 'hope': 154, 'feel': 93, 'better': 30, 'soon': 293, 'used': 354, 'phone': 245, 've': 356, 'hi': 151, 'fan': 90, 'amazing': 10, 'make': 209, 'going': 119, 'start': 298, 'sunday': 309, 'gone': 120, 'probably': 256, 'follow': 98, 'left': 181, 'getting': 113, 'hang': 134, 'song': 291, 'crazy': 61, 'hair': 133, 'use': 353, 'need': 228, 'free': 105, 'great': 125, 'looking': 195, 'rain': 260, 'wanted': 362, 'ya': 391, 'morning': 225, 'food': 102, 'end': 83, 'send': 283, 'ask': 17, 'people': 243, 'dude': 79, 'coming': 56, 'waiting': 359, 'post': 253, 'wait': 358, 'play': 250, 'finally': 96, 'wow': 386, 'dear': 68, 'hopefully': 155, 'thank': 317, 'welcome': 372, 'face': 86, 'ur': 352, 'doing': 76, 'eat': 81, 'birthday': 32, 'followers': 99, 'thats': 319, 'life': 183, 'wasn': 363, 'able': 3, 'big': 31, 'house': 159, 'fun': 109, 'ff': 95, 'alexalltimelow': 9, 'stuff': 303, 'sorry': 295, 'lovely': 202, 'job': 170, 'site': 289, 'look': 194, 'seriously': 285, 'hours': 158, 'friday': 106, 'tired': 332, 'playing': 251, 'trying': 344, 'followfriday': 100, 'guys': 129, 'girl': 114, 'hahaha': 132, 'hugs': 161, 'wish': 375, 'fail': 88, 'friends': 108, 'aww': 21, 'andyclemmensen': 12, 'talking': 315, 'later': 179, 'tonight': 336, 'care': 47, 'tell': 316, 'game': 111, 'stupid': 304, 'pic': 246, 'home': 153, 'buy': 43, 'baby': 23, 'seen': 282, 'movie': 226, 'lunch': 206, 'sounds': 297, 'ashleytisdale': 16, 'bring': 40, 'add': 5, 'unfortunately': 351, 'came': 45, 'sooo': 294, 'jealous': 169, 'times': 330, 'early': 80, 'check': 51, 'forward': 104, 'link': 185, 'thought': 325, 'wouldn': 385, 'yea': 393, 'nope': 233, 'bed': 26, 'reason': 266, 'omg': 238, 'saw': 277, 'ha': 130, 'awesome': 20, 'damn': 65, 'monday': 222, 'cause': 48, 'actually': 4, 'aplusk': 14, 'say': 278, 'reply': 268, 'lot': 198, 'till': 328, 'hey': 150, 'real': 264, 'lost': 197, 'pics': 247, 'tried': 340, 'luck': 204, 'called': 44, 'works': 383, 'stop': 301, 'rest': 269, 'took': 337, 'didnt': 72, 'dad': 64, 'says': 279, 'head': 142, 'sure': 311, 'away': 19, 'haven': 140, 'anymore': 13, 'hard': 138, 'ppl': 254, 'watch': 364, 'aw': 18, 'seeing': 281, 'best': 28, 'yesterday': 399, 'lucky': 205, 'means': 215, 'weeks': 371, 'weekend': 370, 'xd': 389, 'yay': 392, 'hate': 139, 'definitely': 69, 'went': 373, 'funny': 110, 'god': 117, 'ugh': 350, 'couldn': 59, 'guy': 128, 'sent': 284, 'super': 310, 'hurt': 162, 'leave': 180, 'days': 67, 'dm': 73, 'hehe': 146, 'guess': 127, 'knew': 175, 'cold': 53, 'email': 82, 'stay': 300, 'money': 223, 'live': 189, 'maybe': 213, 'blog': 34, 'isn': 168, 'happened': 136, 'tv': 345, 'fine': 97, 'glad': 116, 'try': 343, 'story': 302, 'listen': 187, 'boo': 35, 'missed': 219, 'bored': 37, 'shit': 286, 'trip': 341, 'run': 273, 'ready': 263, 'summer': 307, 'sweet': 312, 'btw': 41, 'totally': 338, 'break': 39, 'heart': 145, 'feeling': 94, '100': 1, 'family': 89, 'xx': 390, 'tweet': 346, 'started': 299, 'using': 355, 'heard': 144, 'book': 36, 'picture': 248, 'reading': 262, 'soo': 292, 'idea': 163, 'happen': 135, 'word': 379, 'world': 384, 'hour': 157, 'hello': 148, 'pay': 242, 'lots': 199, 'room': 272, 'weather': 368, 'believe': 27, 'month': 224, 'sound': 296, 'pretty': 255, 'chance': 49, 'mind': 217, 'sucks': 306, 'words': 380, 'cuz': 63, 'excited': 85, 'tweets': 347, 'sick': 287, 'hell': 147, 'win': 374, 'years': 396, 'course': 60, 'change': 50, 'tinyurl': 331, 'list': 186, 'news': 230, 'goes': 118, 'wrong': 387, 'mom': 221, 'late': 178, 'train': 339, 'mean': 214, 'kind': 173, 'suck': 305, 'making': 211, 'gets': 112, 'til': 327, 'thx': 326, 'quite': 258, 'saturday': 276, 'problem': 257, 'aren': 15, 'kids': 172, 'agree': 7, 'enjoy': 84, 'man': 212, 'hit': 152, 'boy': 38, 'okay': 236, 'lmao': 191, 'tho': 324, 'place': 249, 'busy': 42, 'missing': 220, 'facebook': 87, 'ah': 8, 'girls': 115, 'true': 342, 'ill': 164, 'wonderful': 377, 'internet': 166, 'watched': 365, 'online': 240, 'taking': 313}\n"
     ]
    }
   ],
   "source": [
    "# Use Count Vectorizer for cleaning and preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=400, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 400)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>30</th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>ah</th>\n",
       "      <th>alexalltimelow</th>\n",
       "      <th>...</th>\n",
       "      <th>xx</th>\n",
       "      <th>ya</th>\n",
       "      <th>yay</th>\n",
       "      <th>yea</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  100  30  able  actually  add  ago  agree  ah  alexalltimelow  ...  xx  \\\n",
       "0   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "1   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "2   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "3   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "4   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "\n",
       "   ya  yay  yea  yeah  year  years  yep  yes  yesterday  \n",
       "0   0    0    0     0     0      0    0    0          0  \n",
       "1   0    0    0     0     0      0    0    0          0  \n",
       "2   0    0    0     0     0      0    0    0          0  \n",
       "3   0    0    0     0     0      0    0    0          0  \n",
       "4   0    0    0     0     0      0    0    0          0  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = vectorizer.transform(X_train)\n",
    "\n",
    "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19998, 400)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>30</th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>ah</th>\n",
       "      <th>alexalltimelow</th>\n",
       "      <th>...</th>\n",
       "      <th>xx</th>\n",
       "      <th>ya</th>\n",
       "      <th>yay</th>\n",
       "      <th>yea</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  100  30  able  actually  add  ago  agree  ah  alexalltimelow  ...  xx  \\\n",
       "0   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "1   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "2   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "3   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "4   0    0   0     0         0    0    0      0   0               0  ...   0   \n",
       "\n",
       "   ya  yay  yea  yeah  year  years  yep  yes  yesterday  \n",
       "0   0    0    0     0     0      0    0    0          0  \n",
       "1   1    0    0     0     0      0    0    0          0  \n",
       "2   0    0    0     0     0      0    0    0          0  \n",
       "3   0    0    0     0     0      0    0    0          0  \n",
       "4   0    0    0     0     0      0    0    0          0  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize X_test\n",
    "test_word_counts = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(X_test_vectorized.shape)\n",
    "X_test_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run classification models to see accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Primary/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7116300583815679\n",
      "Test Accuracy: 0.7059205920592059\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = LR.predict(X_train_vectorized)\n",
    "test_predictions = LR.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7077671238014277\n",
      "Test Accuracy: 0.7054205420542055\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_vectorized)\n",
    "test_predictions = MNB.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8965008563463389\n",
      "Test Accuracy: 0.6791179117911791\n"
     ]
    }
   ],
   "source": [
    "# Check against Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=200).fit(X_train_vectorized, y_train)\n",
    "\n",
    "train_predictions = RFC.predict(X_train_vectorized)\n",
    "test_predictions = RFC.predict(X_test_vectorized)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(y_train, train_predictions)}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, test_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(df.SentimentText_tokenized, min_count=20, window=3, size=400, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3731\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v.wv.vocab)\n",
    "print(f'Vocabulary Size: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 0.723713755607605),\n",
       " ('updates', 0.7034378051757812),\n",
       " ('facebook', 0.701595664024353),\n",
       " ('address', 0.6986019611358643),\n",
       " ('account', 0.6972944736480713),\n",
       " ('myspace', 0.6967622637748718),\n",
       " ('dm', 0.6963673233985901),\n",
       " ('info', 0.6834712028503418),\n",
       " ('list', 0.6820206046104431),\n",
       " ('others', 0.6798641681671143)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('twitter', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
