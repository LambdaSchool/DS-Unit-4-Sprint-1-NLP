{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "s = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "solution1 = ' '.join(s.split())\n",
    "\n",
    "wrdsp = r'[a-zA-Z]+\\s'\n",
    "last = r'[a-zA-Z]+\\.'\n",
    "\n",
    "solution2 = ''.join(re.findall(wrdsp, s) + re.findall(last, s))\n",
    "\n",
    "print(solution1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month day\n",
       "0  2015  March   8\n",
       "1  2015  March  15\n",
       "2  2015  March  22\n",
       "3  2015  March  29\n",
       "4  2015  April   5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt'\n",
    "dates_ = pd.read_csv(dates_url, \n",
    "                    header=None\n",
    "                   ).rename(columns={0: 'monthday', 1:'year'})\n",
    "\n",
    "def monthdaysplit(dat): \n",
    "    def getsplit(i: int): \n",
    "        return dat.monthday.apply(lambda s: s.split()[i])\n",
    "    return dat.assign(month = getsplit(0), day = getsplit(1)).drop('monthday', axis=1)\n",
    "\n",
    "monthdaysplit(dates_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month day  year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_pat = re.compile(r'^[A-Z]{1}[a-z]+')\n",
    "day_pat = re.compile(r'[0-9]{1,2}')\n",
    "year_pat = re.compile(r'2015')\n",
    "\n",
    "rows_list=[]\n",
    "dates_df = pd.DataFrame(columns=['month', 'day', 'year'])\n",
    "with open('dates.txt', 'r') as f:\n",
    "    for i,s in enumerate(f.readlines()):\n",
    "        month = month_pat.search(s)\n",
    "        day = day_pat.search(s)\n",
    "        year = year_pat.search(s)\n",
    "        \n",
    "        rows_list.append({'month': month.group(), 'day': day.group(), 'year': year.group()})\n",
    "\n",
    "dates_df = pd.DataFrame(rows_list, columns=['month', 'day', 'year'])\n",
    "\n",
    "dates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59089                                 [@, beechercreature]\n",
       "14850    [..., on, date, ., boy, doesn, want, a, gf, ,,...\n",
       "77872    [@, cadmium66, i, think, the, skanks, might, l...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv'\n",
    "\n",
    "def clean_bow(dat: pd.DataFrame) -> pd.DataFrame: \n",
    "    replacewords = stopwords.words('english') + [c for c in string.punctuation]\n",
    "    return dat.assign(SentimentText = dat.SentimentText.str.lower().replace(replacewords, ''))\n",
    "\n",
    "df_bow = clean_bow(pd.read_csv(bow_url).sample(2**14))\n",
    "print(df_bow.shape)\n",
    "\n",
    "df_bow.SentimentText.sample(3).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "TF-IDF is sort of like _entropy_ in that it measures our **uncertainty** about variables and penalizes stuff that's too common to be interesting. \n",
    "\n",
    "Given a list of documents, _term frequency `t`_ is a count for each word in each document. _document frequency_ `d` for each word documents the number of times it occurs when iterating through the whole list of documents, so `1/d` is what you're interested in. \n",
    "\n",
    "Ultimately, you want to divide these quantities and put them into `log`, because uncertainty measures always use `log`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(12288,) -- ', '(4096,) -- ', '(12288,) -- ', '(4096,) -- ']\n"
     ]
    }
   ],
   "source": [
    "cntvecz = CountVectorizer(max_features=2**13)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_bow.SentimentText, df_bow.Sentiment)\n",
    "\n",
    "print([str(x.shape) + ' -- ' for x in [X_train, X_test, y_train, y_test]])\n",
    "\n",
    "cntvecz.fit(X_train)\n",
    "\n",
    "BoWd_train = pd.DataFrame(cntvecz.transform(X_train).toarray(), columns=cntvecz.get_feature_names())\n",
    "\n",
    "BoWd_test = pd.DataFrame(cntvecz.transform(X_test).toarray(), columns=cntvecz.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quinn/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification \n",
      "--MEAN ACCURACY--\n",
      "            \tTrain score: 0.9945\n",
      "            \tTest score: 0.7231\n",
      "--ROC AUC--\n",
      "        \tTrain rocauc: 0.0005414\n",
      "        \tTest rocauc: 0.2152\n",
      "--BRIER SCORE LOSS--\n",
      "        \tTrain brier: 0.7581\n",
      "        \tTest brier: 0.4491\n",
      "                                \n",
      "Logistic Regression\n",
      "--MEAN ACCURACY--\n",
      "            \tTrain score: 0.8867\n",
      "            \tTest score: 0.7371\n",
      "--ROC AUC--\n",
      "        \tTrain rocauc: 0.04773\n",
      "        \tTest rocauc: 0.1954\n",
      "--BRIER SCORE LOSS--\n",
      "        \tTrain brier: 0.6072\n",
      "        \tTest brier: 0.5063\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, criterion='entropy')\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "rfc.fit(BoWd_train, y_train)\n",
    "lr.fit(BoWd_train, y_train)\n",
    "\n",
    "def report3(model): \n",
    "    \n",
    "    train_predict = [t[0] for t in model.predict_proba(BoWd_train)]\n",
    "    test_predict = [t[0] for t in model.predict_proba(BoWd_test)]\n",
    "    \n",
    "    avgaccur = f\"\"\"--MEAN ACCURACY--\n",
    "            \\tTrain score: {model.score(BoWd_train, y_train):.4}\n",
    "            \\tTest score: {model.score(BoWd_test, y_test):.4}\"\"\"\n",
    "    rocauc = f\"\"\"\\n--ROC AUC--\n",
    "        \\tTrain rocauc: {roc_auc_score(y_train, train_predict):.4}\n",
    "        \\tTest rocauc: {roc_auc_score(y_test.values,test_predict):.4}\"\"\"\n",
    "\n",
    "    brier = f\"\"\"\\n--BRIER SCORE LOSS--\n",
    "        \\tTrain brier: {brier_score_loss(y_train, train_predict):.4}\n",
    "        \\tTest brier: {brier_score_loss(y_test, test_predict):.4}\"\"\"\n",
    "\n",
    "    return avgaccur + rocauc + brier\n",
    "\n",
    "print(\"Random Forest Classification \")\n",
    "print(report3(rfc))\n",
    "print(''.join([' ' for _ in range(2**5)]))\n",
    "print(\"Logistic Regression\")\n",
    "print(report3(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quinn/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('whats', 0.9979201555252075),\n",
       " ('keeps', 0.9976176023483276),\n",
       " ('film', 0.9975176453590393),\n",
       " ('photo', 0.9971795082092285),\n",
       " ('sun', 0.9971530437469482),\n",
       " ('dr', 0.9968245625495911),\n",
       " ('cold', 0.9968036413192749),\n",
       " ('10', 0.9965555667877197),\n",
       " ('favorite', 0.9960423111915588),\n",
       " ('rock', 0.9960168600082397)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 2\n",
    "size = 50\n",
    "window = 4\n",
    "\n",
    "sentences = X_train.apply(word_tokenize)\n",
    "\n",
    "model = Word2Vec(sentences, min_count=min_count, size=size, window=window)\n",
    "\n",
    "print(len(list(model.wv.vocab.keys())))\n",
    "\n",
    "model.most_similar('twitter',topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
