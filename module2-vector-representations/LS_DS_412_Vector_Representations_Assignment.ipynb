{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "# Document Representations: Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "5dd045fe-6e4c-458c-e2fc-253c3da9c805"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/TomasFox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/TomasFox/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#!pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "# 1) (optional) Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description\n",
    "\n",
    "If you choose to not to scrape the data, there is a CSV with outdated data in the directory. Remeber, if you scrape Indeed, you're helping yourself find a job. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.1.4) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /anaconda3/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME             MODEL            VERSION                            \n",
      "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "package   en-core-web-lg   en_core_web_lg   \u001b[38;5;1m2.0.0\u001b[0m   --> 2.1.0     \n",
      "link      en               en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n",
      "\u001b[1m\n",
      "============================== Install updates ==============================\u001b[0m\n",
      "Use the following commands to update the model packages:\n",
      "python -m spacy download en_core_web_lg\n",
      "\n",
      "Collecting en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 826.9MB 18.6MB/s ta 0:00:01  1% |▌                               | 13.6MB 1.6MB/s eta 0:08:39    1% |▋                               | 14.9MB 977kB/s eta 0:13:51    2% |▊                               | 17.3MB 3.1MB/s eta 0:04:25    2% |█                               | 23.1MB 1.1MB/s eta 0:12:05    2% |█                               | 24.4MB 944kB/s eta 0:14:10    3% |█                               | 27.6MB 2.5MB/s eta 0:05:24    7% |██▎                             | 60.1MB 2.5MB/s eta 0:05:08    10% |███▍                            | 88.5MB 16.0MB/s eta 0:00:47    11% |███▋                            | 93.0MB 10.6MB/s eta 0:01:10    13% |████▎                           | 109.4MB 2.3MB/s eta 0:05:17    13% |████▍                           | 114.2MB 1.7MB/s eta 0:06:55    14% |████▌                           | 116.0MB 3.3MB/s eta 0:03:33    15% |█████                           | 129.2MB 5.3MB/s eta 0:02:13�███▍                         | 166.5MB 5.5MB/s eta 0:02:01 eta 0:00:50    21% |██████▉                         | 175.5MB 4.1MB/s eta 0:02:41    22% |███████                         | 183.3MB 3.8MB/s eta 0:02:51    22% |███████                         | 183.7MB 11.4MB/s eta 0:00:57    24% |███████▊                        | 200.3MB 19.5MB/s eta 0:00:33    25% |████████▎                       | 214.3MB 5.3MB/s eta 0:01:55    26% |████████▋                       | 222.5MB 10.9MB/s eta 0:00:56    28% |█████████▎                      | 238.4MB 1.5MB/s eta 0:06:34    29% |█████████▌                      | 245.8MB 3.4MB/s eta 0:02:53    31% |██████████                      | 257.0MB 2.2MB/s eta 0:04:21    31% |██████████▏                     | 262.1MB 1.3MB/s eta 0:07:08    32% |██████████▍                     | 269.0MB 2.2MB/s eta 0:04:12    34% |███████████                     | 285.6MB 3.4MB/s eta 0:02:42    39% |████████████▋                   | 326.6MB 6.5MB/s eta 0:01:18    40% |████████████▉                   | 332.3MB 5.1MB/s eta 0:01:38    40% |█████████████                   | 335.7MB 2.1MB/s eta 0:03:54    41% |█████████████▎                  | 343.8MB 3.2MB/s eta 0:02:33    42% |█████████████▌                  | 348.1MB 3.3MB/s eta 0:02:26    43% |█████████████▉                  | 357.3MB 937kB/s eta 0:08:21    48% |███████████████▌                | 399.6MB 11.9MB/s eta 0:00:36    49% |███████████████▊                | 405.7MB 14.4MB/s eta 0:00:30    49% |███████████████▊                | 406.5MB 1.8MB/s eta 0:03:54    50% |████████████████▎               | 421.3MB 1.3MB/s eta 0:05:09    52% |████████████████▊               | 431.2MB 25.2MB/s eta 0:00:16    52% |█████████████████               | 437.4MB 506kB/s eta 0:12:49    54% |█████████████████▎              | 446.7MB 597kB/s eta 0:10:37    56% |██████████████████              | 465.8MB 354kB/s eta 0:17:00    56% |██████████████████▏             | 468.7MB 724kB/s eta 0:08:15    58% |██████████████████▋             | 480.6MB 3.2MB/s eta 0:01:50    62% |████████████████████            | 518.7MB 14.4MB/s eta 0:00:22    63% |████████████████████▍           | 526.1MB 12.8MB/s eta 0:00:24    69% |██████████████████████▏         | 573.7MB 913kB/s eta 0:04:38█████▎         | 575.2MB 4.5MB/s eta 0:00:56    69% |██████████████████████▍         | 577.6MB 981kB/s eta 0:04:15    74% |███████████████████████▊        | 612.7MB 5.4MB/s eta 0:00:40    76% |████████████████████████▌       | 632.9MB 9.1MB/s eta 0:00:22    78% |█████████████████████████       | 646.1MB 2.9MB/s eta 0:01:02    78% |█████████████████████████▏      | 651.2MB 18.7MB/s eta 0:00:10    79% |█████████████████████████▍      | 656.5MB 2.9MB/s eta 0:00:59    79% |█████████████████████████▌      | 660.1MB 6.5MB/s eta 0:00:26    81% |██████████████████████████      | 670.3MB 6.4MB/s eta 0:00:25    82% |██████████████████████████▌     | 685.8MB 2.9MB/s eta 0:00:49    83% |██████████████████████████▉     | 693.4MB 4.2MB/s eta 0:00:32    90% |█████████████████████████████   | 751.3MB 5.2MB/s eta 0:00:15�██████████▉ | 797.9MB 3.0MB/s eta 0:00:10    97% |███████████████████████████████▏| 804.5MB 6.2MB/s eta 0:00:04    99% |███████████████████████████████▊| 819.0MB 5.9MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "  Found existing installation: en-core-web-lg 2.0.0\n",
      "    Uninstalling en-core-web-lg-2.0.0:\n",
      "      Successfully uninstalled en-core-web-lg-2.0.0\n",
      "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-lg-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "## 2) Use Spacy to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
       "      <td>Data scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
       "      <td>Data Scientist I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0  b\"<div><div>Job Requirements:</div><ul><li><p>...   \n",
       "1           1  b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...   \n",
       "2           2  b'<div><p>As a Data Scientist you will be work...   \n",
       "3           3  b'<div class=\"jobsearch-JobMetadataHeader icl-...   \n",
       "4           4  b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...   \n",
       "\n",
       "                          title  \n",
       "0               Data scientist   \n",
       "1              Data Scientist I  \n",
       "2  Data Scientist - Entry Level  \n",
       "3                Data Scientist  \n",
       "4                Data Scientist  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('job_listings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\"<div><div>Job Requirements:</div><ul><li><p>...\n",
       "1    b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...\n",
       "2    b'<div><p>As a Data Scientist you will be work...\n",
       "3    b'<div class=\"jobsearch-JobMetadataHeader icl-...\n",
       "4    b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['description']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Parses a string into a list of semantic units (words)\n",
    "\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: tokens parsed out by the mechanics of your choice\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ned's Solution:\n",
    "    sample = re.sub(r'[^a-zA-Z ^0-9]', '', text)\n",
    "    tokens = sample.lower().split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [bdivdivjob, requirementsdivullipnconceptual, ...\n",
       "1    [bdivjob, descriptionbrnbrnpas, a, data, scien...\n",
       "2    [bdivpas, a, data, scientist, you, will, be, w...\n",
       "3    [bdiv, classjobsearchjobmetadataheader, icluxs...\n",
       "4    [bullilocation, usa, xe2x80x93, multiple, loca...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = text.apply(tokenize)\n",
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use spacy \n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for review in tokenizer.pipe(text):\n",
    "    doc_tokens = [token.text for token in review]\n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "# 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 2, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "dtm = vectorizer.transform(text)\n",
    "dtm.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(dtm.todense(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02115</th>\n",
       "      <th>03</th>\n",
       "      <th>0356</th>\n",
       "      <th>04</th>\n",
       "      <th>057</th>\n",
       "      <th>062</th>\n",
       "      <th>06366</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zf</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zogsports</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  02115  03  0356  04  057  062  06366  08  ...  zero  zeus  zf  \\\n",
       "0   0    0      0   0     0   0    0    0      0   0  ...     0     0   0   \n",
       "1   0    0      0   0     0   0    0    0      0   0  ...     0     0   0   \n",
       "2   0    0      0   0     0   0    0    0      0   0  ...     0     0   0   \n",
       "3   0    0      0   0     0   0    0    0      0   0  ...     0     0   0   \n",
       "4   0    0      0   0     0   0    0    0      0   0  ...     0     0   0   \n",
       "\n",
       "   zheng  zillow  zogsports  zones  zoom  zuckerberg  zurich  \n",
       "0      0       0          0      0     0           0       0  \n",
       "1      0       0          0      0     0           0       0  \n",
       "2      0       0          0      0     0           0       0  \n",
       "3      1       0          0      0     0           0       0  \n",
       "4      0       0          0      0     0           0       0  \n",
       "\n",
       "[5 rows x 9202 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "li            12785\n",
       "div            6870\n",
       "data           4452\n",
       "br             3894\n",
       "ul             2636\n",
       "experience     1584\n",
       "xe2            1417\n",
       "x80            1404\n",
       "business       1210\n",
       "work           1077\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "# 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "\"\"\" Update those tokens \"\"\"\n",
    "for doc in tokenizer.pipe(df['description'], batch_size=500):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc: \n",
    "        if (token.is_stop == False) and (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "            \n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 3748),\n",
       " ('business', 1008),\n",
       " ('experience', 940),\n",
       " ('work', 875),\n",
       " ('team', 726),\n",
       " ('machine', 660),\n",
       " ('learning', 640),\n",
       " ('science', 564),\n",
       " ('statistical', 530),\n",
       " ('new', 517)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Object from Base Python\n",
    "from collections import Counter\n",
    "\n",
    "# The object `Counter` takes an iterable, but you can instantiate an empty one and update it. \n",
    "word_counts = Counter()\n",
    "\n",
    "\n",
    "# Update it based on a split of each of our documents\n",
    "df['tokens'].apply(lambda x: word_counts.update(x))\n",
    "\n",
    "\n",
    "# Print out the 10 most common words\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    " # 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>04</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1079302</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>125</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yeti</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zenreach</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zf</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   04   10  100  1079302   11   12  125   14  ...  yes  yeti  york  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0  0.0  ...  0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0  0.0  ...  0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0  0.0  ...  0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0  0.0  ...  0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0  0.0  ...  0.0   0.0   0.0   \n",
       "\n",
       "   young  yrs  zenreach  zeus   zf  zillow  zurich  \n",
       "0    0.0  0.0       0.0   0.0  0.0     0.0     0.0  \n",
       "1    0.0  0.0       0.0   0.0  0.0     0.0     0.0  \n",
       "2    0.0  0.0       0.0   0.0  0.0     0.0     0.0  \n",
       "3    0.0  0.0       0.0   0.0  0.0     0.0     0.0  \n",
       "4    0.0  0.0       0.0   0.0  0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "dtm = tfidf.fit_transform(df['description'])\n",
    "\n",
    "docs = pd.DataFrame(dtm.todense(), columns = tfidf.get_feature_names())\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "nn.fit(dtm.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.86248789, 0.87371026, 0.88181475, 0.88777735]]),\n",
       " array([[  0, 276, 301, 264,  71]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.kneighbors(dtm.todense()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = [\"\"\"Data scientists utilize their analytical, statistical,\n",
    "and programming skills to collect, analyze, and interpret large data sets. \n",
    "They then use this information to develop data-driven solutions to \n",
    "difficult business challenges. Data scientists commonly \n",
    "have a bachelor's degree in statistics, math, computer science, \n",
    "or economics. Data scientists have a wide range of technical\n",
    "competencies including: statistics and machine learning, coding\n",
    "languages, databases, machine learning, and reporting technologies.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.27520712, 1.28386618, 1.28605981, 1.28728691, 1.28728691]]),\n",
       " array([[261, 327, 176, 184, 147]]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = tfidf.transform(ideal)\n",
    "nn.kneighbors(new.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'<div><p>The Data Science Engineer, Mintel Futures is a core part of Mintel\\\\xe2\\\\x80\\\\x99s data science team that will have the opportunity to work on a wide array of initiatives across varying aspects of Mintel\\\\xe2\\\\x80\\\\x99s business and data. This individual will help manage the full analytics lifecycle of advanced projects; help to identify new, impactful ways to apply machine learning to Mintel\\\\xe2\\\\x80\\\\x99s data; work alongside data scientists and business stakeholders to implement solutions that provide valuable insights; and aide in the design and development of a modern data analytics environment.</p>\\\\n<p><b>\\\\nWhat You Will Do:</b></p>\\\\n<ul>\\\\n<li>Play an integral role in shaping the underlying technology environment for Mintel\\\\xe2\\\\x80\\\\x99s fast growing team of data scientists and data analysts</li>\\\\n<li>Assist in the acquisition and management of a variety of data sources for large-scale analysis</li>\\\\n<li>Identify opportunities for predictive modeling or other machine learning techniques and experiment with solutions that focus on adding value to our clients and analysts</li>\\\\n<li>Work alongside software engineers and other data scientists to ensure the proper deployment, management and monitoring of machine learning models and/or other data pipelines in Mintel\\\\xe2\\\\x80\\\\x99s production platforms</li>\\\\n<li>Grow the technical and engineering capabilities of other data scientists and data analysts on the team</li>\\\\n<li>Develop engineering best practices, documentation and process flows that facilitate collaboration and knowledge transfer</li>\\\\n<li>Continually learn - as a part of a fluid, innovation focused team, you will stay current on emerging data science and data engineering technologies</li><br/>\\\\n</ul>\\\\n<p>\\\\n</p><p><b>What We Are Looking For:</b></p>\\\\n<ul>\\\\n<li>3+ years professional experience with building data focused solutions (integrations, pipelines, data warehousing, etc.)</li>\\\\n<li>1+ years professional experience as a data scientist or machine learning engineer preferred and/or having demonstrable analytical capability and working knowledge of data science principles</li>\\\\n<li>Proficient engineer with the ability to lead the deployment of machine learning or other analytical models</li>\\\\n<li>Expert level SQL skills and expert level proficiency in at least one programming language (Python, C++, Java, etc.)</li>\\\\n<li>Strong communication skills with the ability to converse with both technical and non-technical stakeholders</li>\\\\n<li>Team player with a collaborative nature</li>\\\\n<li>Strong work ethic with an inherent sense of ownership and responsibility</li>\\\\n<li>Experience building data solutions in at least one public cloud environment, AWS or GCP preferred</li>\\\\n<li>Advanced degree holder in a STEM or related quantitative discipline preferred</li>\\\\n</ul><p><b>Equal Opportunity Employer: Race/Color/Sex/Sexual Orientation/Gender Identity/Religion/National Origin/Disability/Vets</b></p></div>'\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'][261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
