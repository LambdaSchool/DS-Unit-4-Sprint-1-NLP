{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Vector Representations\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "5dd045fe-6e4c-458c-e2fc-253c3da9c805"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "## 1) *Optional:* Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description\n",
    "\n",
    "If you choose to not to scrape the data, there is a CSV with outdated data in the directory. Remeber, if you scrape Indeed, you're helping yourself find a job. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given the url of a page, this function returns the soup object.\n",
    "    \n",
    "    Parameters:\n",
    "        url: the link to get soup object for\n",
    "    \n",
    "    Returns:\n",
    "        soup: soup object\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def grab_job_links(soup):\n",
    "    \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "        urls: a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Loop thru all the posting links\n",
    "    for link in soup.find_all('div', {'class': 'title'}):\n",
    "        # Since sponsored job postings are represented by \"a target\" instead of \"a href\", no need to worry here\n",
    "        partial_url = link.a.get('href')\n",
    "        # This is a partial url, we need to attach the prefix\n",
    "        url = 'https://indeed.com' + partial_url\n",
    "        # Make sure this is not a sponsored posting\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "def get_urls(query, num_pages, location):\n",
    "    \"\"\"\n",
    "    Get all the job posting URLs resulted from a specific search.\n",
    "    \n",
    "    Parameters:\n",
    "        query: job title to query\n",
    "        num_pages: number of pages needed\n",
    "        location: city to search in\n",
    "    \n",
    "    Returns:\n",
    "        urls: a list of job posting URL's (when num_pages valid)\n",
    "        max_pages: maximum number of pages allowed ((when num_pages invalid))\n",
    "    \"\"\"\n",
    "    # We always need the first page\n",
    "    base_url = 'https://indeed.com/jobs?q={}&l={}'.format(query, location)\n",
    "    soup = get_soup(base_url)\n",
    "    urls = grab_job_links(soup)\n",
    "    \n",
    "    # Get the total number of postings found \n",
    "    posting_count_string = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n",
    "    posting_count_string = posting_count_string[posting_count_string.find('of')+2:].strip()\n",
    "    #print('posting_count_string: {}'.format(posting_count_string))\n",
    "    #print('type is: {}'.format(type(posting_count_string)))\n",
    "    \n",
    "    try:\n",
    "        posting_count = int(posting_count_string)\n",
    "    except ValueError: # deal with special case when parsed string is \"360 jobs\"\n",
    "        posting_count = int(re.search('\\d+', posting_count_string).group(0))\n",
    "        #print('posting_count: {}'.format(posting_count))\n",
    "        #print('\\ntype: {}'.format(type(posting_count)))\n",
    "    finally:\n",
    "        posting_count = 330 # setting to 330 when unable to get the total\n",
    "        pass\n",
    "    \n",
    "    # Limit nunmber of pages to get\n",
    "    max_pages = round(posting_count / 10) - 3\n",
    "    if num_pages > max_pages:\n",
    "        print('returning max_pages!!')\n",
    "        return max_pages\n",
    "    \n",
    "        # Additional work is needed when more than 1 page is requested\n",
    "    if num_pages >= 2:\n",
    "        # Start loop from page 2 since page 1 has been dealt with above\n",
    "        for i in range(2, num_pages+1):\n",
    "            num = (i-1) * 10\n",
    "            base_url = 'https://indeed.com/jobs?q={}&l={}&start={}'.format(query, location, num)\n",
    "            try:\n",
    "                soup = get_soup(base_url)\n",
    "                # We always combine the results back to the list\n",
    "                urls += grab_job_links(soup)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Check to ensure the number of urls gotten is correct\n",
    "    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n",
    "\n",
    "    return urls     \n",
    "\n",
    "\n",
    "\n",
    "def get_posting(url):\n",
    "    \"\"\"\n",
    "    Get the text portion including both title and job description of the job posting from a given url\n",
    "    \n",
    "    Parameters:\n",
    "        url: The job posting link\n",
    "        \n",
    "    Returns:\n",
    "        title: the job title (if \"data scientist\" is in the title)\n",
    "        posting: the job posting content    \n",
    "    \"\"\"\n",
    "    # Get the url content as BS object\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # The job title is held in the h3 tag\n",
    "    title = soup.find(name='h3').getText().lower()\n",
    "    posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "\n",
    "    return title, posting.lower()\n",
    "\n",
    "        \n",
    "    #if 'data scientist' in title:  # We'll proceed to grab the job posting text if the title is correct\n",
    "        # All the text info is contained in the div element with the below class, extract the text.\n",
    "        #posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "        #return title, posting.lower()\n",
    "    #else:\n",
    "        #return False\n",
    "    \n",
    "        # Get rid of numbers and symbols other than given\n",
    "        #text = re.sub(\"[^a-zA-Z'+#&]\", \" \", text)\n",
    "        # Convert to lower case and split to list and then set\n",
    "        #text = text.lower().strip()\n",
    "    \n",
    "        #return text\n",
    "\n",
    "\n",
    "\n",
    "def get_data(query, num_pages, location='United States'):\n",
    "    \"\"\"\n",
    "    Get all the job posting data and save in a json file using below structure:\n",
    "    \n",
    "    {<count>: {'title': ..., 'posting':..., 'url':...}...}\n",
    "    \n",
    "    The json file name has this format: \"\"<query>.json\"\n",
    "    \n",
    "    Parameters:\n",
    "        query: Indeed query keyword such as 'Data Scientist'\n",
    "        num_pages: Number of search results needed\n",
    "        location: location to search for\n",
    "    \n",
    "    Returns:\n",
    "        postings_dict: Python dict including all posting data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the queried title to Indeed format\n",
    "    query = '+'.join(query.lower().split())\n",
    "    \n",
    "    postings_dict = {}\n",
    "    urls = get_urls(query, num_pages, location)\n",
    "    \n",
    "    #  Continue only if the requested number of pages is valid (when invalid, a number is returned instead of list)\n",
    "    if isinstance(urls, list):\n",
    "        num_urls = len(urls)\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                title, posting = get_posting(url)\n",
    "                postings_dict[i] = {}\n",
    "                postings_dict[i]['title'], postings_dict[i]['posting'], postings_dict[i]['url'] = \\\n",
    "                title, posting, url\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            percent = (i+1) / num_urls\n",
    "            # Print the progress the \"end\" arg keeps the message in the same line \n",
    "            print(\"Progress: {:2.0f}%\".format(100*percent), end='\\r')\n",
    "\n",
    "        # Save the dict as json file\n",
    "        file_name = query.replace('+', '_') + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(postings_dict, f)\n",
    "        \n",
    "        print('All {} postings have been scraped and saved!'.format(num_urls))    \n",
    "        #return postings_dict\n",
    "    else:\n",
    "        print(\"Due to similar results, maximum number of pages is only {}. Please try again!\".format(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 15 postings have been scraped and saved!\n"
     ]
    }
   ],
   "source": [
    " get_data('data journalist', 1, location='United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/job_listings.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "## 2) Use Spacy to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_html'] = df['description'].apply(lambda x: clean_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_hex(text):\n",
    "    new_text = re.sub(r\"\\\\[a-z][a-z]?[0-9]+\",'', text)\n",
    "#     new_text = re.sub(r'[^a-zA-Z ^0-9]', '', new_text)\n",
    "    new_text = re.sub(\"[!@#$+%*:()'-]\",'',new_text) # remove punc.\n",
    "    new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "    new_text = re.sub(r\"\\n\", \" \", new_text)\n",
    "    new_text = re.sub(r\"\\\\n\", \" \", new_text)\n",
    "    new_text = new_text.replace('\\\\', \"\")\n",
    "    new_text = new_text.strip(\"'b\")\n",
    "    new_text = new_text.strip(\"'\")\n",
    "    new_text = new_text.replace('\"', \"\")\n",
    "#     new_text = re.sub(\"[!@#$+%*:()'-]\",'',new_text)\n",
    "#     new_text = re.sub(r'\\d+','',new_text)\n",
    "    new_text = new_text.lower()\n",
    "    new_text = re.sub(' +', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_html'] = df['no_html'].apply(lambda x: clean_hex(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhUHuMr-X-II"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(text):\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['no_html'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = df['no_html']\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', min_df = 0.05, max_df= 0.90)\n",
    "vect.fit(data)\n",
    "\n",
    "sparse_dtm = vect.transform(data)\n",
    "\n",
    "dtm = pd.DataFrame(sparse_dtm.todense(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "## 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count(docs):\n",
    "\n",
    "        word_counts = Counter()\n",
    "        appears_in = Counter()\n",
    "        \n",
    "        total_docs = len(docs)\n",
    "\n",
    "        for doc in docs:\n",
    "            word_counts.update(doc)\n",
    "            appears_in.update(set(doc))\n",
    "\n",
    "        temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "        total = wc['count'].sum()\n",
    "\n",
    "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "        wc = wc.sort_values(by='rank')\n",
    "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "        t2 = zip(appears_in.keys(), appears_in.values())\n",
    "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "        wc = ac.merge(wc, on='word')\n",
    "\n",
    "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "        return wc.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = count(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc_top20 = wc[wc['rank'] <= 20]\n",
    "\n",
    "squarify.plot(sizes=wc_top20['pct_total'], label=wc_top20['word'], alpha=.8 )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = df['no_html']\n",
    "# Instantiate vectorizer object\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "# Create a vocabulary and get word counts per document\n",
    "sparse = tfidf.fit_transform(data)\n",
    "\n",
    "# Print word counts\n",
    "\n",
    "# Get feature names to use as dataframe column headers\n",
    "dtm = pd.DataFrame(sparse.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# Fit on TF-IDF Vectors\n",
    "nn  = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors([dtm.iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_job = [ \"\"\"Data journalist. Looking to hire an analytical journalist with strong grasp of data analysis, data storytelling and data visualizations. Is able to find news\n",
    "stories in large volumes of data and create thought-provoking and revealing visuals with the data. Strong reporting skills. Is proficient in Python, pandas, d3, graphing libraries,\n",
    "html, javascript, matplotlib.\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = tfidf.transform(ideal_job)\n",
    "\n",
    "nn.kneighbors(new.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[147]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
    " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
