{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EP_DS_422_BOW_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "U4S1-TEST",
      "language": "python",
      "name": "u4s1-test"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardopadilla3/DS-Unit-4-Sprint-1-NLP/blob/main/module2-vector-representations/EP_DS_422_BOW_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FprGt_A1Lhqz"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Vector Representations\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyj-f9FDcVFp"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7bcmqfGXrFG"
      },
      "source": [
        "## 1) *Clean:* Job Listings from indeed.com that contain the title \"Data Scientist\" \n",
        "\n",
        "You have `job_listings.csv` in the data folder for this module. The text data in the description column is still messy - full of html tags. Use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to clean up this column. You will need to read through the documentation to accomplish this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yRonGTvWq0F"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/eduardopadilla3/DS-Unit-4-Sprint-1-NLP/main/module2-vector-representations/data/job_listings.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W69283i3XZjY"
      },
      "source": [
        "df = df[['description', 'title']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcYlc1URXhlC"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import html\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    #soup = BeautifulSoup(text,features='html.parser')\n",
        "    #text = soup.get_text()\n",
        "\n",
        "    text = BeautifulSoup(html.unescape(text), \"lxml\").text\n",
        "    \n",
        "    #text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
        "    #text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.replace('\\\\n',' ')\n",
        "    \n",
        "    text = text.replace('\\\\xe2\\\\x80\\\\x93',' ')\n",
        "    text = text.replace('\\\\xe2\\\\x80\\\\x99',' ')\n",
        "    text = text.replace('\\\\xe2\\\\x80\\\\xa6',' ')\n",
        "    text = text.replace('\\\\xe2\\\\x80\\\\x94',' ')\n",
        "    text = text.replace('\\\\xc2\\\\xa8','')\n",
        "    text = text[1:]\n",
        "    text = re.sub('[^a-zA-Z 0-9]','',text)\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW0e6qDpakeN"
      },
      "source": [
        "df['description'] = df['description'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzFCg8BzbvKc"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7BsGKqGa1i1",
        "outputId": "667b9d38-eeab-4693-8644-68861ef84687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Job Requirements Conceptual understanding in Machine Learning models like Naive Bayes KMeans SVM Apriori Linear Logistic Regression Neural Random Forests Decision Trees KNN along with handson experience in at least 2 of them Intermediate to expert level coding skills in PythonR Ability to write functions clean and efficient data manipulation are mandatory for this role Exposure to packages like NumPy SciPy Pandas Matplotlib etc in Python or GGPlot2 dplyr tidyR in R Ability to communicate Model findings to both Technical and NonTechnical stake holders Hands on experience in SQLHive or similar programming language Must show past work via GitHub Kaggle or any other published article Masters degree in StatisticsMathematicsComputer Science or any other quant specific field Apply Now</td>\n",
              "      <td>Data scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Job Description  As a Data Scientist 1 you will help us build machine learning models data pipelines and microservices to help our clients navigate their healthcare journey You will do so by empowering and improving the next generation of Accolade Applications and user experiences A day in the life  Work with a small agile team to design and develop mobile applications in an iterative fashion Work with a tightknit group of development team members in Seattle Contribute to best practices and help guide the future of our applications Operates effectively as a collaborative member of the development team Operates effectively as an individual for quick turnaround of enhancements and fixes Responsible for meeting expectations and deliverables on time with high quality Drive and implement new features within our mobile applications Perform thorough manual testing and writing test cases that cover all areas Identify new development toolsapproaches that will increase code quality efficiency and best practices Develop and champion the the development processes coding style guidelines and architectural designs necessary to innovate and maintain great product quality Effectively turns design documents and graphics into performant usable UI Demonstrates creative technical and analytical skills Demonstrates ability to communicate effectively in both technical and business environments  Qualifications  What we are looking for  Master s Degree in Computer Science Math or related field Computer Science fundamentals as illustrated through algorithm design problem solving and complexity analysis Must have 1 year realworld experience developing and deploying microservices or data pipelines Must have a fundamental understanding of key machine learning concepts such as accuracy measures crossvalidation and open source machine learning libraries Fluent in Python and SQL Proficient with writing unitfunctional tests and familiar with automation frameworks Experience with cloud infrastructure such as AWS or Azure is a plus Experience with distributed data pipelines such as a Spark is a plus Strong written and oral communication skills Desire and willingness to work in an Agile collaborative innovative flexible and teamoriented environment Handson detailoriented methodical  inquisitive A motivated selfstarter with a solid level of experience that quickly grasps complex challenges A skillful communicator with experience working with technical management teams  A service oriented person who thinks Customer First Fast fail entrepreneurial spirit Thrives in a fastpaced environment where continuous improvement is the norm and the bar for quality is extremely high Excited by the challenges of working in a product team undergoing rapid international growth Additional Information  What is important to us Creating an enduring company that is hyperfocused on our culture and making a meaningful impact in the lives of our employees members and customers The secret to our success is We find joy and purpose in serving others Making a difference in our members  and customers  lives is what we do Even when it s hard we do the right thing for the right reasons We are strong individually and together we re powerful Trusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways having fun along the way We roll up our sleeves and get stuff done Results motivate us And we arent afraid of the hard work or tough decisions needed to get us there We re boldly and relentlessly reinventing healthcare Were curious and act big  not afraid to knock down barriers or take calculated risks to change the world one person at a time All your information will be kept confidential according to EEO guidelines</td>\n",
              "      <td>Data Scientist I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a Data Scientist you will be working on consulting side of our business You will be responsible for analyzing large complex datasets and identify meaningful patterns that lead to actionable recommendations You will be performing thorough testing and validation of models and support various aspects of the business with data analytics Ability to do statistical modeling build predictive models and leverage machine learning algorithms This position will combine the typical Data Scientist math and analytical skills with research advanced business communication and presentation skills Primary job location is in Sacramento but workfromhome option is available  Qualifications Bachelors MS or PhD in a relevant field Computer Science Engineering Statistics Physics Applied Math Experience in R andor Python is preferred</td>\n",
              "      <td>Data Scientist - Entry Level</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4969  6756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng the incumbent will join the CalMHSA Mental Health Tech Suite Innovation INN Evaluation Team This large statewide multiyear study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team The incumbent will collaborate with faculty and other team researchers and will be expected to create under supervision and direction variables describing the usage of the apps the interactions between users and the effectiveness of the apps The incumbent will also be expected to interact with the vendors of the apps around data issues  The University of California Irvine is an Equal OpportunityAffirmative Action Employer advancing inclusive excellence All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability age protected veteran status or other protected categories covered by the UC nondiscrimination policy  Salary Monthly 496858  675583 Total Hours 85 MF Contract Position Final candidate subject to background check As a federal contractor UC Irvine is required to use EVerify to confirm the work status of individuals assigned to perform substantial work under certain federal contractssubcontracts  Please attach your resume</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Location USA   multiple locations 2 years of Analytics experience Understand business requirements and technical requirements Can handle data extraction preparation and transformation Create and implement data models</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description                         title\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Job Requirements Conceptual understanding in Machine Learning models like Naive Bayes KMeans SVM Apriori Linear Logistic Regression Neural Random Forests Decision Trees KNN along with handson experience in at least 2 of them Intermediate to expert level coding skills in PythonR Ability to write functions clean and efficient data manipulation are mandatory for this role Exposure to packages like NumPy SciPy Pandas Matplotlib etc in Python or GGPlot2 dplyr tidyR in R Ability to communicate Model findings to both Technical and NonTechnical stake holders Hands on experience in SQLHive or similar programming language Must show past work via GitHub Kaggle or any other published article Masters degree in StatisticsMathematicsComputer Science or any other quant specific field Apply Now               Data scientist \n",
              "1  Job Description  As a Data Scientist 1 you will help us build machine learning models data pipelines and microservices to help our clients navigate their healthcare journey You will do so by empowering and improving the next generation of Accolade Applications and user experiences A day in the life  Work with a small agile team to design and develop mobile applications in an iterative fashion Work with a tightknit group of development team members in Seattle Contribute to best practices and help guide the future of our applications Operates effectively as a collaborative member of the development team Operates effectively as an individual for quick turnaround of enhancements and fixes Responsible for meeting expectations and deliverables on time with high quality Drive and implement new features within our mobile applications Perform thorough manual testing and writing test cases that cover all areas Identify new development toolsapproaches that will increase code quality efficiency and best practices Develop and champion the the development processes coding style guidelines and architectural designs necessary to innovate and maintain great product quality Effectively turns design documents and graphics into performant usable UI Demonstrates creative technical and analytical skills Demonstrates ability to communicate effectively in both technical and business environments  Qualifications  What we are looking for  Master s Degree in Computer Science Math or related field Computer Science fundamentals as illustrated through algorithm design problem solving and complexity analysis Must have 1 year realworld experience developing and deploying microservices or data pipelines Must have a fundamental understanding of key machine learning concepts such as accuracy measures crossvalidation and open source machine learning libraries Fluent in Python and SQL Proficient with writing unitfunctional tests and familiar with automation frameworks Experience with cloud infrastructure such as AWS or Azure is a plus Experience with distributed data pipelines such as a Spark is a plus Strong written and oral communication skills Desire and willingness to work in an Agile collaborative innovative flexible and teamoriented environment Handson detailoriented methodical  inquisitive A motivated selfstarter with a solid level of experience that quickly grasps complex challenges A skillful communicator with experience working with technical management teams  A service oriented person who thinks Customer First Fast fail entrepreneurial spirit Thrives in a fastpaced environment where continuous improvement is the norm and the bar for quality is extremely high Excited by the challenges of working in a product team undergoing rapid international growth Additional Information  What is important to us Creating an enduring company that is hyperfocused on our culture and making a meaningful impact in the lives of our employees members and customers The secret to our success is We find joy and purpose in serving others Making a difference in our members  and customers  lives is what we do Even when it s hard we do the right thing for the right reasons We are strong individually and together we re powerful Trusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways having fun along the way We roll up our sleeves and get stuff done Results motivate us And we arent afraid of the hard work or tough decisions needed to get us there We re boldly and relentlessly reinventing healthcare Were curious and act big  not afraid to knock down barriers or take calculated risks to change the world one person at a time All your information will be kept confidential according to EEO guidelines              Data Scientist I\n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      As a Data Scientist you will be working on consulting side of our business You will be responsible for analyzing large complex datasets and identify meaningful patterns that lead to actionable recommendations You will be performing thorough testing and validation of models and support various aspects of the business with data analytics Ability to do statistical modeling build predictive models and leverage machine learning algorithms This position will combine the typical Data Scientist math and analytical skills with research advanced business communication and presentation skills Primary job location is in Sacramento but workfromhome option is available  Qualifications Bachelors MS or PhD in a relevant field Computer Science Engineering Statistics Physics Applied Math Experience in R andor Python is preferred  Data Scientist - Entry Level\n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4969  6756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng the incumbent will join the CalMHSA Mental Health Tech Suite Innovation INN Evaluation Team This large statewide multiyear study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team The incumbent will collaborate with faculty and other team researchers and will be expected to create under supervision and direction variables describing the usage of the apps the interactions between users and the effectiveness of the apps The incumbent will also be expected to interact with the vendors of the apps around data issues  The University of California Irvine is an Equal OpportunityAffirmative Action Employer advancing inclusive excellence All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability age protected veteran status or other protected categories covered by the UC nondiscrimination policy  Salary Monthly 496858  675583 Total Hours 85 MF Contract Position Final candidate subject to background check As a federal contractor UC Irvine is required to use EVerify to confirm the work status of individuals assigned to perform substantial work under certain federal contractssubcontracts  Please attach your resume                Data Scientist\n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Location USA   multiple locations 2 years of Analytics experience Understand business requirements and technical requirements Can handle data extraction preparation and transformation Create and implement data models                Data Scientist"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4xFZNtX1m2"
      },
      "source": [
        "## 2) Use Spacy to tokenize the listings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhUHuMr-X-II",
        "outputId": "6f2cd4b1-9343-4938-8a59-28281ac4fb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "##### Your Code Here #####\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.2->en_core_web_md==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGmY8jG3UjzG",
        "outputId": "8f1bd8e7-869b-4c69-f83c-8bd416a5fd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "pip install -U spacy==2.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy==2.2.2 in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.2) (7.3.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==2.2.2) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJqVOf8ST5fw"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IFZyQWhVC3u"
      },
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-ZQMy9VEJZ"
      },
      "source": [
        "# Tokenizer Pipe\n",
        "\n",
        "tokens = []\n",
        "\n",
        "\"\"\" Make them tokens \"\"\"\n",
        "for doc in tokenizer.pipe(df['description'], batch_size=500):\n",
        "    doc_tokens = [token.text for token in doc]\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['tokens'] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfaPSOk8VRvi",
        "outputId": "e8743dac-b2a9-4104-a499-fdf5884ec1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "df['tokens'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                        [Job, Requirements, Conceptual, understanding, in, Machine, Learning, models, like, Naive, Bayes, KMeans, SVM, Apriori, Linear, Logistic, Regression, Neural, Random, Forests, Decision, Trees, KNN, along, with, handson, experience, in, at, least, 2, of, them, Intermediate, to, expert, level, coding, skills, in, PythonR, Ability, to, write, functions, clean, and, efficient, data, manipulation, are, mandatory, for, this, role, Exposure, to, packages, like, NumPy, SciPy, Pandas, Matplotlib, etc, in, Python, or, GGPlot2, dplyr, tidyR, in, R, Ability, to, communicate, Model, findings, to, both, Technical, and, NonTechnical, stake, holders, Hands, on, experience, in, SQLHive, or, similar, programming, language, Must, show, past, work, via, GitHub, Kaggle, ...]\n",
              "1                                                                                       [Job, Description,  , As, a, Data, Scientist, 1, you, will, help, us, build, machine, learning, models, data, pipelines, and, microservices, to, help, our, clients, navigate, their, healthcare, journey, You, will, do, so, by, empowering, and, improving, the, next, generation, of, Accolade, Applications, and, user, experiences, A, day, in, the, life,  , Work, with, a, small, agile, team, to, design, and, develop, mobile, applications, in, an, iterative, fashion, Work, with, a, tightknit, group, of, development, team, members, in, Seattle, Contribute, to, best, practices, and, help, guide, the, future, of, our, applications, Operates, effectively, as, a, collaborative, member, of, the, development, team, ...]\n",
              "2    [As, a, Data, Scientist, you, will, be, working, on, consulting, side, of, our, business, You, will, be, responsible, for, analyzing, large, complex, datasets, and, identify, meaningful, patterns, that, lead, to, actionable, recommendations, You, will, be, performing, thorough, testing, and, validation, of, models, and, support, various, aspects, of, the, business, with, data, analytics, Ability, to, do, statistical, modeling, build, predictive, models, and, leverage, machine, learning, algorithms, This, position, will, combine, the, typical, Data, Scientist, math, and, analytical, skills, with, research, advanced, business, communication, and, presentation, skills, Primary, job, location, is, in, Sacramento, but, workfromhome, option, is, available,  , Qualifications, Bachelors, MS, ...]\n",
              "3                        [4969,  , 6756, a, monthContractUnder, the, general, supervision, of, Professors, Dana, Mukamel, and, Kai, Zheng, the, incumbent, will, join, the, CalMHSA, Mental, Health, Tech, Suite, Innovation, INN, Evaluation, Team, This, large, statewide, multiyear, study, will, evaluate, the, effectiveness, of, two, new, and, innovative, applications, offered, to, people, with, mental, health, conditions, which, include, opportunities, for, online, chatting, between, users, and, online, listeners, Responsibilities, of, the, incumbent, will, include, managing, and, analyzing, text, data, created, by, users, of, the, two, mental, health, applications, as, part, of, the, research, and, evaluation, objectives, of, the, team, The, incumbent, will, collaborate, with, faculty, and, ...]\n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Location, USA,   , multiple, locations, 2, years, of, Analytics, experience, Understand, business, requirements, and, technical, requirements, Can, handle, data, extraction, preparation, and, transformation, Create, and, implement, data, models]\n",
              "Name: tokens, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgCZNL_YycP"
      },
      "source": [
        "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PZ8Pj_YxcF"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "#Learn our Vocab\n",
        "vect.fit(df['description'])\n",
        "\n",
        "# Get sparse dtm\n",
        "dtm = vect.transform(df['description'])\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c05-hdS7Wbbp",
        "outputId": "90828362-0eca-4b9c-a99f-70eec71f65bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>12</th>\n",
              "      <th>2019</th>\n",
              "      <th>40</th>\n",
              "      <th>401k</th>\n",
              "      <th>ab</th>\n",
              "      <th>abilities</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>academic</th>\n",
              "      <th>access</th>\n",
              "      <th>accommodation</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>achieve</th>\n",
              "      <th>acquisition</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actionable</th>\n",
              "      <th>actively</th>\n",
              "      <th>activities</th>\n",
              "      <th>ad</th>\n",
              "      <th>addition</th>\n",
              "      <th>additional</th>\n",
              "      <th>address</th>\n",
              "      <th>adhoc</th>\n",
              "      <th>ads</th>\n",
              "      <th>advance</th>\n",
              "      <th>advanced</th>\n",
              "      <th>advertising</th>\n",
              "      <th>age</th>\n",
              "      <th>agencies</th>\n",
              "      <th>agency</th>\n",
              "      <th>agile</th>\n",
              "      <th>ai</th>\n",
              "      <th>algorithm</th>\n",
              "      <th>algorithms</th>\n",
              "      <th>allow</th>\n",
              "      <th>alongside</th>\n",
              "      <th>amazon</th>\n",
              "      <th>...</th>\n",
              "      <th>validation</th>\n",
              "      <th>value</th>\n",
              "      <th>values</th>\n",
              "      <th>variety</th>\n",
              "      <th>various</th>\n",
              "      <th>verbal</th>\n",
              "      <th>veteran</th>\n",
              "      <th>veterans</th>\n",
              "      <th>video</th>\n",
              "      <th>vision</th>\n",
              "      <th>visit</th>\n",
              "      <th>visual</th>\n",
              "      <th>visualization</th>\n",
              "      <th>visualizations</th>\n",
              "      <th>voice</th>\n",
              "      <th>walmart</th>\n",
              "      <th>want</th>\n",
              "      <th>way</th>\n",
              "      <th>ways</th>\n",
              "      <th>web</th>\n",
              "      <th>welcome</th>\n",
              "      <th>wide</th>\n",
              "      <th>work</th>\n",
              "      <th>workforce</th>\n",
              "      <th>working</th>\n",
              "      <th>workplace</th>\n",
              "      <th>works</th>\n",
              "      <th>world</th>\n",
              "      <th>worldwide</th>\n",
              "      <th>write</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>xc2xb7</th>\n",
              "      <th>xc2xbb</th>\n",
              "      <th>xefx83x98</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>york</th>\n",
              "      <th>youll</th>\n",
              "      <th>youre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   10  100  12  2019  40  401k  ...  xefx83x98  year  years  york  youll  youre\n",
              "0   0    0   0     0   0     0  ...          0     0      0     0      0      0\n",
              "1   0    0   0     0   0     0  ...          0     1      0     0      0      0\n",
              "2   0    0   0     0   0     0  ...          0     0      0     0      0      0\n",
              "3   0    0   0     0   0     0  ...          0     0      0     0      0      0\n",
              "4   0    0   0     0   0     0  ...          0     0      1     0      0      0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo1iH_UeY7_n"
      },
      "source": [
        "## 4) Visualize the most common word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1ht-lXakoIe",
        "outputId": "adbde74c-d574-47e5-c515-a8cc8c3db622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dependencies for the week (instead of conda)\n",
        "!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-30 15:12:39--  https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     137  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-30 15:12:39 (6.82 MB/s) - ‘requirements.txt’ saved [137/137]\n",
            "\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 6.6MB/s \n",
            "\u001b[?25hCollecting pyLDAvis==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 54.8MB/s \n",
            "\u001b[?25hCollecting spacy==2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 54.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 45.4MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 55.7MB/s \n",
            "\u001b[?25hCollecting squarify==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/2b/2e77c35326efec19819cd1d729540d4d235e6c2a3f37658288a363a67da5/squarify-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (50.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (7.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (8.5.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (20.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (19.0.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.2.5)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=a43924afaf60c5bf0bb2d945970578a559a9db649a55c7d71af11094804ab091\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: gensim, funcy, pyLDAvis, spacy, scikit-learn, seaborn, squarify\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: spacy 2.2.2\n",
            "    Uninstalling spacy-2.2.2:\n",
            "      Successfully uninstalled spacy-2.2.2\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: seaborn 0.11.0\n",
            "    Uninstalling seaborn-0.11.0:\n",
            "      Successfully uninstalled seaborn-0.11.0\n",
            "Successfully installed funcy-1.15 gensim-3.8.1 pyLDAvis-2.1.2 scikit-learn-0.22.2 seaborn-0.9.0 spacy-2.2.3 squarify-0.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn",
                  "spacy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQKRgs7koB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxm9u6S4kGCg",
        "outputId": "cb58556d-0d9c-4725-e82c-62b062327688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\"\"\"\n",
        "Import Statements\n",
        "\"\"\"\n",
        "\n",
        "# Base\n",
        "from collections import Counter\n",
        "import re\n",
        " \n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Libraries\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5LB00uyZKV5"
      },
      "source": [
        "##### Your Code Here \n",
        "def count(docs):\n",
        "\n",
        "        word_counts = Counter()\n",
        "        appears_in = Counter()\n",
        "        \n",
        "        total_docs = len(docs)\n",
        "\n",
        "        for doc in docs:\n",
        "            word_counts.update(doc)\n",
        "            appears_in.update(set(doc))\n",
        "\n",
        "        temp = zip(word_counts.keys(), word_counts.values())\n",
        "        \n",
        "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "        total = wc['count'].sum()\n",
        "\n",
        "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
        "        \n",
        "        wc = wc.sort_values(by='rank')\n",
        "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "        t2 = zip(appears_in.keys(), appears_in.values())\n",
        "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "        wc = ac.merge(wc, on='word')\n",
        "\n",
        "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "        \n",
        "        return wc.sort_values(by='rank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i9YH5-8jn4P"
      },
      "source": [
        "wc = count(df['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFV7bA_ilATl",
        "outputId": "588c6e00-9e73-4699-919a-d7e49457d32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "wc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>appears_in</th>\n",
              "      <th>count</th>\n",
              "      <th>rank</th>\n",
              "      <th>pct_total</th>\n",
              "      <th>cul_pct_total</th>\n",
              "      <th>appears_in_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>and</td>\n",
              "      <td>425</td>\n",
              "      <td>11531</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.054871</td>\n",
              "      <td>0.054871</td>\n",
              "      <td>0.997653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>to</td>\n",
              "      <td>422</td>\n",
              "      <td>6777</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.032249</td>\n",
              "      <td>0.087120</td>\n",
              "      <td>0.990610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>the</td>\n",
              "      <td>414</td>\n",
              "      <td>4945</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.023531</td>\n",
              "      <td>0.110651</td>\n",
              "      <td>0.971831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>of</td>\n",
              "      <td>420</td>\n",
              "      <td>4561</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.021704</td>\n",
              "      <td>0.132355</td>\n",
              "      <td>0.985915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>in</td>\n",
              "      <td>421</td>\n",
              "      <td>3448</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.148763</td>\n",
              "      <td>0.988263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
              "19   and         425  11531   1.0   0.054871       0.054871        0.997653\n",
              "90    to         422   6777   2.0   0.032249       0.087120        0.990610\n",
              "167  the         414   4945   3.0   0.023531       0.110651        0.971831\n",
              "55    of         420   4561   4.0   0.021704       0.132355        0.985915\n",
              "40    in         421   3448   5.0   0.016408       0.148763        0.988263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-n4NNHslLAw",
        "outputId": "a28738fd-c789-43d6-eb4c-884c188172df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['pct_total'], label=wc_top20['word'], alpha=.8 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwFsTqrVZMYi"
      },
      "source": [
        "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gx2gZCbl5Np",
        "outputId": "e55938b1-e1ef-462f-8877-4ce2d50e60c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Instantiate vectorizer object\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "#tfidf = TfidfVectorizer(max_features=1000)  # To compare to above\n",
        "\n",
        "# Create a vocabulary and get word counts per document\n",
        "# Similiar to fit_predict\n",
        "dtm = tfidf.fit_transform(df['description'])\n",
        "\n",
        "# Print word counts\n",
        "\n",
        "# Get feature names to use as dataframe column headers\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "# View Feature Matrix as DataFrame\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>02</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>100000</th>\n",
              "      <th>1020</th>\n",
              "      <th>105000</th>\n",
              "      <th>1079302</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>125000</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>140000</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>180</th>\n",
              "      <th>1800flowerscom</th>\n",
              "      <th>1strategy</th>\n",
              "      <th>20</th>\n",
              "      <th>200</th>\n",
              "      <th>2000</th>\n",
              "      <th>200000</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2015</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>247</th>\n",
              "      <th>25</th>\n",
              "      <th>30</th>\n",
              "      <th>300</th>\n",
              "      <th>30328</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>xbox</th>\n",
              "      <th>xc2xb7</th>\n",
              "      <th>xc2xbb</th>\n",
              "      <th>xdivisionfunction</th>\n",
              "      <th>xe2x80x98big</th>\n",
              "      <th>xe2x80x98real</th>\n",
              "      <th>xe2x80x98think</th>\n",
              "      <th>xe2x80x9cbest</th>\n",
              "      <th>xe2x80x9cbig</th>\n",
              "      <th>xe2x80x9clive</th>\n",
              "      <th>xe2x80x9cmachine</th>\n",
              "      <th>xe2x80x9cmake</th>\n",
              "      <th>xe2x80x9cplantxe2x80x9d</th>\n",
              "      <th>xe2x80x9csurge</th>\n",
              "      <th>xe2x80x9cteam</th>\n",
              "      <th>xe2x80x9ctraditionalxe2x80x9d</th>\n",
              "      <th>xe2x80x9cwhole</th>\n",
              "      <th>xe2x80x9cwhyxe2x80x9d</th>\n",
              "      <th>xe2x80x9cwork</th>\n",
              "      <th>xefx82xa7</th>\n",
              "      <th>xefx83x98</th>\n",
              "      <th>xgboost</th>\n",
              "      <th>xpo</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yearthe</th>\n",
              "      <th>yes</th>\n",
              "      <th>yeti</th>\n",
              "      <th>york</th>\n",
              "      <th>youd</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>youre</th>\n",
              "      <th>youve</th>\n",
              "      <th>yrs</th>\n",
              "      <th>zenreach</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zf</th>\n",
              "      <th>zillow</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045476</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111217</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    02   10  100  1000  100000  1020  ...  yrs  zenreach  zeus   zf  zillow  zurich\n",
              "0  0.0  0.0  0.0   0.0     0.0   0.0  ...  0.0       0.0   0.0  0.0     0.0     0.0\n",
              "1  0.0  0.0  0.0   0.0     0.0   0.0  ...  0.0       0.0   0.0  0.0     0.0     0.0\n",
              "2  0.0  0.0  0.0   0.0     0.0   0.0  ...  0.0       0.0   0.0  0.0     0.0     0.0\n",
              "3  0.0  0.0  0.0   0.0     0.0   0.0  ...  0.0       0.0   0.0  0.0     0.0     0.0\n",
              "4  0.0  0.0  0.0   0.0     0.0   0.0  ...  0.0       0.0   0.0  0.0     0.0     0.0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGeY0mFbLhrH"
      },
      "source": [
        "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "Mi4syjknLhrH",
        "outputId": "6df58332-27dc-46b5-fc15-0b8dc4ef09a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Fit on DTM\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbzFHH4cnRHA",
        "outputId": "974c8f0d-ff58-48ca-a567-fb4c0b323218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nn.kneighbors([dtm.iloc[0].values])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 1.28451201, 1.28621286, 1.30047779, 1.31185378]]),\n",
              " array([[  0, 276, 274, 366, 115]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLWlrncSna1r"
      },
      "source": [
        "ideal_ds_job = [\"\"\"\n",
        "Lead the exploitation of the player-tracking dataset, converting 2-Dimensions and 3-Dimensions raw data into physical, technical and tactical basketball concepts and insights.\n",
        "Develop models to accurately evaluate and forecast player and team performance. Contribute to the development of basketball thought and workflow optimization of the department and the organization as a whole.\n",
        "                \n",
        "\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV-Ece_5umna"
      },
      "source": [
        "new = tfidf.transform(ideal_ds_job)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFfahq0-uvQc",
        "outputId": "a6d692f1-8f63-4318-a474-123851cb6185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 26 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n14XDVrfuy00",
        "outputId": "8e065d2f-1a33-4041-917e-14a9231adbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "new.todense()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UApqCYFqu2o9",
        "outputId": "ee452795-8930-4a20-a0cc-cb5505673681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.32330898, 1.33534104, 1.34750933, 1.34750933, 1.34786949]]),\n",
              " array([[262, 353, 339, 333,   8]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cohBKGQNu4yS",
        "outputId": "7e0d2058-e2eb-44bd-8e95-40360ecae966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df['description'][262]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ARVR team at Facebook is helping more people around the world come together and connect through worldclass hardware and software With global departments dedicated to research computer vision haptics social interaction and more ARVR is committed to driving the state of the art forward through relentless innovation  The ARVR Core Tech Org explores develops and helps deliver cuttingedge technologies that serve as the foundation of current and future ARVR products From mixed reality and human interaction to natural inputs and beyond Core Tech is focused on taking new technologies from early concept to the product level while iterating prototyping and realizing the human value and new experiences they open up  Were looking for a creative Data Scientist to help usher in the next era of humancomputer interaction The Data Scientist will be responsible for architecting and implementing endtoend datadriven solutions to drive algorithm development into the expanding landscape of virtual and augmented reality They will work with large amounts of structured and unstructured data and will help direct data collection efforts based on continuous regression testing to identify opportunities for improvement RESPONSIBILITIES Partner with a crossfunctional team of computer vision engineers product managers and data collection coordinators to formulate the definition of perceptually good positional tracking Provide guidance to data collection teams on test protocols including design of experiments number of samples and protocols for data collection Implement analyze and evaluate telemetry from devices in the field to determine positional tracking metrics meet KPI thresholds Align tracking performance with user perception by identifying usercentric dimensions to capture Identify tracking performance regressions and automate alerts to product and engineering stakeholders Summarize algorithm development training and validation of the algorithms in technical reports MINIMUM QUALIFICATIONS Degree in Computer Science Electrical Engineering Mathematics Physics Robotics or related technical field 4 years working with data analysis languages such as SQL Python and R Experience in C development Experience working with unstructured data like imagesvideos Experience with algorithm evaluation and determination of success criteria Experience working independently with minimal guidance PREFERRED QUALIFICATIONS Working knowledge of PHPHack Experience working in the field of computer vision hardware andor robotics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdEdDrYLvHpn",
        "outputId": "48b7bcfb-a3b8-41c7-d414-8891e47e0be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "df['description'][339]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'At YETI we believe that time spent outdoors matters more than ever and our gear can make that time extraordinary When you work here youll have the opportunity to create exceptional meaningful work and problem solve with innovative team members by your side Together youll help our customers get the highquality gear they need to make the most of their adventures We are built for the wild  The Data Scientist will lead the development and delivery of statistical forecast modeling and advanced analytics capabilities for YETI in Sales Supply Chain Demand Planning Marketing and ECommerce This position will primarily leverage R and R Studio to develop operational forecasts and advanced analytics to be imbedded in functional dashboards to drive insights throughout the organization  Responsibilities  Develop and maintain algorithms and statistical models for demand in Sales Marketing Supply Chain and Demand Planning Establish predictive analytics utilizing statistical models and hybrid models to improve forecast effectiveness Assess internal and external causal data to improve forecast quality Identify new opportunities to expand utilization of advanced techniques and tools in the demand planning process ie machine learning artificial intelligence pattern recognition optimization etc Conduct data mining and enhancing data collection that is relevant for building analytic systems Data harmonization with third party sources Perform Adhoc analysis and present results effectively  Qualifications and Attributes  Masters or Undergraduate Degree in Data Science Applied Mathematics Statistics Computer Science Engineering Operations Research or other closely related field 24 years of experience as a Data Scientist or closely relevant role performing the following Applying data mining methodologies Machine Learning Classification Clustering Association Regression etc Selecting features building and optimizing classifiers using artificial intelligence and machine learning techniques Predictive analytics Bayesian modeling Time Series modeling Neural Networks Gradient Boosting Stochastic modeling Graph Analysis etc Equivalent combination of education training and relevant experience may be considered in place of the requirements above Proficiency in analytical tools supporting data analysis reporting and visualization Tableau PowerBI Shiny Proficiency in highlevel programming languages for numerical  statistical computation and visualization R Python Experience working with large data sets and big data  distributed computing platforms Hadoop Hive Spark Working knowledge of retail and syndicated sales data Knowledge of predictive analytics machine learning data discovery data structure optimization and time series analysis Analytical Acumen adept at simplifying complex problems uncovering root causes linking causality to provide predictive guidance based on data must be able to harmonize disparate information and recognizelink patterns to prescribe recommended actions to achieve business results Business Acumen Demonstrates understanding of sales operations and financial aspects of the business ability to analyze business processes and to recommend and implement process improvements Ability to collaborate with IT to prepare and manipulate structured and unstructured data for data discovery and mining from multiple disparate resources Ability to create informative visualizations that display large amounts of data andor complex relationships in simple easily consumable dashboards Strong communication skills with the ability to translate data analytics into coherent reports and presentations for internal and external customers with varying degrees of technical knowledge Collaborative with strong team orientation Ability to business partner and influence at all levels of the organization  YETI is an Equal Opportunity Employer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81rptfmnvUF6",
        "outputId": "a486388b-14dc-4c52-f943-38ca58cdf7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df['description'][8]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MS in a quantitative discipline such as Statistics Mathematics Physics Engineering Computer Science or Economics5 years work experienceProficiency in at least one statistical software package such as Python R or MatlabExpertise using SQL for acquiring and transforming dataOutstanding quantitative modeling and statistical analysis skillsExcellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists engineering teams and business audiences  Where will Amazons growth come from in the next year What about over the next five Which product lines are poised to quintuple in size Are we investing enough in our infrastructure or too much How do our customers react to changes in prices product selection or delivery times These are among the most important questions at Amazon today The Topline Forecast team in the Supply Chain Optimization Technologies SCOT organization is dedicated to answering these questions using statistical methods We develop cutting edge data pipelines build accurate predictive models and deploy automated software solutions to provide forecasting insights to business leaders at the most senior levels throughout the company We are looking for a talented driven and analytical researcher to help us answer these and many more questions This Data Scientist role will design quantitative systems and forecasting models that generate multibillion dollar predictions of the highest level of visibility and importance for Amazons financial and operational planning A successful candidate will be a problem solver who enjoys diving into data is excited by difficult modeling challenges and possesses strong communication skills to effectively interface between technical and business teams As a Data Scientist on the Topline team you will collaborate directly with economists and statisticians to produce modeling solutions you will partner with software developers and data engineers to build endtoend data pipelines and production code and you will have exposure to senior leadership as we communicate results and provide scientific guidance to the business Key Responsibilities Implement statistical and machine learning methods to solve specific business problemsImprove upon existing methodologies by developing new data sources testing model enhancements and finetuning model parametersDirectly contribute to the design and development of automated forecasting systemsBuild customerfacing reporting tools to provide insights and metrics which track forecast performance and explain varianceCollaborate with researchers software developers and business leaders to define product requirements provide analytical support and communicate feedback Amazon is an Equal Opportunity Employer   Minority  Female  Disability  Veteran  Gender Identity  Sexual Orientation  Experience building complex data visualizationsExperience working in commandline Linux environmentsExperience with objectoriented programming languagesExperience with causal inference applied time series modeling or machine learning forecasting applicationsStrong project management skills'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiDfTWceoRkH"
      },
      "source": [
        "## Stretch Goals\n",
        "\n",
        " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
        " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
        " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
        " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
        "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
        " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
      ]
    }
  ]
}