{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Sprint Challenge*\n",
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "**Part 1 - Working with Text Data**\n",
    "Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[\\s]+', ' ', whitespace_string.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [],
   "source": [
    "request = requests.get('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt')\n",
    "date = request.text.replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'March 8, 2015\\nMarch 15, 2015\\nMarch 22, 2015\\nMarch 29, 2015\\nApril 5, 2015\\nApril 12, 2015\\nApril 19, 2015\\nApril 26, 2015\\nMay 3, 2015\\nMay 10, 2015\\nMay 17, 2015\\nMay 24, 2015\\nMay 31, 2015\\nJune 7, 2015\\nJune 14, 2015\\nJune 21, 2015\\nJune 28, 2015\\nJuly 5, 2015\\nJuly 12, 2015\\nJuly 19, 2015'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = [re.findall(r'[\\d]{1,2},', i)[0] for i in date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8,',\n",
       " '15',\n",
       " '22',\n",
       " '29',\n",
       " '5,',\n",
       " '12',\n",
       " '19',\n",
       " '26',\n",
       " '3,',\n",
       " '10',\n",
       " '17',\n",
       " '24',\n",
       " '31',\n",
       " '7,',\n",
       " '14',\n",
       " '21',\n",
       " '28',\n",
       " '5,',\n",
       " '12',\n",
       " '19']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [re.findall(r'[A-Z][a-z]+', i)[0] for i in date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March',\n",
       " 'March',\n",
       " 'March',\n",
       " 'March',\n",
       " 'April',\n",
       " 'April',\n",
       " 'April',\n",
       " 'April',\n",
       " 'May',\n",
       " 'May',\n",
       " 'May',\n",
       " 'May',\n",
       " 'May',\n",
       " 'June',\n",
       " 'June',\n",
       " 'June',\n",
       " 'June',\n",
       " 'July',\n",
       " 'July',\n",
       " 'July']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [re.findall(r'[\\d]{4}', i)[0] for i in date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015',\n",
       " '2015']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Day' : day, 'Month': month, 'Year': year})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8,</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15,</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22,</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29,</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12,</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19,</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26,</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3,</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10,</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17,</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24,</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31,</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7,</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14,</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21,</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28,</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5,</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12,</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19,</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Day  Month  Year\n",
       "0    8,  March  2015\n",
       "1   15,  March  2015\n",
       "2   22,  March  2015\n",
       "3   29,  March  2015\n",
       "4    5,  April  2015\n",
       "5   12,  April  2015\n",
       "6   19,  April  2015\n",
       "7   26,  April  2015\n",
       "8    3,    May  2015\n",
       "9   10,    May  2015\n",
       "10  17,    May  2015\n",
       "11  24,    May  2015\n",
       "12  31,    May  2015\n",
       "13   7,   June  2015\n",
       "14  14,   June  2015\n",
       "15  21,   June  2015\n",
       "16  28,   June  2015\n",
       "17   5,   July  2015\n",
       "18  12,   July  2015\n",
       "19  19,   July  2015"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[is, so, sad, for, my, APL, friend, ..., ..., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[I, missed, the, New, Moon, trailer, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, its, already, 7:30, :, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, !, !,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "      <td>[or, i, just, worry, too, much, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "      <td>[Juuuuuuuuuuuuuuuuussssst, Chillin, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "      <td>[Sunny, Again, Work, Tomorrow, :, -|, TV, Toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "      <td>[handed, in, my, uniform, today, ., i, miss, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "      <td>[hmmmm, ..., ., i, wonder, how, she, my, numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I must think about positive..</td>\n",
       "      <td>[I, must, think, about, positive..]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face a...</td>\n",
       "      <td>[thanks, to, all, the, haters, up, in, my, fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "      <td>[this, weekend, has, sucked, so, far]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more!</td>\n",
       "      <td>[jb, isnt, showing, in, australia, any, more, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win.</td>\n",
       "      <td>[ok, thats, it, you, win, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;-------- This is the way i feel right ...</td>\n",
       "      <td>[&amp;, lt, ;, --, --, --, --, This, is, the, way,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man.... I'm completely useless rt no...</td>\n",
       "      <td>[awhhe, man, ..., ., I, 'm, completely, useles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Feeling strangely fine. Now I'm gonna go l...</td>\n",
       "      <td>[Feeling, strangely, fine, ., Now, I, 'm, gon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>HUGE roll of thunder just now...SO scary!!!!</td>\n",
       "      <td>[HUGE, roll, of, thunder, just, now, ..., SO, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>I just cut my beard off. It's only been gr...</td>\n",
       "      <td>[I, just, cut, my, beard, off, ., It, 's, only...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText  \\\n",
       "0           0                       is so sad for my APL frie...   \n",
       "1           0                     I missed the New Moon trail...   \n",
       "2           1                            omg its already 7:30 :O   \n",
       "3           0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           0           i think mi bf is cheating on me!!!   ...   \n",
       "5           0                  or i just worry too much?           \n",
       "6           1                 Juuuuuuuuuuuuuuuuussssst Chillin!!   \n",
       "7           0         Sunny Again        Work Tomorrow  :-|  ...   \n",
       "8           1        handed in my uniform today . i miss you ...   \n",
       "9           1           hmmmm.... i wonder how she my number @-)   \n",
       "10          0                      I must think about positive..   \n",
       "11          1        thanks to all the haters up in my face a...   \n",
       "12          0                     this weekend has sucked so far   \n",
       "13          0             jb isnt showing in australia any more!   \n",
       "14          0                               ok thats it you win.   \n",
       "15          0      &lt;-------- This is the way i feel right ...   \n",
       "16          0      awhhe man.... I'm completely useless rt no...   \n",
       "17          1      Feeling strangely fine. Now I'm gonna go l...   \n",
       "18          0       HUGE roll of thunder just now...SO scary!!!!   \n",
       "19          0      I just cut my beard off. It's only been gr...   \n",
       "\n",
       "                                            Tokenized  \n",
       "0   [is, so, sad, for, my, APL, friend, ..., ..., ...  \n",
       "1           [I, missed, the, New, Moon, trailer, ...]  \n",
       "2                     [omg, its, already, 7:30, :, O]  \n",
       "3   [.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...  \n",
       "4   [i, think, mi, bf, is, cheating, on, me, !, !,...  \n",
       "5                  [or, i, just, worry, too, much, ?]  \n",
       "6           [Juuuuuuuuuuuuuuuuussssst, Chillin, !, !]  \n",
       "7   [Sunny, Again, Work, Tomorrow, :, -|, TV, Toni...  \n",
       "8   [handed, in, my, uniform, today, ., i, miss, y...  \n",
       "9   [hmmmm, ..., ., i, wonder, how, she, my, numbe...  \n",
       "10                [I, must, think, about, positive..]  \n",
       "11  [thanks, to, all, the, haters, up, in, my, fac...  \n",
       "12              [this, weekend, has, sucked, so, far]  \n",
       "13   [jb, isnt, showing, in, australia, any, more, !]  \n",
       "14                       [ok, thats, it, you, win, .]  \n",
       "15  [&, lt, ;, --, --, --, --, This, is, the, way,...  \n",
       "16  [awhhe, man, ..., ., I, 'm, completely, useles...  \n",
       "17  [Feeling, strangely, fine, ., Now, I, 'm, gon,...  \n",
       "18  [HUGE, roll, of, thunder, just, now, ..., SO, ...  \n",
       "19  [I, just, cut, my, beard, off, ., It, 's, only...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     is so sad for my APL friend.............'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    ''.join(re.sub(\"(@[A-Za-z0-9]+)\",\"\", data.loc[i][1].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tokenized'] = data['SentimentText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>[is, so, sad, for, my, APL, friend, ..., ..., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>[I, missed, the, New, Moon, trailer, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>[omg, its, already, 7:30, :, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>[.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, !, !,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "      <td>[or, i, just, worry, too, much, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "      <td>[Juuuuuuuuuuuuuuuuussssst, Chillin, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "      <td>[Sunny, Again, Work, Tomorrow, :, -|, TV, Toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "      <td>[handed, in, my, uniform, today, ., i, miss, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "      <td>[hmmmm, ..., ., i, wonder, how, she, my, numbe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0                       is so sad for my APL frie...   \n",
       "1          0                     I missed the New Moon trail...   \n",
       "2          1                            omg its already 7:30 :O   \n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4          0           i think mi bf is cheating on me!!!   ...   \n",
       "5          0                  or i just worry too much?           \n",
       "6          1                 Juuuuuuuuuuuuuuuuussssst Chillin!!   \n",
       "7          0         Sunny Again        Work Tomorrow  :-|  ...   \n",
       "8          1        handed in my uniform today . i miss you ...   \n",
       "9          1           hmmmm.... i wonder how she my number @-)   \n",
       "\n",
       "                                           Tokenized  \n",
       "0  [is, so, sad, for, my, APL, friend, ..., ..., ...  \n",
       "1          [I, missed, the, New, Moon, trailer, ...]  \n",
       "2                    [omg, its, already, 7:30, :, O]  \n",
       "3  [.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...  \n",
       "4  [i, think, mi, bf, is, cheating, on, me, !, !,...  \n",
       "5                 [or, i, just, worry, too, much, ?]  \n",
       "6          [Juuuuuuuuuuuuuuuuussssst, Chillin, !, !]  \n",
       "7  [Sunny, Again, Work, Tomorrow, :, -|, TV, Toni...  \n",
       "8  [handed, in, my, uniform, today, ., i, miss, y...  \n",
       "9  [hmmmm, ..., ., i, wonder, how, she, my, numbe...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "#### Your Answer Here #####\n",
    "\n",
    "\n",
    "TF-IDF scores determine how important a word is to a document, in a collection of documents.\n",
    "\n",
    "TF-IDF calculates the importance of the word dpending on the frequency the word appears in a document. If the word appears in multiple documents (also known as a corpus) the word is less important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "X_train = data['SentimentText']\n",
    "y_train = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(max_features=None,\n",
    "                           ngram_range=(1,1),\n",
    "                           stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(tfid,\n",
    "                         GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC train score:  0.7323111899794309\n",
      "ROC AUC test score:  0.717155859771513\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC train score: ', roc_auc_score(train_pred, y_train))\n",
    "print('ROC AUC test score: ', roc_auc_score(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donaldo\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(data['Tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donaldo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.7177311778068542),\n",
       " ('everyone', 0.6967092156410217),\n",
       " ('myspace', 0.6674982905387878),\n",
       " ('Twitter', 0.6450036764144897),\n",
       " ('someone', 0.6031166315078735),\n",
       " ('everything', 0.5901314616203308),\n",
       " ('msn', 0.5884891748428345),\n",
       " ('message', 0.5767815113067627),\n",
       " ('who', 0.5759022235870361),\n",
       " ('phone', 0.5758541822433472)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
