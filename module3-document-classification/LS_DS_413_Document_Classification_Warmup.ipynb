{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification Warmup\n",
    "## *Data Science Unit 4 Sprint 1 Module 3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1 - Extract Text Features and Use Them In Classification Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the locally saved file from the link above\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the feature and target variables\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x2864 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3051 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the tf-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate and fit the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "# Vectorize the training and testing data\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "# Display the properties of the vectorized text\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "# Import the classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate and fit a model\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.611"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the Pipeline\n",
    "pipe = Pipeline([('vect', vectorizer), # vectorizer\n",
    "                 ('clf', classifier) # classifier\n",
    "                ])\n",
    "\n",
    "# Define the parameter space for the grid serach\n",
    "parameters = {'clf__C': [1, 10, 1000000]} # C: regularization strength\n",
    "\n",
    "\n",
    "# Implement a grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Print out the best score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2 - Apply Latent Semantic Indexing (LSA) to a Document Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent semantic analysis (LSA)\n",
    "- Also referred to as *latent semantic indexing* (LSI)\n",
    "- Find a set of concepts that are common to all of the documents in the corpus\n",
    "- A word count or some vector representation is determined for each document\n",
    "- Similarity between documents is then calculated as the cosine similarity between two document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "- Previously, each yelp review could be thought of as a document\n",
    "- For each document, the tf-idf vector is calculated\n",
    "- In the resulting matrix, each row is a word in the corpus and each column is an individual document\n",
    "- A large corpus would produce a large matrix with a lot of unnecessary information\n",
    "- SVD is used to find the most important \"parts\" by reducing the number of rows (words) while preserving enough information for later (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read in the locally saved file from the link above\n",
    "\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Instantiate the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Instantiate the LSA (SVD) algorithm (defaults)\n",
    "svd = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.607"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# LSA part\n",
    "lsa = Pipeline([('vect', vectorizer), ('svd', svd)])\n",
    "\n",
    "# Combine into one pipeline\n",
    "pipe = Pipeline([('lsa', lsa), ('clf', classifier)])\n",
    "\n",
    "# Define the parameter space for the grid search\n",
    "parameters = {\n",
    "    'lsa__svd__n_components': (100,250),\n",
    "    'lsa__vect__max_df': (0.9, 1.0), # max document frequency\n",
    "}\n",
    "\n",
    "# Implement a grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# Display the best score from the grid-search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object 3 - Benchmark and Compare Various Vectorization Methods in Document Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "- Represent individual words as vectors (each element represents a different component)\n",
    "- The overall sentence vector is the average of all word vectors in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and the large pretrained model (includes word embeddings)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy including word embeddings:  0.856\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the locally saved file from UCI website\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Function to return the vector for each sentence in a document\n",
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]\n",
    "\n",
    "# Get the vectors for each sentence (mean of all the word vectors)\n",
    "X_train = get_word_vectors(sentences_train)\n",
    "X_test = get_word_vectors(sentences_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "# Print out the accuracy score\n",
    "print(\"Accuracy including word embeddings: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
