{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 1, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification (Assignment)\n",
    "\n",
    "This notebook is for you to practice skills during lecture.\n",
    "\n",
    "Today's guided module project and assignment will be different. You already know how to do classification. You ready know how to extract features from documents. So? That means you're ready to combine and practice those skills in a kaggle competition. We we will open with a five minute sprint explaining the competition, and then give you 25 minutes to work. After those twenty five minutes are up, I will give a 5-minute demo an NLP technique that will help you with document classification (*and **maybe** the competition*).\n",
    "\n",
    "Today's all about having fun and practicing your skills.\n",
    "\n",
    "## Sections\n",
    "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
    "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
    "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy\n",
    "* <a href=\"#p4\">Part 4</a>: Post Lecture Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction & Classification Pipelines (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along \n",
    "\n",
    "What you should be doing now:\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model (try using the pipe method I just demoed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to change the path\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train['description'] = [x.replace('\\n', '') for x in train['description']]\n",
    "test['description'] = [x.replace('\\n', '') for x in test['description']]\n",
    "\n",
    "train, val = train_test_split(train, test_size=.8, train_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>3164</td>\n",
       "      <td>Hot wet oak, berries, Red Hots, and wet cornme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3099</td>\n",
       "      <td>This 20 year old expression of Tullibardine ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>338</td>\n",
       "      <td>The fifth in a series of Glenlivet Cellar Coll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>743</td>\n",
       "      <td>With the quintessential traditional bourbon bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>2565</td>\n",
       "      <td>A release chosen by Douglas Maclean (and named...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  ratingCategory\n",
       "3453  3164  Hot wet oak, berries, Red Hots, and wet cornme...               1\n",
       "1997  3099  This 20 year old expression of Tullibardine ha...               1\n",
       "3699   338  The fifth in a series of Glenlivet Cellar Coll...               1\n",
       "3762   743  With the quintessential traditional bourbon bo...               1\n",
       "3673  2565  A release chosen by Douglas Maclean (and named...               1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>1711</td>\n",
       "      <td>Glenrothes is always this fascinating mix of t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>4403</td>\n",
       "      <td>South Africa’s most established distillery now...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>363</td>\n",
       "      <td>Full-flavored, confident, and very dynamic. Br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1280</td>\n",
       "      <td>Rich amber in hue, sweet, and mellow with init...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>3853</td>\n",
       "      <td>Who could refuse a blend of single malts wholl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  ratingCategory\n",
       "863   1711  Glenrothes is always this fascinating mix of t...               1\n",
       "2791  4403  South Africa’s most established distillery now...               0\n",
       "2006   363  Full-flavored, confident, and very dynamic. Br...               1\n",
       "2468  1280  Rich amber in hue, sweet, and mellow with init...               1\n",
       "2756  3853  Who could refuse a blend of single malts wholl...               0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def tokenize(document):\n",
    "    \n",
    "    doc = nlp(document)\n",
    "    \n",
    "    return [token.lemma_.strip().lower() for token in doc\n",
    "            if (token.is_stop != True) \n",
    "            and (token.is_punct != True) \n",
    "            and (token.like_num != True)\n",
    "            and (token.is_currency != True)\n",
    "           and (token.is_digit != True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words=spacy.lang.en.stop_words.STOP_WORDS, ngram_range=(1,2))\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "pipe = Pipeline([('vect', vect), ('clf', rfc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Search Space\n",
    "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:    3.4s finished\n",
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('vect',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              stop_words={\"'d\",\n",
       "                                                                          \"'l...\n",
       "                                                                     random_state=None,\n",
       "                                                                     verbose=0,\n",
       "                                                                     warm_start=False))],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-2,\n",
       "                   param_distributions={'clf__max_depth': (15, 20),\n",
       "                                        'clf__n_estimators': (5, 10),\n",
       "                                        'vect__max_df': (0.75, 1.0),\n",
       "                                        'vect__max_features': (500, 1000),\n",
       "                                        'vect__min_df': (0.02, 0.05)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': ( 0.75, 1.0),\n",
    "    'vect__min_df': (.02, .05),\n",
    "    'vect__max_features': (500,1000),\n",
    "    'clf__n_estimators':(5, 10,),\n",
    "    'clf__max_depth':(15,20)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe,parameters, cv=5, n_jobs=-2, verbose=1)\n",
    "random_search.fit(train['description'], train['ratingCategory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission File\n",
    "*Note:* In a typical Kaggle competition, you are only allowed two submissions a day, so you only submit if you feel you cannot achieve higher test accuracy. For this competition the max daily submissions are capped at **20**. Submit for each demo and for your assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030581039755351"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions on test sample\n",
    "pred = random_search.predict(val['description'])\n",
    "accuracy_score(val['ratingCategory'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__min_df': 0.02,\n",
       " 'vect__max_features': 1000,\n",
       " 'vect__max_df': 1.0,\n",
       " 'clf__n_estimators': 10,\n",
       " 'clf__max_depth': 15}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.predict(test['description'])\n",
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  ratingCategory\n",
       "0  3461               1\n",
       "1  2604               1\n",
       "2  3341               1\n",
       "3  3764               1\n",
       "4  2306               1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Sure the Category is an Integer\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subNumber = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "\n",
    "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
    "subNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You're trying to achieve a minimum of 70% Accuracy on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model & try: \n",
    "    - Creating a Text Extraction & Classification Pipeline\n",
    "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
    "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
    "4. Make a submission to Kaggle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words=spacy.lang.en.stop_words.STOP_WORDS, ngram_range=(1,2))\n",
    "svd = TruncatedSVD()\n",
    "clf = RandomForestClassifier()\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Search Space\n",
    "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:    2.1s finished\n",
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('lsi',\n",
       "                                              Pipeline(memory=None,\n",
       "                                                       steps=[('vect',\n",
       "                                                               TfidfVectorizer(analyzer='word',\n",
       "                                                                               binary=False,\n",
       "                                                                               decode_error='strict',\n",
       "                                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                                               encoding='utf-8',\n",
       "                                                                               input='content',\n",
       "                                                                               lowercase=True,\n",
       "                                                                               max_df=1.0,\n",
       "                                                                               max_features=None,\n",
       "                                                                               min_df=1,\n",
       "                                                                               ngram_range=(1,\n",
       "                                                                                            2),\n",
       "                                                                               norm='l2',\n",
       "                                                                               preprocessor=None,\n",
       "                                                                               s...\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-2,\n",
       "                   param_distributions={'clf__max_depth': (5, 10, 15, 20),\n",
       "                                        'clf__n_estimators': (5, 10),\n",
       "                                        'lsi__svd__n_components': [10, 100,\n",
       "                                                                   250],\n",
       "                                        'lsi__vect__max_df': (0.75, 1.0),\n",
       "                                        'lsi__vect__max_features': (500, 1000),\n",
       "                                        'lsi__vect__min_df': (0.02, 0.05)},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('lsi', lsi), ('clf', clf)])\n",
    "\n",
    "parameters = {\n",
    "    'lsi__svd__n_components': [10,100,250],\n",
    "    'lsi__vect__max_df': (0.75, 1.0),\n",
    "    'clf__max_depth':(5,10,15,20),\n",
    "    'lsi__vect__min_df': (.02, .05),\n",
    "    'lsi__vect__max_features': (500,1000),\n",
    "    'clf__n_estimators':(5, 10,),\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, parameters, cv=5, n_jobs=-2, verbose=1)\n",
    "random_search.fit(train['description'], train['ratingCategory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7116207951070337"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions on test sample\n",
    "pred = random_search.predict(val['description'])\n",
    "accuracy_score(val['ratingCategory'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = grid_search.predict(test['description'])\n",
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  ratingCategory\n",
       "0  3461               0\n",
       "1  2604               0\n",
       "2  3341               1\n",
       "3  3764               1\n",
       "4  2306               1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Sure the Category is an Integer\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "\n",
    "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
    "subNumber += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.687882\n",
       "0    0.291310\n",
       "2    0.020808\n",
       "Name: ratingCategory, dtype: float64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ratingCategory'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with Spacy (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \n",
    "    doc = nlp(document)\n",
    "    \n",
    "    return [token.lemma_.strip().lower() for token in doc\n",
    "            if (token.is_stop != True) \n",
    "            and (token.is_punct != True) \n",
    "            and (token.like_num != True)\n",
    "            and (token.is_currency != True)\n",
    "           and (token.is_digit != True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your Dataset\n",
    "\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=spacy.lang.en.stop_words.STOP_WORDS, ngram_range=(1,4), tokenizer=LemmaTokenizer())\n",
    "svd = TruncatedSVD()\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
    "# lsi = Pipeline([('svd', svd)])\n",
    "# clf = GradientBoostingClassifier()\n",
    "clf = LogisticRegression(multi_class='multinomial', n_jobs=-1)\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', clf)])\n",
    "\n",
    "parameters = {\n",
    "    'lsi__svd__n_components': randint(10, 250),\n",
    "    'lsi__vect__max_df': uniform(0.75, 1.0),\n",
    "    'lsi__vect__min_df': uniform(.02, .05),\n",
    "    'lsi__vect__max_features': randint(500, 1000),\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'clf__solver': ['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'clf__class_weight': [{0: .5, 1: .3, 2: .2}, 'balanced'],\n",
    "    'clf__C': uniform(.1, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-2)]: Done 186 tasks      | elapsed:   58.0s\n",
      "[Parallel(n_jobs=-2)]: Done 436 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done 786 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-2)]: Done 1236 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-2)]: Done 1786 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-2)]: Done 2000 out of 2000 | elapsed: 10.1min finished\n",
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['d', 'doe', 'ha', 'le', 'll', 'm', 'n', 's', 't', 'u', 've', 'wa', '‘', '’'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('lsi',\n",
       "                                              Pipeline(memory=None,\n",
       "                                                       steps=[('vect',\n",
       "                                                               TfidfVectorizer(analyzer='word',\n",
       "                                                                               binary=False,\n",
       "                                                                               decode_error='strict',\n",
       "                                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                                               encoding='utf-8',\n",
       "                                                                               input='content',\n",
       "                                                                               lowercase=True,\n",
       "                                                                               max_df=1.0,\n",
       "                                                                               max_features=None,\n",
       "                                                                               min_df=1,\n",
       "                                                                               ngram_range=(1,\n",
       "                                                                                            4),\n",
       "                                                                               norm='l2',\n",
       "                                                                               preprocessor=None,...\n",
       "                                        'lsi__vect__max_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x13e166518>,\n",
       "                                        'lsi__vect__max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x157058d68>,\n",
       "                                        'lsi__vect__min_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1570584e0>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(pipe, \n",
    "                                   parameters, \n",
    "                                   cv=10, \n",
    "                                   n_iter=200,\n",
    "                                   n_jobs=-2, \n",
    "                                   verbose=True)\n",
    "random_search.fit(train['description'], train['ratingCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7223276121650105"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6960244648318042"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions on test sample\n",
    "pred = random_search.predict(val['description'])\n",
    "accuracy_score(val['ratingCategory'], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test sample\n",
    "pred = random_search.predict(test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  ratingCategory\n",
       "0  3461               1\n",
       "1  2604               1\n",
       "2  3341               1\n",
       "3  3764               1\n",
       "4  2306               1"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Sure the Category is an Integer\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "\n",
    "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
    "subNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "What you should be doing now:\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model & try: \n",
    "    - Creating a Text Extraction & Classification Pipeline\n",
    "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
    "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
    "    - Try to extract word embeddings with Spacy and use those embeddings as your features for a classification model.\n",
    "4. Make a submission to Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue Word Embedding Work Here\n",
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]\n",
    "\n",
    "X = get_word_vectors(train['description'])\n",
    "\n",
    "len(X) == len(train['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = get_word_vectors(val['description'])\n",
    "X_test = get_word_vectors(test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=spacy.lang.en.stop_words.STOP_WORDS, tokenizer=LemmaTokenizer())\n",
    "svd = TruncatedSVD()\n",
    "# lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
    "lsi = Pipeline([('svd', svd)])\n",
    "nn = KNeighborsClassifier(n_jobs=-1)\n",
    "cnb = ComplementNB()\n",
    "\n",
    "pipe = Pipeline([('lsi', lsi), ('nn', nn)])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'lsi__svd__n_components': randint(2, 250),\n",
    "    'lsi__svd__algorithm': ['randomized', 'arpack'],\n",
    "#     'lsi__vect__max_df': uniform(0.5, 1.0),\n",
    "#     'lsi__vect__min_df': uniform(.01, .1),\n",
    "#     'lsi__vect__max_features': randint(300, 1000),\n",
    "#     'lsi__vect__ngram_range': [(1,3), (2,4), (1,4), (2,3)],\n",
    "    'nn__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'nn__weights': ['uniform', 'distance'],\n",
    "    'nn__n_neighbors': randint(3,10),\n",
    "    'nn__metric': ['minkowski', 'euclidean', 'mahalanobis']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done 186 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-2)]: Done 436 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-2)]: Done 786 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-2)]: Done 1236 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done 1786 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-2)]: Done 2436 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-2)]: Done 3186 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-2)]: Done 4036 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-2)]: Done 4986 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-2)]: Done 6036 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-2)]: Done 7186 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-2)]: Done 8436 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-2)]: Done 9786 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-2)]: Done 10000 out of 10000 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('lsi',\n",
       "                                              Pipeline(memory=None,\n",
       "                                                       steps=[('svd',\n",
       "                                                               TruncatedSVD(algorithm='randomized',\n",
       "                                                                            n_components=2,\n",
       "                                                                            n_iter=5,\n",
       "                                                                            random_state=None,\n",
       "                                                                            tol=0.0))],\n",
       "                                                       verbose=False)),\n",
       "                                             ('nn',\n",
       "                                              KNeighborsClassifier(algorithm='auto',\n",
       "                                                                   leaf_size=30,\n",
       "                                                                   metric='minkowski',\n",
       "                                                                   metric_params=None,\n",
       "                                                                   n_jobs=-1,\n",
       "                                                                   n_neighbors=5,\n",
       "                                                                   p=2,\n",
       "                                                                   w...\n",
       "                                        'lsi__svd__n_components': <scipy.stats._distn_infrastructure.rv_frozen object at 0x123662828>,\n",
       "                                        'nn__algorithm': ['ball_tree',\n",
       "                                                          'kd_tree', 'brute'],\n",
       "                                        'nn__metric': ['minkowski', 'euclidean',\n",
       "                                                       'mahalanobis'],\n",
       "                                        'nn__n_neighbors': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1474c8550>,\n",
       "                                        'nn__weights': ['uniform', 'distance']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(pipe, \n",
    "                                   parameters, \n",
    "                                   cv=10, \n",
    "                                   n_iter=1000,\n",
    "                                   n_jobs=-2, \n",
    "                                   verbose=True)\n",
    "random_search.fit(X, train['ratingCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7417494730502859"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lsi__svd__algorithm': 'randomized',\n",
       " 'lsi__svd__n_components': 160,\n",
       " 'nn__algorithm': 'ball_tree',\n",
       " 'nn__metric': 'minkowski',\n",
       " 'nn__n_neighbors': 7,\n",
       " 'nn__weights': 'uniform'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7073394495412844"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = random_search.predict(X_val)\n",
    "accuracy_score(val['ratingCategory'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = random_search.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')\n",
    "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
    "subNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Lecture Assignment\n",
    "<a id=\"p4\"></a>\n",
    "\n",
    "Your primary assignment this afternoon is to achieve a minimum of 70% accuracy on the Kaggle competition. Once you have achieved 70% accuracy, please work on the following: \n",
    "\n",
    "1. Research \"Sentiment Analysis\". Provide answers in markdown to the following questions: \n",
    "    - What is \"Sentiment Analysis\"? \n",
    "    - Is Document Classification different than \"Sentiment Analysis\"? Provide evidence for your response\n",
    "    - How do create labeled sentiment data? Are those labels really sentiment?\n",
    "    - What are common applications of sentiment analysis?\n",
    "2. Research our why word embeddings worked better for the lecture notebook than on the whiskey competition.\n",
    "    - This [text classification documentation](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) from Google might be of interest\n",
    "    - Neural Networks are becoming more popular for document classification. Why is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
