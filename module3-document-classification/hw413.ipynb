{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LS DS 413 assignment  \n",
    "\n",
    "note: answers at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('projects/DS-Unit-4-Sprint-1-NLP/module2-vector-representations/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the 2s just for shiggles\n",
    "\n",
    "cond = df['ratingCategory'] != 2\n",
    "df = df[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(X):\n",
    "    \"\"\"\n",
    "        customized cleaning function\n",
    "        X: series\n",
    "    \n",
    "        return: np.array\n",
    "    \"\"\"\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    number = re.compile('[0-9]+\\s')  #remove freestanding numbers\n",
    "\n",
    "    X = X.map(lambda t : t.lower()) # lwercase text\n",
    "    X = X.map(lambda t : re.sub(\"\\\\\\\\n\", ' ', t))\n",
    "    X = X.map(lambda t : re.sub(\"\\\\\\\\x..\", '',t))   \n",
    "    X = X.map(lambda t : re.sub(\"^b[\\'\\\"]\", '',t))   \n",
    "    X = X.map(lambda t : BeautifulSoup(t, \"lxml\").text) # strip html tags\n",
    "    X = X.map(lambda t : REPLACE_BY_SPACE_RE.sub(' ', t))  # symbols by space in text\n",
    "    X = X.map(lambda t : BAD_SYMBOLS_RE.sub('', t)) # delete symbols which are in BAD_SYMBOLS_RE\n",
    "    X = X.map(lambda t : number.sub('', t))         # remove groups of digits\n",
    "    X = X.map(lambda t : ' '.join(word for word in t.split() if word not in STOPWORDS))# delete stopwords\n",
    "    return np.array(X)\n",
    "\n",
    "clean = FunctionTransformer(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('clean',\n",
       "                                        FunctionTransformer(accept_sparse=False,\n",
       "                                                            check_inverse=True,\n",
       "                                                            func=<function clean_text at 0x7fbad9fee510>,\n",
       "                                                            inv_kw_args=None,\n",
       "                                                            inverse_func=None,\n",
       "                                                            kw_args=None,\n",
       "                                                            validate=False)),\n",
       "                                       ('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>...\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__C': [0.5, 1],\n",
       "                         'clf__kernel': ('linear', 'poly', 'rbf'),\n",
       "                         'vect__max_df': (0.7, 0.75, 0.8),\n",
       "                         'vect__max_features': (300, 500, 600),\n",
       "                         'vect__min_df': (0.01, 0.02, 0.03)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcpipe = Pipeline([('clean', clean),\n",
    "                  ('vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                  ('clf', SVC())\n",
    "                   ])\n",
    " \n",
    "parameters = {\n",
    "    'vect__max_df': ( 0.7, 0.75, 0.8),\n",
    "    'vect__min_df': (.01, .02, .03),\n",
    "    'vect__max_features': (300,500,600),\n",
    "    'clf__kernel':('linear','poly','rbf'),\n",
    "    'clf__C':([.5,1])\n",
    "}\n",
    "data = df.description\n",
    "target = df.ratingCategory\n",
    "grid_search = GridSearchCV(svcpipe, parameters, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1,\n",
       " 'clf__kernel': 'rbf',\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__max_features': 600,\n",
       " 'vect__min_df': 0.01}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516170081731536"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(model.predict(test.description),index=test.id, columns =['ratingCategory'])\n",
    "sub.to_csv('2labelsvc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "\n",
    "pipe = Pipeline([('clean', clean),\n",
    "                 ('vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                 ('clf', RandomForestClassifier())\n",
    "                ])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': ( 0.05, 0.7, 0.75, 0.8),\n",
    "    'vect__min_df': (.01, .02, .03),\n",
    "    'vect__max_features': (300,500,800),\n",
    "    'clf__n_estimators':(8, 10,15),\n",
    "    'clf__max_depth':([15])\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   59.6s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('clean',\n",
       "                                        FunctionTransformer(accept_sparse=False,\n",
       "                                                            check_inverse=True,\n",
       "                                                            func=<function clean_text at 0x7fbad9fee510>,\n",
       "                                                            inv_kw_args=None,\n",
       "                                                            inverse_func=None,\n",
       "                                                            kw_args=None,\n",
       "                                                            validate=False)),\n",
       "                                       ('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>...\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__max_depth': [15],\n",
       "                         'clf__n_estimators': (8, 10, 15),\n",
       "                         'vect__max_df': (0.05, 0.7, 0.75, 0.8),\n",
       "                         'vect__max_features': (300, 500, 800),\n",
       "                         'vect__min_df': (0.01, 0.02, 0.03)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.description\n",
    "target = df.ratingCategory\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': 15,\n",
       " 'clf__n_estimators': 10,\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__min_df': 0.01}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371929174005748"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = pd.read_csv('projects/DS-Unit-4-Sprint-1-NLP/module2-vector-representations/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3461</td>\n",
       "      <td>\\nStyle: Speyside single malt scotch Color: Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2604</td>\n",
       "      <td>\\nVery bright and lively, with a nice balance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3341</td>\n",
       "      <td>\\nA new oloroso-forward Chivas positioned to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>\\nAged in bourbon casks and then enhanced in R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306</td>\n",
       "      <td>\\nThere is a freshness to the wood on the nose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                        description\n",
       "0  3461  \\nStyle: Speyside single malt scotch Color: Wa...\n",
       "1  2604  \\nVery bright and lively, with a nice balance ...\n",
       "2  3341  \\nA new oloroso-forward Chivas positioned to s...\n",
       "3  3764  \\nAged in bourbon casks and then enhanced in R...\n",
       "4  2306  \\nThere is a freshness to the wood on the nose..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(model.predict(test.description),index=test.id, columns =['ratingCategory'])\n",
    "sub.to_csv('3nd_rfc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,ratingCategory\n",
      "3461,1\n",
      "2604,1\n",
      "3341,1\n",
      "3764,1\n",
      "2306,1\n",
      "1438,1\n",
      "2571,1\n",
      "4148,1\n",
      "4452,1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, # Just here for demo. \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, # Just here for demo. \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=10)\n",
    "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "# LSI\n",
    "lsi = Pipeline([('clean', clean),\n",
    "                 ('vect', vect),\n",
    "                 ('svd', svd)\n",
    "                ])\n",
    "\n",
    "# Pipe\n",
    "rfc = RandomForestClassifier()\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', rfc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1202 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('lsi',\n",
       "                                        Pipeline(memory=None,\n",
       "                                                 steps=[('clean',\n",
       "                                                         FunctionTransformer(accept_sparse=False,\n",
       "                                                                             check_inverse=True,\n",
       "                                                                             func=<function clean_text at 0x7fbb4802aa60>,\n",
       "                                                                             inv_kw_args=None,\n",
       "                                                                             inverse_func=None,\n",
       "                                                                             kw_args=None,\n",
       "                                                                             validate=False)),\n",
       "                                                        ('vect',\n",
       "                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                         binary=False,\n",
       "                                                                         decode_error='s...\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__max_depth': (5, 15, 80),\n",
       "                         'clf__n_estimators': (50, 100),\n",
       "                         'lsi__svd__n_components': [50, 100],\n",
       "                         'lsi__vect__max_df': (0.7, 0.75, 0.8, 1),\n",
       "                         'lsi__vect__max_features': [100, 500, 1000],\n",
       "                         'lsi__vect__min_df': [0.05, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parameters = {'clf__max_depth': (5,15,80),\n",
    "     'clf__n_estimators': (50,100),\n",
    "     'lsi__vect__max_df': (0.7,0.75,0.8,1),\n",
    "     'lsi__vect__max_features': [100,500,1000],\n",
    "     'lsi__vect__min_df': [0.05, 0.01],\n",
    "    'lsi__svd__n_components': [50,100]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727430548281775"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': 80,\n",
       " 'clf__n_estimators': 50,\n",
       " 'lsi__svd__n_components': 50,\n",
       " 'lsi__vect__max_df': 0.8,\n",
       " 'lsi__vect__max_features': 1000,\n",
       " 'lsi__vect__min_df': 0.01}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsisub = pd.DataFrame(model2.predict(test.description),index=test.id, columns =['ratingCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('lsi_rfc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Lecture Assignment\n",
    "<a id=\"p4\"></a>\n",
    "\n",
    "Your primary assignment this afternoon is to achieve a minimum of 70% accuracy on the Kaggle competition. Once you have achieved 70% accuracy, please work on the following: \n",
    "\n",
    "1. Research \"Sentiment Analysis\". Provide answers in markdown to the following questions: \n",
    "    - What is \"Sentiment Analysis\"? \n",
    "    - Is Document Classification different than \"Sentiment Analysis\"? Provide evidence for your response\n",
    "    - How do create labeled sentiment data? Are those labels really sentiment?\n",
    "    - What are common applications of sentiment analysis?\n",
    "2. Research our why word embeddings worked better for the lecture notebook than on the whiskey competition.\n",
    "    - This [text classification documentation](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) from Google might be of interest\n",
    "    - Neural Networks are becoming more popular for document classification. Why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "  a. sentiment analysis is about trying to identify the emotional dimensions of natural language\n",
    "  b. document classification is different because in many cases the classes are more objective, more related to literal meaning and structure. Emotional content can be diffucult to parse and relies more on context and subtle nuance. For example the same word,  \"Great!\" can be taken literaly to mean i am ahppy aobut something. or in a different context 'Great.' can mean i am nonplussed about a situation, taken sarcastically. you would need context to detect this, and in speech flesh humans rely on tone, body languge to convey extra information.\n",
    "  c. we can create labeled sentiment data by letting humans evaluate the text and label it. they are a subjective approximation. a lot of scientific methodology would need to be used to get good data\n",
    "  d. common applications would be to gain customer feedback, on a product or service. to identify problems. to classify users in a community \n",
    "2. Neural nets seem offer a 'shortcut' to impressive performance on classification problems, since theire complexity can extract structure without the need for multiple human assisted steps. however in this regard they are somewhat of a black box, in that the exact causual process to arrive at a prediction is not easily translatable to human thinking. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
