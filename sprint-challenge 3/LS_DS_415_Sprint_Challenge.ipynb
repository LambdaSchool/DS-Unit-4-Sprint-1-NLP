{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \n",
    "    tokens = re.sub(r'[^a-zA-Z ^0-9]', '', doc)\n",
    "    tokens = tokens.lower().split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup(df_column, spec_chars_remove = []):\n",
    "    \"\"\"\n",
    "    Input: dataframe column and list of specific characters to remove, \n",
    "    \n",
    "    Output: List of cleaned observations\n",
    "    \"\"\"\n",
    "    \n",
    "    df_column = df_column.copy()\n",
    "    \n",
    "    soupy = [BeautifulSoup(df_column[ii], 'lxml').get_text() \n",
    "             for ii in range(df_column.shape[0])\n",
    "            ]\n",
    "    \n",
    "    for char in spec_chars_remove:\n",
    "        soupy = [soupy[ii].replace(char, ' ') \n",
    "                 for ii in range(len(soupy))\n",
    "                ]\n",
    "        \n",
    "    to_clean = ['[^A-Za-z ]+', '   ', '  ']\n",
    "    \n",
    "    for char in to_clean:\n",
    "        soupy = [re.sub(char, ' ', soupy[ii]) \n",
    "                 for ii in range(len(soupy))\n",
    "                ]\n",
    "        \n",
    "    df_feature = pd.Series([nlp(soupy[ii].lower().strip()) \n",
    "                            for ii in range(len(soupy))\n",
    "                           ])\n",
    "        \n",
    "    for row in range(df_feature.shape[0]):\n",
    "        \n",
    "        tokens = [token.lemma_ for token in df_feature[row]]\n",
    "        \n",
    "        tokens_redo = []\n",
    "    \n",
    "        for ii in range(len(tokens)):\n",
    "        \n",
    "            if (len(tokens[ii]) > 2) & ~(tokens[ii] == '-PRON-'):\n",
    "                tokens_redo.append(tokens[ii])\n",
    "        \n",
    "        df_feature[row] = \" \".join(tokens_redo)\n",
    "         \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['text_cleaned'] = clean_soup(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words.union(['review', 'pron', 'star', \n",
    "                                            'place','food', 'time', 'good',\n",
    "                                            'come', 'order', 'experience'\n",
    "                                            'service', 'know', 'restaurant',\n",
    "                                            'want', 'wait', 'till', 'think',\n",
    "                                            'tell', 'look', 'minute', 'year',\n",
    "                                            'find', 'thing', 'people', 'great',\n",
    "                                            'need', 'price', 'work', 'service',\n",
    "                                            'like', 'try' ,'love'\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j_m/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abundant</th>\n",
       "      <th>aburi</th>\n",
       "      <th>acai</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  ability  able  absolute  absolutely  abundant  aburi  acai  accent  \\\n",
       "0  0.0      0.0   0.0       0.0         0.0       0.0    0.0   0.0     0.0   \n",
       "1  0.0      0.0   0.0       0.0         0.0       0.0    0.0   0.0     0.0   \n",
       "2  0.0      0.0   0.0       0.0         0.0       0.0    0.0   0.0     0.0   \n",
       "3  0.0      0.0   0.0       0.0         0.0       0.0    0.0   0.0     0.0   \n",
       "4  0.0      0.0   0.0       0.0         0.0       0.0    0.0   0.0     0.0   \n",
       "\n",
       "   accept  ...  yup  zach  zen  zero  zest  zip  zombie  zone  zoo  zucchini  \n",
       "0     0.0  ...  0.0   0.0  0.0   0.0   0.0  0.0     0.0   0.0  0.0       0.0  \n",
       "1     0.0  ...  0.0   0.0  0.0   0.0   0.0  0.0     0.0   0.0  0.0       0.0  \n",
       "2     0.0  ...  0.0   0.0  0.0   0.0   0.0  0.0     0.0   0.0  0.0       0.0  \n",
       "3     0.0  ...  0.0   0.0  0.0   0.0   0.0  0.0     0.0   0.0  0.0       0.0  \n",
       "4     0.0  ...  0.0   0.0  0.0   0.0   0.0  0.0     0.0   0.0  0.0       0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorize cleaned text\n",
    "tfidf = TfidfVectorizer(stop_words = stop_words, max_features = 5000)\n",
    "\n",
    "dtm = tfidf.fit_transform(yelp['text_cleaned'])\n",
    "\n",
    "dtm = pd.DataFrame(dtm.todense(), columns = tfidf.get_feature_names())\n",
    "\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=10, p=2, radius=1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = [\"\"\"This was the greatest place to have lunch, \n",
    "                    the atmosphere was great, food was great, \n",
    "                    service was outstanding. Cheap prices, \n",
    "                    great lunch deals\"\"\"]\n",
    "\n",
    "fake = tfidf.transform(fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars:\n",
      " 6625    3\n",
      "6311    5\n",
      "6204    5\n",
      "469     4\n",
      "1192    5\n",
      "7594    5\n",
      "555     3\n",
      "2199    3\n",
      "7486    4\n",
      "2318    5\n",
      "Name: stars, dtype: int64\n",
      "\n",
      "                Review:\n",
      " 6625    Food and service are ok. There are much better...\n",
      "6311    天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用...\n",
      "6204    旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。\\n質問にも丁寧...\n",
      "469     O  o  thenk 6nnn  .b  cgv  xx TV cvg  9 nvehxc...\n",
      "1192    This place is so cute! It has a great atmosphe...\n",
      "7594    Great place to hang out SteveO is awesome and ...\n",
      "555     Chingu Korean BBQ's Lunch Specials for $5.99! ...\n",
      "2199    Came for lunch and didn't see much of a crowd ...\n",
      "7486    Their view is amazing. Go there for LUNCH TIME...\n",
      "2318    Great experience! The customer service is outs...\n",
      "Name: text, dtype: object\n",
      "\n",
      "                Review Cleaned:\n",
      " 6625     food and service there much well place for lunch\n",
      "6311                                                     \n",
      "6204                                                     \n",
      "469     thenk nnn cgv cvg nvehxcfvvv and the vghvhridd...\n",
      "1192    this place cute have great atmosphere the staf...\n",
      "7594    great place hang out steveo awesome and make f...\n",
      "555     chingu korean bbq lunch special for choice for...\n",
      "2199    come for lunch and didn see much crowd and asi...\n",
      "7486    view amazing there for lunch time because chea...\n",
      "2318    great experience the customer service outstand...\n",
      "Name: text_cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "nearest_vectors, nearest_loc = nn.kneighbors(fake.todense())\n",
    "\n",
    "for neighbor in list(nearest_loc):\n",
    "    print(f\"\"\"Stars:\\n {yelp['stars'].iloc[neighbor]}\\n\n",
    "                Review:\\n {yelp['text'].iloc[neighbor][0:300]}\\n\n",
    "                Review Cleaned:\\n {yelp['text_cleaned'].iloc[neighbor][0:300]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews found by nearest neighbors model are all rated with 3 and higher stars, \n",
    "and they mostly use similar language to which I used in the fake review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000,), (1000,), (9000,), (1000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(yelp['text_cleaned'], yelp['stars'],\n",
    "                                                  test_size=0.10, random_state=42, stratify = yelp['stars'])\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "svd = TruncatedSVD(algorithm='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed:  6.5min remaining:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  7.4min finished\n",
      "/home/j_m/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5633333333333334\n",
      "\n",
      "Best hyperparameters: \n",
      "{'clf__class_weight': None, 'clf__max_depth': 40, 'clf__max_features': 0.88, 'clf__min_samples_leaf': 5, 'clf__n_estimators': 250, 'lsi__svd__n_components': 30, 'lsi__vect__max_df': 0.95, 'lsi__vect__max_features': 5000, 'lsi__vect__min_df': 0.01, 'lsi__vect__ngram_range': (1, 1)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pipline using latent semantic indexing\n",
    "\n",
    "lsi = Pipeline([('vect', vect), \n",
    "                ('svd', svd)])\n",
    "\n",
    "pipeline = Pipeline([('lsi', lsi),\n",
    "                 ('clf', rfc)])\n",
    "\n",
    "# The pipeline puts together a bunch fit then transform,fit then predict.\n",
    "parameters = {\n",
    "    'lsi__vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'lsi__vect__max_df': [.95],\n",
    "    'lsi__vect__min_df': [.01],\n",
    "    'lsi__vect__max_features': [5000],\n",
    "    'lsi__svd__n_components': [30],\n",
    "    'clf__min_samples_leaf': [5, 24],\n",
    "    'clf__n_estimators': [250],\n",
    "    'clf__max_depth': [20, 40],\n",
    "    'clf__max_features': [0.88, 0.98],\n",
    "    'clf__class_weight': [None] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    parameters,  \n",
    "    cv = 3, \n",
    "    verbose = 10,\n",
    "    n_jobs = -1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}\\n')\n",
    "print(f'Best hyperparameters: \\n{grid_search.best_params_}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:\n",
      "0.559\n"
     ]
    }
   ],
   "source": [
    "best_pipeline = grid_search.best_estimator_\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f'Validation Accuracy:\\n{best_pipeline.score(X_val, y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.66      0.60       150\n",
      "           2       0.13      0.03      0.04        76\n",
      "           3       0.28      0.10      0.15       110\n",
      "           4       0.39      0.30      0.34       218\n",
      "           5       0.64      0.85      0.73       446\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      1000\n",
      "   macro avg       0.40      0.39      0.37      1000\n",
      "weighted avg       0.49      0.56      0.51      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_pred = best_pipeline.predict(X_val)\n",
    "print(classification_report(y_val, grid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model predicts my fake review would give\n",
      "the location a rating of 5 stars.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_review_pred = best_pipeline.predict(fake_review)\n",
    "\n",
    "print(f\"\"\"\n",
    "The model predicts my fake review would give\n",
    "the location a rating of {fake_review_pred[0]} stars.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model does fairly well with 1 or 5-star values, but has trouble predicting the others. \n",
    "It is highly uncertain of what a 2 and 3-star review is and that's due to a few factors, mainly,\n",
    "there is less data available to train, relative to the other ratings. Also, the \n",
    "wording between 1, 2, and 3-star reviews may not differ too much, causing confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import pyLDAvis.gensim\n",
    "import warnings #for LDA warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(df_feature, addl_stop_words = ['-PRON-']):\n",
    "    \"\"\"\n",
    "    Input: Column of a dataframe/ Pandas Series, \n",
    "    stop words you'd like to add to nlp's defaults\n",
    "    \n",
    "    Output: List consisting of tokens for each observation\n",
    "    \n",
    "    Assumes: nlp object initialized as nlp\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "    STOP_WORDS = nlp.Defaults.stop_words.union(addl_stop_words)\n",
    "\n",
    "    for doc in tokenizer.pipe(df_feature, batch_size=500):\n",
    "\n",
    "        doc_tokens = []\n",
    "\n",
    "        for token in doc: \n",
    "            if token.text not in STOP_WORDS:\n",
    "                doc_tokens.append(token.text.lower())\n",
    "\n",
    "        tokens.append(doc_tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [beware, fake, fake, fake, small, business, lo...\n",
       "1    [lunch, togo, quick, staff, friendly, complain...\n",
       "2    [vegas, dozen, step, foot, circus, circus, rea...\n",
       "3    [night, close, street, party, actually, group,...\n",
       "4    [bad, lunch, senior, pay, eat, hot, salad, noo...\n",
       "Name: text_tokens, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['text_tokens'] = make_tokens(yelp['text_cleaned'], stop_words)\n",
    "yelp['text_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(yelp['text_tokens'])\n",
    "id2word.filter_extremes(no_below=5, no_above=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in yelp['text_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=20,\n",
    "                   workers=6,\n",
    "                   num_topics = 3 # tuned after initial results of pyLDAvis\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "experience little drink\n",
      "\n",
      "------ Topic 1 ------\n",
      "don staff nice\n",
      "\n",
      "------ Topic 2 ------\n",
      "nice definitely don\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [re.findall(r'\"([^\"]*)\"', t[1]) for t in lda.print_topics()]\n",
    "\n",
    "topics = [' '.join(t[0:3]) for t in words]\n",
    "\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el11441405839715730965955232636\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el11441405839715730965955232636_data = {\"mdsDat\": {\"x\": [0.008951573239755883, -0.0005198115726953956, -0.008431761667060484], \"y\": [0.0041419866335772005, -0.009100353259597635, 0.004958366626020441], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [36.270843505859375, 32.40118408203125, 31.32796859741211]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [819.0, 1468.0, 1891.0, 1753.0, 1165.0, 539.0, 649.0, 1383.0, 663.0, 840.0, 777.0, 703.0, 621.0, 505.0, 793.0, 821.0, 697.0, 1397.0, 659.0, 619.0, 1120.0, 695.0, 894.0, 671.0, 1125.0, 1390.0, 1056.0, 1454.0, 948.0, 663.0, 31.796953201293945, 348.3130187988281, 6.7755208015441895, 12.692946434020996, 10.830484390258789, 3.554975986480713, 109.0042495727539, 4.694426536560059, 18.111482620239258, 27.333768844604492, 6.326673984527588, 8.559182167053223, 4.534191131591797, 15.846805572509766, 8.487785339355469, 5.083261489868164, 13.439435958862305, 5.037303447723389, 8.9030122756958, 4.4282121658325195, 6.08597469329834, 3.872177839279175, 9.90754508972168, 6.046975612640381, 43.40309524536133, 13.207039833068848, 8.24137020111084, 12.612797737121582, 352.21417236328125, 45.37697219848633, 449.5338134765625, 377.1341247558594, 16.44496726989746, 91.83206176757812, 163.76109313964844, 467.5044250488281, 122.57437133789062, 322.9618225097656, 81.42750549316406, 28.54015350341797, 537.8051147460938, 33.183067321777344, 250.374755859375, 71.09989929199219, 636.1765747070312, 850.7003173828125, 600.4102172851562, 687.9163208007812, 437.4354248046875, 217.17214965820312, 100.79859161376953, 372.16156005859375, 198.62451171875, 190.5794677734375, 524.2619018554688, 666.8408203125, 670.2584228515625, 418.305419921875, 644.0486450195312, 350.06207275390625, 575.9735107421875, 212.90716552734375, 368.5728454589844, 606.8319702148438, 355.7082824707031, 433.5342102050781, 441.1546936035156, 683.2914428710938, 590.1146240234375, 469.8961486816406, 355.2034912109375, 452.1020812988281, 382.6092834472656, 481.8084716796875, 455.5199279785156, 446.76885986328125, 395.03814697265625, 440.70928955078125, 417.950927734375, 398.9291076660156, 389.9084777832031, 14.705658912658691, 7.026177883148193, 6.2159295082092285, 28.27223014831543, 22.32139778137207, 5.538474082946777, 14.607588768005371, 7.694438934326172, 4.096344470977783, 5.828358173370361, 4.588005542755127, 9.219265937805176, 6.872363567352295, 466.63519287109375, 7.780462741851807, 8.924700736999512, 10.064380645751953, 12.703804969787598, 3.874450922012329, 4.9804511070251465, 4.964775562286377, 3.8709475994110107, 29.500917434692383, 4.398324489593506, 4.397462368011475, 21.87186622619629, 4.359820365905762, 5.446425437927246, 8.722105979919434, 3.744098663330078, 112.13579559326172, 67.64151763916016, 408.3653259277344, 15.469646453857422, 248.9400177001953, 78.38809204101562, 244.31515502929688, 65.16500091552734, 852.3812866210938, 311.1313171386719, 358.81695556640625, 158.4069061279297, 85.35332489013672, 504.1086120605469, 585.3799438476562, 292.10565185546875, 475.84918212890625, 402.73089599609375, 563.6223754882812, 462.5634765625, 123.38681030273438, 186.34837341308594, 480.4046936035156, 620.5250244140625, 463.9186706542969, 214.68324279785156, 377.12493896484375, 295.4063720703125, 432.0843811035156, 238.90806579589844, 229.92698669433594, 426.62066650390625, 603.5567626953125, 429.9945983886719, 265.54547119140625, 466.1517028808594, 370.4172058105469, 333.85003662109375, 399.50286865234375, 453.9914245605469, 317.0483093261719, 446.1202087402344, 452.1954650878906, 430.214111328125, 437.561279296875, 348.648193359375, 404.9261169433594, 388.6820373535156, 349.02215576171875, 362.97149658203125, 361.24053955078125, 6.570212364196777, 5.658249378204346, 4.882394790649414, 6.598513603210449, 3.565528631210327, 36.81465148925781, 35.41865921020508, 7.623685359954834, 4.101355075836182, 4.6518940925598145, 4.5977983474731445, 5.079870700836182, 6.789083480834961, 5.072909832000732, 9.551777839660645, 3.9208567142486572, 3.3286523818969727, 3.882171392440796, 3.8533663749694824, 9.72130012512207, 5.962283134460449, 5.389432907104492, 4.842141151428223, 16.07759666442871, 40.04251480102539, 42.54281234741211, 3.7390713691711426, 29.171382904052734, 6.847891807556152, 3.7150931358337402, 78.91654205322266, 109.10517120361328, 31.237384796142578, 71.25765991210938, 298.7756042480469, 333.6434631347656, 673.163818359375, 97.73528289794922, 325.6077575683594, 227.67593383789062, 305.7069091796875, 239.4240264892578, 145.4763641357422, 317.8171081542969, 263.6101379394531, 288.9888000488281, 194.80496215820312, 113.63032531738281, 283.37542724609375, 157.91165161132812, 579.2184448242188, 62.67449188232422, 344.8389587402344, 329.2174987792969, 567.435791015625, 169.94956970214844, 380.1346740722656, 285.1506652832031, 237.20770263671875, 558.0816650390625, 304.1293640136719, 499.5834655761719, 205.24569702148438, 424.5144348144531, 401.3141784667969, 386.0830383300781, 320.62835693359375, 444.7336730957031, 401.1713562011719, 497.0489196777344, 314.7646484375, 358.0519714355469, 368.1706848144531, 395.0423583984375, 446.7522277832031, 323.1309509277344, 373.7077331542969, 424.75994873046875, 346.95166015625, 361.23297119140625, 375.7610168457031, 383.032958984375, 349.2453918457031, 332.5566101074219, 355.3534851074219, 332.22247314453125, 332.24786376953125, 329.270263671875], \"Term\": [\"wasn\", \"experience\", \"don\", \"nice\", \"table\", \"cook\", \"favorite\", \"friendly\", \"offer\", \"server\", \"big\", \"super\", \"beer\", \"taco\", \"store\", \"long\", \"awesome\", \"little\", \"stop\", \"thank\", \"friend\", \"meat\", \"dish\", \"excellent\", \"room\", \"recommend\", \"pretty\", \"drink\", \"check\", \"stay\", \"naan\", \"cook\", \"brad\", \"alex\", \"vision\", \"amis\", \"massage\", \"todd\", \"brow\", \"shoot\", \"lion\", \"sisig\", \"toujours\", \"village\", \"ranchero\", \"bonne\", \"loaf\", \"dread\", \"voucher\", \"cockroach\", \"essayer\", \"dawn\", \"roach\", \"pen\", \"exceptional\", \"splash\", \"dans\", \"accessory\", \"thank\", \"smoothie\", \"store\", \"excellent\", \"veal\", \"wow\", \"variety\", \"dish\", \"product\", \"week\", \"mom\", \"facial\", \"pretty\", \"english\", \"money\", \"broth\", \"amazing\", \"nice\", \"delicious\", \"definitely\", \"visit\", \"far\", \"drop\", \"flavor\", \"wonderful\", \"stuff\", \"feel\", \"didn\", \"eat\", \"fresh\", \"day\", \"salad\", \"friendly\", \"change\", \"clean\", \"ask\", \"area\", \"right\", \"lot\", \"don\", \"staff\", \"bad\", \"bit\", \"customer\", \"sauce\", \"recommend\", \"little\", \"chicken\", \"bar\", \"drink\", \"menu\", \"taste\", \"leave\", \"backyard\", \"res\", \"mrs\", \"lash\", \"fade\", \"lebanese\", \"cirque\", \"poisoning\", \"pristine\", \"singer\", \"awsome\", \"science\", \"monitor\", \"wasn\", \"catfish\", \"attendant\", \"uptown\", \"caring\", \"mental\", \"hatch\", \"upside\", \"lax\", \"consistent\", \"scope\", \"sole\", \"yard\", \"shockingly\", \"instagram\", \"barbershop\", \"suv\", \"cute\", \"establishment\", \"server\", \"shine\", \"kid\", \"face\", \"bread\", \"chain\", \"don\", \"stay\", \"big\", \"style\", \"patient\", \"table\", \"recommend\", \"offer\", \"room\", \"location\", \"friendly\", \"use\", \"decor\", \"wine\", \"new\", \"staff\", \"fry\", \"atmosphere\", \"small\", \"burger\", \"friend\", \"option\", \"selection\", \"taste\", \"nice\", \"way\", \"quality\", \"menu\", \"night\", \"vegas\", \"feel\", \"chicken\", \"pay\", \"drink\", \"day\", \"experience\", \"didn\", \"bar\", \"eat\", \"ask\", \"hour\", \"little\", \"definitely\", \"russian\", \"sonoran\", \"jackson\", \"halo\", \"deeply\", \"boba\", \"cat\", \"mover\", \"dependable\", \"yearly\", \"standing\", \"bmw\", \"baker\", \"imperial\", \"oasis\", \"einstein\", \"sanitation\", \"pleasurable\", \"brittany\", \"german\", \"supper\", \"savoury\", \"bliss\", \"vietnamese\", \"honey\", \"bagel\", \"unappetizing\", \"pub\", \"bbb\", \"overcrowded\", \"honest\", \"evening\", \"rip\", \"sample\", \"beer\", \"super\", \"experience\", \"box\", \"awesome\", \"pick\", \"stop\", \"buy\", \"bite\", \"meat\", \"let\", \"favorite\", \"point\", \"saturday\", \"help\", \"fix\", \"little\", \"rate\", \"long\", \"lunch\", \"drink\", \"dog\", \"check\", \"car\", \"family\", \"ask\", \"happy\", \"chicken\", \"huge\", \"customer\", \"friend\", \"hour\", \"pizza\", \"menu\", \"table\", \"eat\", \"cheese\", \"night\", \"lot\", \"bad\", \"day\", \"walk\", \"way\", \"didn\", \"leave\", \"new\", \"definitely\", \"staff\", \"delicious\", \"use\", \"don\", \"room\", \"fry\", \"taste\"], \"Total\": [819.0, 1468.0, 1891.0, 1753.0, 1165.0, 539.0, 649.0, 1383.0, 663.0, 840.0, 777.0, 703.0, 621.0, 505.0, 793.0, 821.0, 697.0, 1397.0, 659.0, 619.0, 1120.0, 695.0, 894.0, 671.0, 1125.0, 1390.0, 1056.0, 1454.0, 948.0, 663.0, 48.740692138671875, 539.6835327148438, 10.529579162597656, 20.106740951538086, 17.258453369140625, 5.750415802001953, 176.56924438476562, 7.683853626251221, 29.775344848632812, 45.086997985839844, 10.561702728271484, 14.422075271606445, 7.6980299949646, 26.92510414123535, 14.447646141052246, 8.657337188720703, 23.07893180847168, 8.663259506225586, 15.411914825439453, 7.708137512207031, 10.599607467651367, 6.75002908706665, 17.309783935546875, 10.58799934387207, 76.02957153320312, 23.13846206665039, 14.440914154052734, 22.182815551757812, 619.4931640625, 79.91438293457031, 793.848388671875, 671.0394897460938, 28.9678955078125, 164.8964385986328, 304.0279541015625, 894.9873046875, 226.30007934570312, 620.0350341796875, 150.5437774658203, 51.14546585083008, 1056.215087890625, 59.813148498535156, 480.6512451171875, 131.51402282714844, 1294.6427001953125, 1753.8016357421875, 1224.4283447265625, 1424.9178466796875, 887.134033203125, 425.6962890625, 190.27767944335938, 759.2261352539062, 396.1257629394531, 382.3673095703125, 1155.5826416015625, 1529.162109375, 1572.2335205078125, 949.7451171875, 1542.996337890625, 785.567626953125, 1383.2135009765625, 451.0513610839844, 861.262939453125, 1553.595703125, 826.4910888671875, 1051.5919189453125, 1080.321533203125, 1891.0262451171875, 1593.672607421875, 1206.5626220703125, 845.92724609375, 1194.9102783203125, 945.88720703125, 1390.1474609375, 1397.7099609375, 1400.34375, 1029.8514404296875, 1454.2652587890625, 1328.8363037109375, 1154.820068359375, 1031.011962890625, 22.804367065429688, 10.922555923461914, 9.940699577331543, 45.399417877197266, 35.92274856567383, 8.97599983215332, 23.95867919921875, 12.946371078491211, 6.935969352722168, 9.932465553283691, 7.935034275054932, 15.98333740234375, 12.000714302062988, 819.2783813476562, 13.743246078491211, 15.936359405517578, 17.978248596191406, 22.758033752441406, 6.944897174835205, 8.937280654907227, 8.917454719543457, 6.965677738189697, 53.37771224975586, 7.958333969116211, 7.964599132537842, 39.70942306518555, 7.953216552734375, 9.937631607055664, 15.923126220703125, 6.909832000732422, 207.08267211914062, 130.8470001220703, 840.4793701171875, 29.00228500366211, 509.8189697265625, 156.45648193359375, 511.3908996582031, 129.59251403808594, 1891.0262451171875, 663.8417358398438, 777.7201538085938, 329.96905517578125, 172.4713897705078, 1165.675048828125, 1390.1474609375, 663.865234375, 1125.175537109375, 953.6180419921875, 1383.2135009765625, 1117.6185302734375, 265.80059814453125, 420.5163269042969, 1193.401611328125, 1593.672607421875, 1167.55712890625, 493.6175231933594, 943.8985595703125, 720.6995849609375, 1120.781982421875, 567.91015625, 552.1620483398438, 1154.820068359375, 1753.8016357421875, 1184.950439453125, 667.96533203125, 1328.8363037109375, 1033.1920166015625, 901.651611328125, 1155.5826416015625, 1400.34375, 869.0906372070312, 1454.2652587890625, 1542.996337890625, 1468.297607421875, 1529.162109375, 1029.8514404296875, 1572.2335205078125, 1553.595703125, 1069.5555419921875, 1397.7099609375, 1424.9178466796875, 10.118456840515137, 9.055573463439941, 8.081214904785156, 11.036673545837402, 6.008452415466309, 62.400848388671875, 60.442928314208984, 13.022595405578613, 7.006980895996094, 8.05683708190918, 8.092127799987793, 9.027546882629395, 12.097418785095215, 9.04875373840332, 17.101016998291016, 7.056033134460449, 6.008584499359131, 7.027066230773926, 7.044999122619629, 17.991100311279297, 11.03495979309082, 9.979201316833496, 8.989233016967773, 30.005704879760742, 74.75630187988281, 79.74645233154297, 7.021445274353027, 54.964786529541016, 12.952068328857422, 7.028687477111816, 149.32965087890625, 212.39224243164062, 60.64802169799805, 141.88882446289062, 621.947021484375, 703.6071166992188, 1468.297607421875, 198.16238403320312, 697.677734375, 484.82623291015625, 659.531982421875, 512.0210571289062, 304.68701171875, 695.4947509765625, 578.4310302734375, 649.0863647460938, 427.8174133300781, 240.7532958984375, 644.76904296875, 344.91217041015625, 1397.7099609375, 128.30966186523438, 821.2950439453125, 792.41796875, 1454.2652587890625, 385.45733642578125, 948.5714111328125, 693.4892578125, 570.8272094726562, 1553.595703125, 768.0228271484375, 1400.34375, 487.0419006347656, 1194.9102783203125, 1120.781982421875, 1069.5555419921875, 868.3897094726562, 1328.8363037109375, 1165.675048828125, 1572.2335205078125, 857.11572265625, 1033.1920166015625, 1080.321533203125, 1206.5626220703125, 1542.996337890625, 895.1175537109375, 1184.950439453125, 1529.162109375, 1031.011962890625, 1193.401611328125, 1424.9178466796875, 1593.672607421875, 1224.4283447265625, 1117.6185302734375, 1891.0262451171875, 1125.175537109375, 1167.55712890625, 1154.820068359375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5870000123977661, 0.5763000249862671, 0.5733000040054321, 0.554099977016449, 0.5482000112533569, 0.5332000255584717, 0.5317999720573425, 0.521399974822998, 0.5170000195503235, 0.513700008392334, 0.5016999840736389, 0.49239999055862427, 0.4848000109195709, 0.48410001397132874, 0.4823000133037567, 0.48170000314712524, 0.4733999967575073, 0.47189998626708984, 0.46540001034736633, 0.45989999175071716, 0.4593000113964081, 0.45840001106262207, 0.4562000036239624, 0.45399999618530273, 0.4535999894142151, 0.45339998602867126, 0.45329999923706055, 0.4496000111103058, 0.4494999945163727, 0.448199987411499, 0.445499986410141, 0.43790000677108765, 0.4480000138282776, 0.42879998683929443, 0.3953999876976013, 0.36480000615119934, 0.4009999930858612, 0.3619000017642975, 0.39959999918937683, 0.4307999908924103, 0.3391999900341034, 0.42500001192092896, 0.3619999885559082, 0.39910000562667847, 0.303600013256073, 0.2906999886035919, 0.30149999260902405, 0.28600001335144043, 0.30709999799728394, 0.3411000072956085, 0.37880000472068787, 0.3012000024318695, 0.3237999975681305, 0.31779998540878296, 0.22380000352859497, 0.1842000037431717, 0.1615999937057495, 0.19419999420642853, 0.1404000073671341, 0.20589999854564667, 0.13809999823570251, 0.26339998841285706, 0.16539999842643738, 0.07410000264644623, 0.17110000550746918, 0.12809999287128448, 0.1185000017285347, -0.003800000064074993, 0.02070000022649765, 0.07109999656677246, 0.14640000462532043, 0.0421999990940094, 0.10899999737739563, -0.045499999076128006, -0.10700000077486038, -0.1282999962568283, 0.0560000017285347, -0.17970000207424164, -0.14249999821186066, -0.04879999905824661, 0.04179999977350235, 0.6883000135421753, 0.6858000159263611, 0.6575000286102295, 0.6534000039100647, 0.651199996471405, 0.64410001039505, 0.6322000026702881, 0.6067000031471252, 0.6003000140190125, 0.5939000248908997, 0.5791000127792358, 0.57669997215271, 0.5695000290870667, 0.5641000270843506, 0.5580000281333923, 0.5472000241279602, 0.5468000173568726, 0.5440000295639038, 0.54339998960495, 0.5422999858856201, 0.5412999987602234, 0.5394999980926514, 0.5339999794960022, 0.5339999794960022, 0.5329999923706055, 0.5306000113487244, 0.5257999897003174, 0.525600016117096, 0.5250999927520752, 0.51419997215271, 0.5135999917984009, 0.46720001101493835, 0.4052000045776367, 0.4984999895095825, 0.4101000130176544, 0.4359000027179718, 0.38830000162124634, 0.43950000405311584, 0.33009999990463257, 0.36910000443458557, 0.35339999198913574, 0.39309999346733093, 0.4235000014305115, 0.28870001435279846, 0.2621000111103058, 0.3059999942779541, 0.266400009393692, 0.26499998569488525, 0.22920000553131104, 0.24480000138282776, 0.3596000075340271, 0.31310001015663147, 0.21699999272823334, 0.18369999527931213, 0.20399999618530273, 0.29440000653266907, 0.2094999998807907, 0.23510000109672546, 0.1738000065088272, 0.26109999418258667, 0.250900000333786, 0.13120000064373016, 0.06030000001192093, 0.11330000311136246, 0.2045000046491623, 0.07940000295639038, 0.10119999945163727, 0.13339999318122864, 0.06480000168085098, 0.0006000000284984708, 0.11860000342130661, -0.05469999834895134, -0.10040000081062317, -0.1005999967455864, -0.12430000305175781, 0.04390000179409981, -0.2295999974012375, -0.25859999656677246, 0.0071000000461936, -0.22130000591278076, -0.24539999663829803, 0.7287999987602234, 0.6904000043869019, 0.6567999720573425, 0.6463000178337097, 0.6388000249862671, 0.6330000162124634, 0.6262000203132629, 0.6251999735832214, 0.6251000165939331, 0.6114000082015991, 0.595300018787384, 0.5856999754905701, 0.5830000042915344, 0.5819000005722046, 0.5781999826431274, 0.5730999708175659, 0.5699999928474426, 0.567300021648407, 0.5572999715805054, 0.5450999736785889, 0.5450000166893005, 0.5446000099182129, 0.5419999957084656, 0.5367000102996826, 0.5364000201225281, 0.5322999954223633, 0.5304999947547913, 0.5271999835968018, 0.5232999920845032, 0.5231000185012817, 0.5228999853134155, 0.4945000112056732, 0.49720001220703125, 0.47189998626708984, 0.42750000953674316, 0.41449999809265137, 0.3808000087738037, 0.4537999927997589, 0.3986000120639801, 0.4047999978065491, 0.3917999863624573, 0.40049999952316284, 0.4214000105857849, 0.3774999976158142, 0.3747999966144562, 0.351500004529953, 0.37400001287460327, 0.4097999930381775, 0.3384999930858612, 0.37940001487731934, 0.27970001101493835, 0.4442000091075897, 0.2928999960422516, 0.2822999954223633, 0.21950000524520874, 0.3416999876499176, 0.24619999527931213, 0.2718999981880188, 0.2824999988079071, 0.13680000603199005, 0.23430000245571136, 0.12999999523162842, 0.29649999737739563, 0.1257999986410141, 0.13359999656677246, 0.14169999957084656, 0.16429999470710754, 0.06610000133514404, 0.09399999678134918, 0.009100000374019146, 0.15889999270439148, 0.10090000182390213, 0.08420000225305557, 0.04410000145435333, -0.07880000025033951, 0.14180000126361847, 0.0066999997943639755, -0.12030000239610672, 0.07150000333786011, -0.03440000116825104, -0.17229999601840973, -0.26499998569488525, -0.09380000084638596, -0.051500000059604645, -0.5110999941825867, -0.05920000001788139, -0.09610000252723694, -0.094200000166893], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.336899757385254, -5.94320011138916, -9.88290023803711, -9.255200386047363, -9.413900375366211, -10.527899742126465, -7.104899883270264, -10.249899864196777, -8.899700164794922, -8.488100051879883, -9.951499938964844, -9.649299621582031, -10.284600257873535, -9.033300399780273, -9.657600402832031, -10.170299530029297, -9.198100090026855, -10.179400444030762, -9.60990047454834, -10.308300018310547, -9.990300178527832, -10.442399978637695, -9.503000259399414, -9.996700286865234, -8.025699615478516, -9.215499877929688, -9.687100410461426, -9.26159954071045, -5.932000160217285, -7.981299877166748, -5.6880998611450195, -5.863699913024902, -8.996199607849121, -7.276299953460693, -6.69789981842041, -5.648900032043457, -6.987500190734863, -6.018700122833252, -7.396500110626221, -8.444999694824219, -5.508800029754639, -8.29419994354248, -6.2733001708984375, -7.532199859619141, -5.340799808502197, -5.05019998550415, -5.398600101470947, -5.262599945068359, -5.7153000831604, -6.415599822998047, -7.18310022354126, -5.8769001960754395, -6.504799842834473, -6.546199798583984, -5.534299850463867, -5.293700218200684, -5.288599967956543, -5.760000228881836, -5.328499794006348, -5.938199996948242, -5.440199851989746, -6.435400009155273, -5.886600017547607, -5.388000011444092, -5.9222002029418945, -5.724299907684326, -5.706900119781494, -5.2692999839782715, -5.415900230407715, -5.643799781799316, -5.923600196838379, -5.682400226593018, -5.8491997718811035, -5.61870002746582, -5.674799919128418, -5.694200038909912, -5.817299842834473, -5.707900047302246, -5.760900020599365, -5.807499885559082, -5.830399990081787, -8.995200157165527, -9.733799934387207, -9.856300354003906, -8.34160041809082, -8.577899932861328, -9.971699714660645, -9.001899719238281, -9.642900466918945, -10.273300170898438, -9.920700073242188, -10.15999984741211, -9.4621000289917, -9.755900382995605, -5.537899971008301, -9.631799697875977, -9.494600296020508, -9.37440013885498, -9.141500473022461, -10.329000473022461, -10.077899932861328, -10.081100463867188, -10.329899787902832, -8.298999786376953, -10.202199935913086, -10.202400207519531, -8.598199844360352, -10.211000442504883, -9.988499641418457, -9.517600059509277, -10.363300323486328, -6.963699817657471, -7.469200134277344, -5.671299934387207, -8.944600105285645, -6.166200160980225, -7.321800231933594, -6.184999942779541, -7.506499767303467, -4.935400009155273, -5.94320011138916, -5.800600051879883, -6.618299961090088, -7.236599922180176, -5.460700035095215, -5.311200141906738, -6.00629997253418, -5.5183000564575195, -5.685200214385986, -5.349100112915039, -5.5467000007629395, -6.868100166320801, -6.4558000564575195, -5.508800029754639, -5.252900123596191, -5.543700218200684, -6.314300060272217, -5.750899791717529, -5.995100021362305, -5.614799976348877, -6.207399845123291, -6.245699882507324, -5.627500057220459, -5.280600070953369, -5.619699954986572, -6.1016998291015625, -5.538899898529053, -5.768799781799316, -5.872799873352051, -5.69320011138916, -5.565400123596191, -5.9243998527526855, -5.582900047302246, -5.569300174713135, -5.619200229644775, -5.602200031280518, -5.829400062561035, -5.679699897766113, -5.720699787139893, -5.8282999992370605, -5.789100170135498, -5.793900012969971, -9.767200469970703, -9.916600227355957, -10.06410026550293, -9.762900352478027, -10.378399848937988, -8.043899536132812, -8.082500457763672, -9.618499755859375, -10.23840045928955, -10.112500190734863, -10.124199867248535, -10.024499893188477, -9.734399795532227, -10.025799751281738, -9.392999649047852, -10.28339958190918, -10.447199821472168, -10.293399810791016, -10.300800323486328, -9.375399589538574, -9.864299774169922, -9.965299606323242, -10.072400093078613, -8.872300148010254, -7.959799766540527, -7.899199962615967, -10.330900192260742, -8.276599884033203, -9.725799560546875, -10.337400436401367, -7.281400203704834, -6.957399845123291, -8.208100318908691, -7.383500099182129, -5.950099945068359, -5.839700222015381, -5.137800216674805, -7.067500114440918, -5.864099979400635, -6.221799850463867, -5.92710018157959, -6.171500205993652, -6.6697001457214355, -5.888299942016602, -6.075300216674805, -5.983399868011475, -6.377799987792969, -6.916800022125244, -6.002999782562256, -6.587699890136719, -5.288099765777588, -7.5117998123168945, -5.806700229644775, -5.853000164031982, -5.308599948883057, -6.5142998695373535, -5.709199905395508, -5.996699810028076, -6.180799961090088, -5.325300216674805, -5.932300090789795, -5.435999870300293, -6.3256001472473145, -5.598800182342529, -5.65500020980835, -5.693699836730957, -5.879499912261963, -5.552299976348877, -5.655399799346924, -5.441100120544434, -5.897900104522705, -5.769100189208984, -5.741199970245361, -5.67080020904541, -5.547800064086914, -5.871699810028076, -5.72629976272583, -5.598199844360352, -5.800600051879883, -5.760200023651123, -5.720799922943115, -5.701600074768066, -5.794000148773193, -5.842899799346924, -5.776599884033203, -5.843999862670898, -5.843900203704834, -5.85290002822876]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.5860392451286316, 0.18031975626945496, 0.2253997027873993, 0.6465493440628052, 0.198938250541687, 0.14920368790626526, 0.4912552237510681, 0.2703448534011841, 0.23790347576141357, 0.6956018805503845, 0.17390047013759613, 0.17390047013759613, 0.4307366609573364, 0.273445188999176, 0.2964339256286621, 0.39070653915405273, 0.25038689374923706, 0.35916680097579956, 0.2633618116378784, 0.4355598986148834, 0.30185315012931824, 0.18824876844882965, 0.5647463202476501, 0.2509983479976654, 0.32106512784957886, 0.2121323198080063, 0.46726444363594055, 0.2520467936992645, 0.6301169991493225, 0.12602339684963226, 0.21925625205039978, 0.657768726348877, 0.1315537542104721, 0.3895363509654999, 0.28344985842704773, 0.3273763060569763, 0.1630166620016098, 0.3134935796260834, 0.5392089486122131, 0.16532452404499054, 0.2479867786169052, 0.5786358118057251, 0.3835504651069641, 0.33888381719589233, 0.2777099609375, 0.2512069642543793, 0.5652156472206116, 0.2512069642543793, 0.30883097648620605, 0.15441548824310303, 0.5404542088508606, 0.24278594553470612, 0.27655088901519775, 0.48074835538864136, 0.22501666843891144, 0.46160563826560974, 0.31373754143714905, 0.419657826423645, 0.32390493154525757, 0.2565232515335083, 0.2166157364845276, 0.3052312731742859, 0.47589820623397827, 0.2224883884191513, 0.2224883884191513, 0.5562210083007812, 0.22154413163661957, 0.22154413163661957, 0.5538603067398071, 0.20833049714565277, 0.20833049714565277, 0.592940628528595, 0.5775448083877563, 0.23101791739463806, 0.23101791739463806, 0.2371792197227478, 0.26745742559432983, 0.4945439100265503, 0.6647939085960388, 0.18994110822677612, 0.18994110822677612, 0.3363376259803772, 0.47713011503219604, 0.18576787412166595, 0.14194466173648834, 0.2838893234729767, 0.5677786469459534, 0.5398663878440857, 0.22050879895687103, 0.2433200627565384, 0.6045269966125488, 0.167924165725708, 0.2015089988708496, 0.3080340325832367, 0.4093244969844818, 0.2816707491874695, 0.2812384366989136, 0.2519427537918091, 0.4667776823043823, 0.2768608033657074, 0.3114684224128723, 0.4109652638435364, 0.26364317536354065, 0.5712268352508545, 0.1757621020078659, 0.19853438436985016, 0.2150789201259613, 0.5790586471557617, 0.3638150691986084, 0.5821040868759155, 0.07276301085948944, 0.277793824672699, 0.5015721917152405, 0.21606186032295227, 0.47223004698753357, 0.3214711546897888, 0.20618493854999542, 0.2688252925872803, 0.3299698829650879, 0.40060243010520935, 0.2963427007198334, 0.3371773362159729, 0.3675116300582886, 0.3192073404788971, 0.3242061138153076, 0.3570551872253418, 0.16695411503314972, 0.6260778903961182, 0.20869264006614685, 0.4284406006336212, 0.34484243392944336, 0.22757278382778168, 0.5189321041107178, 0.2594660520553589, 0.2594660520553589, 0.28101617097854614, 0.5620323419570923, 0.16860969364643097, 0.644822359085083, 0.16120558977127075, 0.19270552694797516, 0.37827107310295105, 0.2661287784576416, 0.35567525029182434, 0.17867259681224823, 0.5408468246459961, 0.2800813913345337, 0.5539815425872803, 0.27699077129364014, 0.2077430784702301, 0.5925900340080261, 0.14814750850200653, 0.29629501700401306, 0.4173697531223297, 0.2929365336894989, 0.2896960973739624, 0.300977498292923, 0.46275290846824646, 0.23701977729797363, 0.3328644037246704, 0.1664322018623352, 0.6657288074493408, 0.4828348457813263, 0.2533479332923889, 0.2638748586177826, 0.490024596452713, 0.2245946079492569, 0.28503096103668213, 0.28542962670326233, 0.14271481335163116, 0.5708592534065247, 0.4361865818500519, 0.28643137216567993, 0.27792999148368835, 0.5229124426841736, 0.19665083289146423, 0.2815682291984558, 0.3035355508327484, 0.25683775544166565, 0.44103455543518066, 0.36117956042289734, 0.45054900646209717, 0.18772874772548676, 0.5771499872207642, 0.23085999488830566, 0.23085999488830566, 0.3032459020614624, 0.30668407678604126, 0.38988760113716125, 0.5308032035827637, 0.3100731670856476, 0.15766431391239166, 0.4261453449726105, 0.25759533047676086, 0.3161107897758484, 0.1417226940393448, 0.2834453880786896, 0.5668907761573792, 0.5517181754112244, 0.25078096985816956, 0.20062477886676788, 0.5660586953163147, 0.18868623673915863, 0.18868623673915863, 0.25984546542167664, 0.5196909308433533, 0.22163289785385132, 0.2824961841106415, 0.20245561003684998, 0.5132014155387878, 0.5618149042129517, 0.2384360432624817, 0.19969017803668976, 0.5655694007873535, 0.2630555331707001, 0.15783332288265228, 0.2485872060060501, 0.2928561568260193, 0.4583539366722107, 0.21731282770633698, 0.4985411763191223, 0.28122836351394653, 0.5670101642608643, 0.21507282555103302, 0.21507282555103302, 0.19486258924007416, 0.6124253273010254, 0.19486258924007416, 0.30306893587112427, 0.2820468246936798, 0.41518694162368774, 0.5097531080245972, 0.2466547191143036, 0.24195653200149536, 0.18025335669517517, 0.37437236309051514, 0.44524121284484863, 0.45345091819763184, 0.34614571928977966, 0.2007645219564438, 0.23774167895317078, 0.30442532896995544, 0.45808762311935425, 0.4899725914001465, 0.24103491008281708, 0.2686946392059326, 0.44011807441711426, 0.3053450882434845, 0.25480520725250244, 0.25607120990753174, 0.38544517755508423, 0.3577859103679657, 0.4164216220378876, 0.40774616599082947, 0.176400825381279, 0.3177574574947357, 0.39741095900535583, 0.28435438871383667, 0.27791517972946167, 0.16674910485744476, 0.5558303594589233, 0.2718210220336914, 0.18121401965618134, 0.6342490911483765, 0.32941728830337524, 0.27473142743110657, 0.39582157135009766, 0.22378171980381012, 0.5594542622566223, 0.22378171980381012, 0.26831313967704773, 0.29312822222709656, 0.4389168620109558, 0.2343807816505432, 0.24107737839221954, 0.5290309190750122, 0.29428955912590027, 0.173898383975029, 0.5350719690322876, 0.31227925419807434, 0.32630375027656555, 0.36089757084846497, 0.31208813190460205, 0.2648642659187317, 0.4209083318710327, 0.22102491557598114, 0.22102491557598114, 0.5525622963905334, 0.201255202293396, 0.50313800573349, 0.201255202293396, 0.12374377250671387, 0.24748754501342773, 0.6187188625335693, 0.2785302400588989, 0.4884086549282074, 0.23341618478298187, 0.26432058215141296, 0.6167479753494263, 0.1101335734128952, 0.28712210059165955, 0.5742442011833191, 0.28712210059165955, 0.37826913595199585, 0.2851567268371582, 0.33656254410743713, 0.22281640768051147, 0.6684492230415344, 0.22281640768051147, 0.28352558612823486, 0.2610509991645813, 0.4564070403575897, 0.5680902004241943, 0.18936340510845184, 0.18936340510845184, 0.3262479305267334, 0.25971052050590515, 0.4142490327358246, 0.5632842779159546, 0.2599773705005646, 0.17331825196743011, 0.326126366853714, 0.4226010739803314, 0.2516731023788452, 0.23377713561058044, 0.3470129370689392, 0.42006829380989075, 0.4082117974758148, 0.25085124373435974, 0.3406393229961395, 0.31296613812446594, 0.27132144570350647, 0.41518494486808777, 0.6173215508460999, 0.17556852102279663, 0.20954951643943787, 0.29044073820114136, 0.2530572712421417, 0.45722848176956177, 0.28798121213912964, 0.5759624242782593, 0.14399060606956482, 0.3145609498023987, 0.3506827652454376, 0.33487945795059204, 0.5380494594573975, 0.29227378964424133, 0.16606464982032776, 0.5201276540756226, 0.3016740381717682, 0.17684340476989746, 0.1666567474603653, 0.5832986235618591, 0.24998511373996735, 0.23036882281303406, 0.1535792201757431, 0.6143168807029724, 0.20119307935237885, 0.6035792231559753, 0.20119307935237885, 0.6565356254577637, 0.18465064465999603, 0.16413390636444092, 0.29495519399642944, 0.40221163630485535, 0.30249667167663574, 0.4852316081523895, 0.344394713640213, 0.17105697095394135, 0.2952016592025757, 0.35811349749565125, 0.3464989960193634, 0.17542816698551178, 0.23390421271324158, 0.5847605466842651, 0.1943165510892868, 0.43984830379486084, 0.36603814363479614, 0.2993431091308594, 0.4208412170410156, 0.2799738645553589, 0.14227406680583954, 0.2845481336116791, 0.5690962672233582, 0.26671090722084045, 0.4928353726863861, 0.23772059381008148, 0.28880763053894043, 0.36474907398223877, 0.34633901715278625, 0.5666792988777161, 0.28333964943885803, 0.18889309465885162, 0.28876325488090515, 0.2433861643075943, 0.47027158737182617, 0.3546794652938843, 0.27637362480163574, 0.3696497082710266, 0.28461378812789917, 0.28461378812789917, 0.5692275762557983, 0.34126707911491394, 0.2033577859401703, 0.45580193400382996, 0.23172517120838165, 0.6179337501525879, 0.23172517120838165, 0.509365975856781, 0.21018445491790771, 0.28119271993637085, 0.2883519232273102, 0.5767038464546204, 0.1441759616136551, 0.5435261130332947, 0.21210774779319763, 0.2474590390920639, 0.2729019820690155, 0.20012813806533813, 0.5276105403900146, 0.32786133885383606, 0.39822426438331604, 0.27546340227127075, 0.5537233948707581, 0.13843084871768951, 0.27686169743537903, 0.303952157497406, 0.21042843163013458, 0.49099966883659363, 0.34672582149505615, 0.42081865668296814, 0.2323494553565979, 0.18310733139514923, 0.6408756375312805, 0.18310733139514923, 0.4127076268196106, 0.3128589987754822, 0.2748214304447174, 0.3132830858230591, 0.16488583385944366, 0.5111461281776428, 0.5777079463005066, 0.2888539731502533, 0.11554159224033356, 0.28173381090164185, 0.4230450987815857, 0.2950650751590729, 0.19765859842300415, 0.19765859842300415, 0.6918051242828369, 0.4455377161502838, 0.2456822246313095, 0.3080574870109558, 0.26076754927635193, 0.2396242320537567, 0.5003917813301086, 0.1664285510778427, 0.1664285510778427, 0.49928563833236694, 0.24921777844429016, 0.2782931923866272, 0.47351375222206116, 0.4049108624458313, 0.2674737572669983, 0.32773464918136597, 0.3006252646446228, 0.20041684806346893, 0.5010421276092529, 0.1876954734325409, 0.5630863904953003, 0.2502606213092804, 0.2513088881969452, 0.5026177763938904, 0.2513088881969452, 0.3133138120174408, 0.4165443778038025, 0.2698483169078827, 0.25937578082084656, 0.4854372441768646, 0.2546165883541107, 0.17240020632743835, 0.5172006487846375, 0.2758403420448303, 0.2514705955982208, 0.5029411911964417, 0.2514705955982208, 0.5988422632217407, 0.2439727783203125, 0.15525540709495544, 0.20135986804962158, 0.6040796041488647, 0.20135986804962158, 0.624043345451355, 0.20801444351673126, 0.20801444351673126, 0.2775722146034241, 0.3994073271751404, 0.32312795519828796, 0.563102662563324, 0.2627812325954437, 0.16267409920692444, 0.2511111795902252, 0.5022223591804504, 0.2511111795902252, 0.22085845470428467, 0.11042922735214233, 0.662575364112854, 0.5618351101875305, 0.21609042584896088, 0.21609042584896088, 0.37021404504776, 0.38966599106788635, 0.24032539129257202, 0.12357689440250397, 0.24715378880500793, 0.6178844571113586, 0.27566811442375183, 0.46848514676094055, 0.2560851275920868, 0.327504962682724, 0.20923928916454315, 0.46396535634994507, 0.5668588876724243, 0.24815821647644043, 0.185173898935318, 0.4995196759700775, 0.1752241849899292, 0.3269107937812805, 0.2727528512477875, 0.47883278131484985, 0.24850815534591675, 0.30841076374053955, 0.21745090186595917, 0.47469672560691833, 0.1812421679496765, 0.27186325192451477, 0.5437265038490295, 0.28944262862205505, 0.5788852572441101, 0.14472131431102753, 0.22304672002792358, 0.4323675036430359, 0.3440066874027252, 0.17819270491600037, 0.4118231534957886, 0.4118231534957886, 0.3455083668231964, 0.3697545826435089, 0.28489285707473755, 0.5682064294815063, 0.17756451666355133, 0.2534329891204834, 0.650715172290802, 0.13014303147792816, 0.2602860629558563, 0.6495168209075928, 0.12990336120128632, 0.25980672240257263, 0.2848416566848755, 0.2848416566848755, 0.569683313369751, 0.2242792397737503, 0.5606980919837952, 0.2242792397737503, 0.16686831414699554, 0.5562276840209961, 0.27811384201049805, 0.288112610578537, 0.414273738861084, 0.2979549765586853, 0.5394240617752075, 0.29931458830833435, 0.1611693948507309, 0.5523356199264526, 0.17260487377643585, 0.2761678099632263, 0.34492257237434387, 0.3704313337802887, 0.2850324809551239, 0.26661595702171326, 0.19996197521686554, 0.5332319140434265, 0.5942409634590149, 0.22284036874771118, 0.18570029735565186, 0.6373687982559204, 0.1738278567790985, 0.23177048563957214, 0.4925974905490875, 0.20515501499176025, 0.3020963966846466, 0.5839637517929077, 0.19465459883213043, 0.19465459883213043, 0.33850303292274475, 0.300519198179245, 0.36084645986557007, 0.24411727488040924, 0.5700138211250305, 0.18674971163272858, 0.3215324282646179, 0.36288437247276306, 0.31562501192092896, 0.5209382772445679, 0.2935318052768707, 0.18547339737415314, 0.31389981508255005, 0.44231337308883667, 0.24255895614624023, 0.5023657083511353, 0.23982281982898712, 0.2600184381008148, 0.5579259395599365, 0.2547053098678589, 0.1879967898130417, 0.2518293857574463, 0.5540246963500977, 0.201463520526886, 0.24823637306690216, 0.24823637306690216, 0.6205909252166748], \"Term\": [\"accessory\", \"accessory\", \"accessory\", \"alex\", \"alex\", \"alex\", \"amazing\", \"amazing\", \"amazing\", \"amis\", \"amis\", \"amis\", \"area\", \"area\", \"area\", \"ask\", \"ask\", \"ask\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"attendant\", \"attendant\", \"attendant\", \"awesome\", \"awesome\", \"awesome\", \"awsome\", \"awsome\", \"awsome\", \"backyard\", \"backyard\", \"backyard\", \"bad\", \"bad\", \"bad\", \"bagel\", \"bagel\", \"bagel\", \"baker\", \"baker\", \"baker\", \"bar\", \"bar\", \"bar\", \"barbershop\", \"barbershop\", \"barbershop\", \"bbb\", \"bbb\", \"bbb\", \"beer\", \"beer\", \"beer\", \"big\", \"big\", \"big\", \"bit\", \"bit\", \"bit\", \"bite\", \"bite\", \"bite\", \"bliss\", \"bliss\", \"bliss\", \"bmw\", \"bmw\", \"bmw\", \"boba\", \"boba\", \"boba\", \"bonne\", \"bonne\", \"bonne\", \"box\", \"box\", \"box\", \"brad\", \"brad\", \"brad\", \"bread\", \"bread\", \"bread\", \"brittany\", \"brittany\", \"brittany\", \"broth\", \"broth\", \"broth\", \"brow\", \"brow\", \"brow\", \"burger\", \"burger\", \"burger\", \"buy\", \"buy\", \"buy\", \"car\", \"car\", \"car\", \"caring\", \"caring\", \"caring\", \"cat\", \"cat\", \"cat\", \"catfish\", \"catfish\", \"catfish\", \"chain\", \"chain\", \"chain\", \"change\", \"change\", \"change\", \"check\", \"check\", \"check\", \"cheese\", \"cheese\", \"cheese\", \"chicken\", \"chicken\", \"chicken\", \"cirque\", \"cirque\", \"cirque\", \"clean\", \"clean\", \"clean\", \"cockroach\", \"cockroach\", \"cockroach\", \"consistent\", \"consistent\", \"consistent\", \"cook\", \"cook\", \"cook\", \"customer\", \"customer\", \"customer\", \"cute\", \"cute\", \"cute\", \"dans\", \"dans\", \"dans\", \"dawn\", \"dawn\", \"dawn\", \"day\", \"day\", \"day\", \"decor\", \"decor\", \"decor\", \"deeply\", \"deeply\", \"deeply\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"dependable\", \"dependable\", \"dependable\", \"didn\", \"didn\", \"didn\", \"dish\", \"dish\", \"dish\", \"dog\", \"dog\", \"dog\", \"don\", \"don\", \"don\", \"dread\", \"dread\", \"dread\", \"drink\", \"drink\", \"drink\", \"drop\", \"drop\", \"drop\", \"eat\", \"eat\", \"eat\", \"einstein\", \"einstein\", \"einstein\", \"english\", \"english\", \"english\", \"essayer\", \"essayer\", \"essayer\", \"establishment\", \"establishment\", \"establishment\", \"evening\", \"evening\", \"evening\", \"excellent\", \"excellent\", \"excellent\", \"exceptional\", \"exceptional\", \"exceptional\", \"experience\", \"experience\", \"experience\", \"face\", \"face\", \"face\", \"facial\", \"facial\", \"facial\", \"fade\", \"fade\", \"fade\", \"family\", \"family\", \"family\", \"far\", \"far\", \"far\", \"favorite\", \"favorite\", \"favorite\", \"feel\", \"feel\", \"feel\", \"fix\", \"fix\", \"fix\", \"flavor\", \"flavor\", \"flavor\", \"fresh\", \"fresh\", \"fresh\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"friendly\", \"fry\", \"fry\", \"fry\", \"german\", \"german\", \"german\", \"halo\", \"halo\", \"halo\", \"happy\", \"happy\", \"happy\", \"hatch\", \"hatch\", \"hatch\", \"help\", \"help\", \"help\", \"honest\", \"honest\", \"honest\", \"honey\", \"honey\", \"honey\", \"hour\", \"hour\", \"hour\", \"huge\", \"huge\", \"huge\", \"imperial\", \"imperial\", \"imperial\", \"instagram\", \"instagram\", \"instagram\", \"jackson\", \"jackson\", \"jackson\", \"kid\", \"kid\", \"kid\", \"lash\", \"lash\", \"lash\", \"lax\", \"lax\", \"lax\", \"leave\", \"leave\", \"leave\", \"lebanese\", \"lebanese\", \"lebanese\", \"let\", \"let\", \"let\", \"lion\", \"lion\", \"lion\", \"little\", \"little\", \"little\", \"loaf\", \"loaf\", \"loaf\", \"location\", \"location\", \"location\", \"long\", \"long\", \"long\", \"lot\", \"lot\", \"lot\", \"lunch\", \"lunch\", \"lunch\", \"massage\", \"massage\", \"massage\", \"meat\", \"meat\", \"meat\", \"mental\", \"mental\", \"mental\", \"menu\", \"menu\", \"menu\", \"mom\", \"mom\", \"mom\", \"money\", \"money\", \"money\", \"monitor\", \"monitor\", \"monitor\", \"mover\", \"mover\", \"mover\", \"mrs\", \"mrs\", \"mrs\", \"naan\", \"naan\", \"naan\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"oasis\", \"oasis\", \"oasis\", \"offer\", \"offer\", \"offer\", \"option\", \"option\", \"option\", \"overcrowded\", \"overcrowded\", \"overcrowded\", \"patient\", \"patient\", \"patient\", \"pay\", \"pay\", \"pay\", \"pen\", \"pen\", \"pen\", \"pick\", \"pick\", \"pick\", \"pizza\", \"pizza\", \"pizza\", \"pleasurable\", \"pleasurable\", \"pleasurable\", \"point\", \"point\", \"point\", \"poisoning\", \"poisoning\", \"poisoning\", \"pretty\", \"pretty\", \"pretty\", \"pristine\", \"pristine\", \"pristine\", \"product\", \"product\", \"product\", \"pub\", \"pub\", \"pub\", \"quality\", \"quality\", \"quality\", \"ranchero\", \"ranchero\", \"ranchero\", \"rate\", \"rate\", \"rate\", \"recommend\", \"recommend\", \"recommend\", \"res\", \"res\", \"res\", \"right\", \"right\", \"right\", \"rip\", \"rip\", \"rip\", \"roach\", \"roach\", \"roach\", \"room\", \"room\", \"room\", \"russian\", \"russian\", \"russian\", \"salad\", \"salad\", \"salad\", \"sample\", \"sample\", \"sample\", \"sanitation\", \"sanitation\", \"sanitation\", \"saturday\", \"saturday\", \"saturday\", \"sauce\", \"sauce\", \"sauce\", \"savoury\", \"savoury\", \"savoury\", \"science\", \"science\", \"science\", \"scope\", \"scope\", \"scope\", \"selection\", \"selection\", \"selection\", \"server\", \"server\", \"server\", \"shine\", \"shine\", \"shine\", \"shockingly\", \"shockingly\", \"shockingly\", \"shoot\", \"shoot\", \"shoot\", \"singer\", \"singer\", \"singer\", \"sisig\", \"sisig\", \"sisig\", \"small\", \"small\", \"small\", \"smoothie\", \"smoothie\", \"smoothie\", \"sole\", \"sole\", \"sole\", \"sonoran\", \"sonoran\", \"sonoran\", \"splash\", \"splash\", \"splash\", \"staff\", \"staff\", \"staff\", \"standing\", \"standing\", \"standing\", \"stay\", \"stay\", \"stay\", \"stop\", \"stop\", \"stop\", \"store\", \"store\", \"store\", \"stuff\", \"stuff\", \"stuff\", \"style\", \"style\", \"style\", \"super\", \"super\", \"super\", \"supper\", \"supper\", \"supper\", \"suv\", \"suv\", \"suv\", \"table\", \"table\", \"table\", \"taco\", \"taco\", \"taco\", \"taste\", \"taste\", \"taste\", \"thank\", \"thank\", \"thank\", \"todd\", \"todd\", \"todd\", \"toujours\", \"toujours\", \"toujours\", \"unappetizing\", \"unappetizing\", \"unappetizing\", \"upside\", \"upside\", \"upside\", \"uptown\", \"uptown\", \"uptown\", \"use\", \"use\", \"use\", \"variety\", \"variety\", \"variety\", \"veal\", \"veal\", \"veal\", \"vegas\", \"vegas\", \"vegas\", \"vietnamese\", \"vietnamese\", \"vietnamese\", \"village\", \"village\", \"village\", \"vision\", \"vision\", \"vision\", \"visit\", \"visit\", \"visit\", \"voucher\", \"voucher\", \"voucher\", \"walk\", \"walk\", \"walk\", \"wasn\", \"wasn\", \"wasn\", \"way\", \"way\", \"way\", \"week\", \"week\", \"week\", \"wine\", \"wine\", \"wine\", \"wonderful\", \"wonderful\", \"wonderful\", \"wow\", \"wow\", \"wow\", \"yard\", \"yard\", \"yard\", \"yearly\", \"yearly\", \"yearly\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el11441405839715730965955232636\", ldavis_el11441405839715730965955232636_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el11441405839715730965955232636\", ldavis_el11441405839715730965955232636_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el11441405839715730965955232636\", ldavis_el11441405839715730965955232636_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.008952  0.004142       1        1  36.270844\n",
       "1     -0.000520 -0.009100       2        1  32.401184\n",
       "0     -0.008432  0.004958       3        1  31.327969, topic_info=    Category         Freq        Term        Total  loglift  logprob\n",
       "465  Default   819.000000        wasn   819.000000  30.0000  30.0000\n",
       "213  Default  1468.000000  experience  1468.000000  29.0000  29.0000\n",
       "154  Default  1891.000000         don  1891.000000  28.0000  28.0000\n",
       "72   Default  1753.000000        nice  1753.000000  27.0000  27.0000\n",
       "103  Default  1165.000000       table  1165.000000  26.0000  26.0000\n",
       "..       ...          ...         ...          ...      ...      ...\n",
       "795   Topic3   332.556610         use  1117.618530  -0.0515  -5.8429\n",
       "154   Topic3   355.353485         don  1891.026245  -0.5111  -5.7766\n",
       "81    Topic3   332.222473        room  1125.175537  -0.0592  -5.8440\n",
       "114   Topic3   332.247864         fry  1167.557129  -0.0961  -5.8439\n",
       "570   Topic3   329.270264       taste  1154.820068  -0.0942  -5.8529\n",
       "\n",
       "[280 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "4346      1  0.586039  accessory\n",
       "4346      2  0.180320  accessory\n",
       "4346      3  0.225400  accessory\n",
       "4010      1  0.646549       alex\n",
       "4010      2  0.198938       alex\n",
       "...     ...       ...        ...\n",
       "1532      2  0.554025       yard\n",
       "1532      3  0.201464       yard\n",
       "5405      1  0.248236     yearly\n",
       "5405      2  0.248236     yearly\n",
       "5405      3  0.620591     yearly\n",
       "\n",
       "[621 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda, corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA shows us that there are 3 distinct topics, which might\n",
    "explain the uncertainty in our model's decision making. We have\n",
    "5 categories, where there are only 3 coherent topics. I would guess \n",
    "that the topics predicted by LDA are '1-star reviews', \n",
    "'2 & 3-star reviews', and '4 & 5-star reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
