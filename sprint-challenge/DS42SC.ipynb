{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string that has a lot of extra whitespace.\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(whitespace_string.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>March</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>April</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>May</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>June</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>July</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Day  Month  Year\n",
       "0     8  March  2015\n",
       "1    15  March  2015\n",
       "2    22  March  2015\n",
       "3    29  March  2015\n",
       "4     5  April  2015\n",
       "5    12  April  2015\n",
       "6    19  April  2015\n",
       "7    26  April  2015\n",
       "8     3    May  2015\n",
       "9    10    May  2015\n",
       "10   17    May  2015\n",
       "11   24    May  2015\n",
       "12   31    May  2015\n",
       "13    7   June  2015\n",
       "14   14   June  2015\n",
       "15   21   June  2015\n",
       "16   28   June  2015\n",
       "17    5   July  2015\n",
       "18   12   July  2015\n",
       "19   19   July  2015"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "r = requests.get(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt\")\n",
    "text = str(r.content)[2:-1].split(\"\\\\r\\\\n\")\n",
    "data = []\n",
    "for l in text:\n",
    "    d = {}\n",
    "    groups = re.match(r'(\\w+)\\s(\\d+),\\s(\\d+)', l).groups()\n",
    "    d[\"Day\"] = int(groups[1])\n",
    "    d[\"Month\"] = groups[0]\n",
    "    d[\"Year\"] = int(groups[2])\n",
    "    data.append(d)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brit2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brit2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\brit2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already, o]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, i, dentist,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                                 [sad, apl, friend]\n",
       "1          0                    [i, missed, new, moon, trailer]\n",
       "2          1                                  [omg, already, o]\n",
       "3          0  [omgaga, im, sooo, im, gunna, cry, i, dentist,...\n",
       "4          0                          [think, mi, bf, cheating]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def split_lemma(v):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [w.lower() for w in word_tokenize(v) if w.isalpha() and w not in stop_words]\n",
    "\n",
    "df = df_base.copy()\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(split_lemma)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "#### TF-IDF Scores\n",
    "\n",
    "TF-IDF, or Term Frequency - Inverse Document Frequency, is a measure of how important a word or term is to a document in a collection of documents. It is calculated through the multiplucation of the relative frequency of the term in the document and the inverse frequency of the term in the collection of documents compared to the total number of terms. However, for both the relative frequency of the term in the document and the inverse frequency of the term in the collection of documents, there are numerous ways to calculate these values. A common and simple method of doing this is:\n",
    "\n",
    "$${tf}(t,d) = \\frac{n_{t}}{n_{d}}$$\n",
    "$${idf}(t,D) = \\log{\\frac{N_{D}}{N_{d}}}$$\n",
    "$${tfidf} = {tf}(t,d) \\cdot {idf}(t,D)$$\n",
    "\n",
    "where:\n",
    "- ${tf(t,d)}$ is the relative frequency of the term in the document\n",
    "- ${idf(t,D)}$ is the inverse frequency of the term in the collection of documents\n",
    "- ${tfidf}$ is the term frequency inverse document frequency (TF-IDF)\n",
    "- ${n_{t}}$ is the frequency of the term in the document\n",
    "- ${n_{d}}$ is the frequency of all terms in the document\n",
    "- ${N_{d}}$ is the frequency of documents that the term appears in in the collection of documents\n",
    "- ${N_{D}}$ is the frequency of all terms in the collection of documents\n",
    "\n",
    "This allows for getting the importance of a term not only to the document, but also to the collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"SentimentText\"]], df[\"Sentiment\"], train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,1), stop_words='english')\n",
    "vectorizer.fit(X_train_vec)\n",
    "train_vec_2 = pd.DataFrame(vectorizer.transform(X_train_vec).toarray(), columns=vectorizer.get_feature_names(), index=X_train.index)\n",
    "test_vec_2 = pd.DataFrame(vectorizer.transform(X_test_vec).toarray(), columns=vectorizer.get_feature_names(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brit2\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5012563913440262\n",
      "0.5074007400740074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(train_vec, y_train)\n",
    "print(LR.score(train_vec_2, y_train))\n",
    "print(LR.score(test_vec_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5049193034216349\n",
      "0.5096009600960096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(train_vec, y_train)\n",
    "print(MNB.score(train_vec_2, y_train))\n",
    "print(MNB.score(test_vec_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train_vec = [\" \".join(c) for c in X_train[\"SentimentText\"].values]\n",
    "X_test_vec = [\" \".join(c) for c in X_test[\"SentimentText\"].values]\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), max_features=10000)\n",
    "tfidf.fit(X_train_vec)\n",
    "train_vec = pd.DataFrame(tfidf.transform(X_train_vec).toarray(), columns=tfidf.get_feature_names(), index=X_train.index)\n",
    "test_vec = pd.DataFrame(tfidf.transform(X_test_vec).toarray(), columns=tfidf.get_feature_names(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brit2\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915015439236914\n",
      "0.7545754575457546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=42).fit(train_vec, y_train)\n",
    "print(LR.score(train_vec, y_train))\n",
    "print(LR.score(test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7841882211748822\n",
      "0.7464246424642464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB().fit(train_vec, y_train)\n",
    "print(MNB.score(train_vec, y_train))\n",
    "print(MNB.score(test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brit2\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8710169237800546\n",
      "0.8311230606585811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(y_train, LR.predict_proba(train_vec)[:,1]))\n",
    "print(roc_auc_score(y_test, LR.predict_proba(test_vec)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1\n",
      "\t ('alexmace', 1.0)\n",
      "\t ('homecoming', 1.0)\n",
      "\t ('lovemaking', 1.0)\n",
      "\t ('liveee', 1.0)\n",
      "\t ('pottstown', 1.0)\n",
      "\t ('canfield', 1.0)\n",
      "\t ('alexmatheson', 1.0)\n",
      "\t ('thoguht', 1.0)\n",
      "\t ('feaked', 1.0)\n",
      "\t ('alexmerced', 1.0)\n",
      "\n",
      "Size: 2\n",
      "\t ('didsys', 1.0000001192092896)\n",
      "\t ('chuckrey', 1.0)\n",
      "\t ('clemartdesign', 1.0)\n",
      "\t ('berniegrace', 1.0)\n",
      "\t ('tulog', 1.0)\n",
      "\t ('skype', 1.0)\n",
      "\t ('walked', 1.0)\n",
      "\t ('mall', 1.0)\n",
      "\t ('nah', 1.0)\n",
      "\t ('constantknot', 1.0)\n",
      "\n",
      "Size: 3\n",
      "\t ('biiig', 0.9999903440475464)\n",
      "\t ('arrested', 0.9999868869781494)\n",
      "\t ('cheekylamb', 0.9999712109565735)\n",
      "\t ('aliciawag', 0.9999397993087769)\n",
      "\t ('implement', 0.9999380111694336)\n",
      "\t ('chinta', 0.9998846650123596)\n",
      "\t ('zeit', 0.9998795390129089)\n",
      "\t ('brettislame', 0.9998740553855896)\n",
      "\t ('sincerely', 0.9998714923858643)\n",
      "\t ('azveganchik', 0.9998651742935181)\n",
      "\n",
      "Size: 4\n",
      "\t ('disapointed', 0.999488115310669)\n",
      "\t ('masseuse', 0.9993946552276611)\n",
      "\t ('tuenti', 0.9989905953407288)\n",
      "\t ('kreactive', 0.9987278580665588)\n",
      "\t ('interlude', 0.9986570477485657)\n",
      "\t ('chefrosebud', 0.9984012246131897)\n",
      "\t ('quot', 0.9983609914779663)\n",
      "\t ('department', 0.9983447194099426)\n",
      "\t ('bonzablue', 0.9983276128768921)\n",
      "\t ('addy', 0.9982764720916748)\n",
      "\n",
      "Size: 5\n",
      "\t ('maryland', 0.9993765950202942)\n",
      "\t ('sneezes', 0.9969210028648376)\n",
      "\t ('meeeeeeee', 0.9962127208709717)\n",
      "\t ('shakespearesaturday', 0.996020495891571)\n",
      "\t ('yellows', 0.995347797870636)\n",
      "\t ('warnings', 0.9951906800270081)\n",
      "\t ('pees', 0.9950819611549377)\n",
      "\t ('bexblonde', 0.9950405955314636)\n",
      "\t ('superman', 0.9939212203025818)\n",
      "\t ('bawa', 0.9936029314994812)\n",
      "\n",
      "Size: 6\n",
      "\t ('skarang', 0.9974748492240906)\n",
      "\t ('cheycalouro', 0.9946697354316711)\n",
      "\t ('advicepig', 0.9918873906135559)\n",
      "\t ('picanic', 0.9917364716529846)\n",
      "\t ('cameronreilly', 0.9914115071296692)\n",
      "\t ('stellalive', 0.9911935329437256)\n",
      "\t ('bomn', 0.990666925907135)\n",
      "\t ('guangzhou', 0.9897284507751465)\n",
      "\t ('adrielhampton', 0.9894282221794128)\n",
      "\t ('amandaaaaaaaaaa', 0.9892399907112122)\n",
      "\n",
      "Size: 7\n",
      "\t ('festive', 0.9953566193580627)\n",
      "\t ('hypocrite', 0.9951341152191162)\n",
      "\t ('astronomer', 0.9899500608444214)\n",
      "\t ('niceee', 0.9872000217437744)\n",
      "\t ('aprileelcich', 0.9870198965072632)\n",
      "\t ('fibro', 0.9857727289199829)\n",
      "\t ('grrrrrrrrr', 0.9836887121200562)\n",
      "\t ('evolution', 0.9826293587684631)\n",
      "\t ('stumped', 0.9820171594619751)\n",
      "\t ('momentary', 0.9818450212478638)\n",
      "\n",
      "Size: 8\n",
      "\t ('omgssh', 0.994279146194458)\n",
      "\t ('brianzwolinski', 0.9787446856498718)\n",
      "\t ('vnv', 0.9771959781646729)\n",
      "\t ('bamboozle', 0.9759553670883179)\n",
      "\t ('annieearly', 0.9752804040908813)\n",
      "\t ('link', 0.9748455882072449)\n",
      "\t ('mycrack', 0.9746325016021729)\n",
      "\t ('privilege', 0.9742053747177124)\n",
      "\t ('awakeningstweet', 0.973713755607605)\n",
      "\t ('momentary', 0.9733511209487915)\n",
      "\n",
      "Size: 9\n",
      "\t ('kinnaka', 0.9821469783782959)\n",
      "\t ('privilege', 0.9749926328659058)\n",
      "\t ('aaronasay', 0.972716212272644)\n",
      "\t ('aeusmcgirl', 0.9713616371154785)\n",
      "\t ('hypocrite', 0.967837393283844)\n",
      "\t ('vnv', 0.9653139114379883)\n",
      "\t ('wtfdjflasjfl', 0.9639029502868652)\n",
      "\t ('awakeningstweet', 0.9618625640869141)\n",
      "\t ('link', 0.9608722925186157)\n",
      "\t ('dms', 0.9607584476470947)\n",
      "\n",
      "Size: 10\n",
      "\t ('kinnaka', 0.981053352355957)\n",
      "\t ('wtfdjflasjfl', 0.9678444266319275)\n",
      "\t ('aaronasay', 0.9650819301605225)\n",
      "\t ('beherenw', 0.9627469182014465)\n",
      "\t ('privilege', 0.9624449014663696)\n",
      "\t ('cinemabizarre', 0.957956075668335)\n",
      "\t ('advertise', 0.9552394151687622)\n",
      "\t ('akarra', 0.9524139761924744)\n",
      "\t ('alba', 0.9522687196731567)\n",
      "\t ('bluefur', 0.9512163400650024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "for i in range(1,11):\n",
    "    model = Word2Vec(df[\"SentimentText\"].values, min_count=1, size=i)\n",
    "    print(\"Size:\", i)\n",
    "    for v in model.wv.most_similar('twitter'):\n",
    "        print(\"\\t\", v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
