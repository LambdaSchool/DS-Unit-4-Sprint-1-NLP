{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Heterogeneous Features with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing using scikit-learn's CountVectorizer or libraries such as spaCY and Gensim can provide powerful insights into text data, allowing us to extract topics which can then be added as features and regressed on to generate predictions.\n",
    "\n",
    "However, what if we want to use the sparse matrix that Countvectorizer produces as a feature along with other categoricals or numerical features in the dataset?\n",
    "\n",
    "The answer is Column-Transformer, and I'll demonstrate it's usage on some yelp review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# text processing imports\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# scikit-learn pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#encoders\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#randomized search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# !python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "yelp = pd.read_json('./data/review_sample.json', lines=True)\n",
    "\n",
    "# First I regex and tokenize using spaCY's lemmatizer\n",
    "cleaning = ['text']\n",
    "\n",
    "def regex_clean(dataframe, target_list):    \n",
    "    for target in target_list:\n",
    "        dataframe[target].apply(lambda x: re.sub(r'[^a-zA-Z ^0-9]', '', x))\n",
    "        dataframe[target].apply(lambda x: re.sub(r'/n', '', x))\n",
    "                        \n",
    "\n",
    "custom_stops = ['<', '>', '\\n', '\"', '\\\\', '|', '</div>', '<ul>', '<li>', '<p>', 'li', 'ul', 'p', ']' , '\\>', '\\n\\n']\n",
    "\n",
    "def tokenize(text): #lemmatize actually\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if ((token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON')):\n",
    "            valid = True\n",
    "            \n",
    "            for stop in custom_stops:\n",
    "                    if stop in token.text:\n",
    "                        valid = False\n",
    "                        break\n",
    "        else:\n",
    "            valid = False\n",
    "                \n",
    "        if valid == True:   \n",
    "            lemmas.append(token.lemma_)\n",
    "    return lemmas\n",
    "\n",
    "regex_clean(yelp, cleaning)\n",
    "yelp['lemmas'] = yelp['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCY's lemmas are now very clean and can be processed by TFIDF into a sparse vector matrix once I process them back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll rejoin the lemmas into a string that TFIDF can process\n",
    "#de-tokenize\n",
    "detokenized_doc = []\n",
    "for i in range(len(yelp['lemmas'])):\n",
    "    t = ' '.join(yelp['lemmas'][i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "yelp['lemma_text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to predict the star rating of a given review, we can use a pipeline to TFIDF vectorize and then predict the review stars. In fact, let's use our pipeline to feed our TFIDF matrix into a TruncatedSVD, which will can perform principle component analysis with a sparse matrix, this should improve our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp['lemma_text']\n",
    "y = yelp['stars']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "svd = TruncatedSVD(n_components=500,\n",
    "                  algorithm='randomized',\n",
    "                  n_iter=10,\n",
    "                  random_state=42)\n",
    "\n",
    "# text processing pipeline\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
    "\n",
    "# classifier\n",
    "clf = SGDClassifier(max_iter=5)\n",
    "\n",
    "# Classifier pipeline\n",
    "pipe = Pipeline([('vect', vect), ('clf', clf)])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "print(\"train model score: %.3f\" % pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is not particularly impressive (I got 57.8% when I ran it) and could be improved with some hyperparamater tuning using RandomizedSearchCV, however, perhaps we would like to take advantage of some of the other features of the dataset first?\n",
    "The 'cool', 'funny', and 'useful' ratings may be helpful. Also, maybe some users leave a certain type of review? Or some businesses are just great, or terrible and always receive the same type of review?\n",
    "Combining all those features with our NLP should improve our model.\n",
    "This brings us to the Column Transformer. This is a modified pipeline that will act as a pipeline, or take multiple pipelines, and concatenate the outputs so we can regress on them. Let's try adding an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "    [\n",
    "    ('onehot', StandardScaler(), ['cool', 'funny', 'useful']),\n",
    "    ('lsi', lsi, 'lemma_text')],\n",
    "    n_jobs=-1, remainder='drop', verbose=True)\n",
    "\n",
    "column_pipe = Pipeline([('ct', column_trans), ('clf', clf)])\n",
    "\n",
    "X = yelp.drop(columns=['stars']) \n",
    "y = yelp['stars']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "column_pipe.fit(X_train,y_train)\n",
    "\n",
    "print(\"train model score: %.3f\" % column_pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note, first our score didn't improve, but we can do some hyperparameter tuning to see if it does. Secondly, we can pass our original `lsi` pipeline directly into the `ColumnTransformer()` we also have the ability to drop all columns not specified (though we could pass them through and the CT would append them to the bag of words matrix that the TFIDF Vectorizer produces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Randomized search cv to optimize\n",
    "params = {\n",
    "    'ct__lsi__vect__max_df' : (.5, .7, 1),\n",
    "    'ct__lsi__vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__loss' : ('hinge', 'modified_huber'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'ct__lsi__svd__n_components': (2, 50, 75, 100)\n",
    "}\n",
    "\n",
    "rsCV = RandomizedSearchCV(estimator=column_pipe, param_distributions=params, \n",
    "                          cv=5, n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "rsCV.fit(X_train,y_train)\n",
    "\n",
    "rsCV.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get a minor improvement on our first attempt at using multiple features. The power of the ColumnTransformer is in enabling you to specify and then tune the treatment of each feature in your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
