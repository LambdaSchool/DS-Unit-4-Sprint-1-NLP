{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_424_Word_Embeddings_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASfGeMfI6Kgs"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
        "\n",
        "# Word Embeddings\n",
        "\n",
        "### Use Word2Vec to train your own model on a dataset.\n",
        "\n",
        "1) **Optional** - Find your own dataset of documents to train you model on. You are going to need a lot of data, so it's probably not realistic to scrape data for this assignment given the time constraints that we're working under. Try to find a dataset that has > 5000 documents.\n",
        "\n",
        "- If you can't find a dataset to use try this one: <https://www.kaggle.com/c/quora-question-pairs>\n",
        "\n",
        "2) Clean/Tokenize the documents.\n",
        "\n",
        "3) Vectorize the model using Word2Vec and explore the results using each of the following at least one time:\n",
        "\n",
        "- your_model.wv.most_similar()\n",
        "- your_model.wv.similarity()\n",
        "- your_model.wv.doesn't_match()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q9Pwm31esBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a6e27605-71d9-4e52-d8f2-ac7ef3eeaa46"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wy5lYo4K8wEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88127ac8-4242-4555-d822-787c31e4f5a6"
      },
      "source": [
        "data_scientist = pd.read_csv('data_scientist_jobs_indeed.csv')\n",
        "data_scientist.Title = \"Data Scientist\"\n",
        "data_analyst = pd.read_csv('data_analyst_jobs_indeed.csv')\n",
        "data_analyst.Title = \"Data Analyst\"\n",
        "data_engineer = pd.read_csv('data_engineer_jobs_indeed.csv')\n",
        "data_engineer.Title = \"Data Engineer\"\n",
        "\n",
        "df = pd.concat([data_scientist, data_analyst, data_engineer], axis=0).drop_duplicates()\n",
        "df.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13180, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPRuV4x2fV_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "bd57fccb-4a94-4595-bb3f-48ab7583655b"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Location</th>\n",
              "      <th>Company</th>\n",
              "      <th>Salary</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1836</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Richmond, VA 23219 (City Center area)</td>\n",
              "      <td>Afton Chemical</td>\n",
              "      <td>\\n                $86,000 - $126,000 a year (I...</td>\n",
              "      <td>This data scientist position will ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3048</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Hollywood, CA 90028</td>\n",
              "      <td>Ticketmaster</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Carry out tasks encompassing data ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3994</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alexander Technology Group</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The right fit for this Data Analys...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1954</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Cincinnati, OH 45249</td>\n",
              "      <td>Procter and Gamble</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As a PhD Data Scientist, you'll be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Cambridge, MA 02140 (North Cambridge area)</td>\n",
              "      <td>Voluntis</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We seek an exceptional data scient...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Title                                    Location  \\\n",
              "1836  Data Scientist       Richmond, VA 23219 (City Center area)   \n",
              "3048  Data Scientist                         Hollywood, CA 90028   \n",
              "3994    Data Analyst                                         NaN   \n",
              "1954  Data Scientist                        Cincinnati, OH 45249   \n",
              "2705  Data Scientist  Cambridge, MA 02140 (North Cambridge area)   \n",
              "\n",
              "                                 Company  \\\n",
              "1836                      Afton Chemical   \n",
              "3048                        Ticketmaster   \n",
              "3994          Alexander Technology Group   \n",
              "1954                  Procter and Gamble   \n",
              "2705                            Voluntis   \n",
              "\n",
              "                                                 Salary  \\\n",
              "1836  \\n                $86,000 - $126,000 a year (I...   \n",
              "3048                                                NaN   \n",
              "3994                                                NaN   \n",
              "1954                                                NaN   \n",
              "2705                                                NaN   \n",
              "\n",
              "                                                Summary  \n",
              "1836              This data scientist position will ...  \n",
              "3048              Carry out tasks encompassing data ...  \n",
              "3994              The right fit for this Data Analys...  \n",
              "1954              As a PhD Data Scientist, you'll be...  \n",
              "2705              We seek an exceptional data scient...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94MSAL_UfV8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset to only Title and Summary\n",
        "df = df[['Title','Summary']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAUZ9NxSfV6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process text function\n",
        "\n",
        "def process_text(text):\n",
        "  from nltk.corpus import stopwords\n",
        "  \"\"\"Remove punctuation, lowercase, and tokenize text.\"\"\"\n",
        "  # TODO: check for special cases like \"I'll\"\n",
        "  text = \"\".join([char.lower() for char in text\n",
        "                  if char not in string.punctuation])\n",
        "  stop = stopwords.words('english')\n",
        "  return [i for i in word_tokenize(text) if i not in stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4QwMMC8os_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e7315812-dff9-4fd1-9ec9-051daeeee423"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Title      0\n",
              "Summary    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZQW_rTpPyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c30e7c3a-2b18-4582-dc2a-5eb218b038af"
      },
      "source": [
        "df['Title'].value_counts(normalize=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data Analyst      0.350607\n",
              "Data Scientist    0.343930\n",
              "Data Engineer     0.305463\n",
              "Name: Title, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6fHZ9Nrqsnq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "984e21ce-17db-4275-a596-037ac054b477"
      },
      "source": [
        "process_text(\"Testing the process text function, hope it's working! . , '' + - = ;;;; ; '.,*&^$#@#$%'\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testing', 'process', 'text', 'function', 'hope', 'working']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi2d_ErfrNRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Tokens'] = df['Summary'].apply(process_text) # Don't forget to take care of missing values if there are any"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nII4oFrrkR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a33612e0-5c8a-4eaf-d7be-f76b23e71c9e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Natural Language Processing (NLP),...</td>\n",
              "      <td>[natural, language, processing, nlp, speech, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Conduct exploratory data analysis,...</td>\n",
              "      <td>[conduct, exploratory, data, analysis, mine, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Data Scientist to join the Data Science team. ...</td>\n",
              "      <td>[data, scientist, join, data, science, team, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Structured and unstructured data:....</td>\n",
              "      <td>[structured, unstructured, data, scale, analyt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Research, evaluate, design and imp...</td>\n",
              "      <td>[research, evaluate, design, implement, machin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Title                                            Summary  \\\n",
              "0  Data Scientist              Natural Language Processing (NLP),...   \n",
              "1  Data Scientist              Conduct exploratory data analysis,...   \n",
              "4  Data Scientist  Data Scientist to join the Data Science team. ...   \n",
              "5  Data Scientist              Structured and unstructured data:....   \n",
              "6  Data Scientist              Research, evaluate, design and imp...   \n",
              "\n",
              "                                              Tokens  \n",
              "0  [natural, language, processing, nlp, speech, d...  \n",
              "1  [conduct, exploratory, data, analysis, mine, d...  \n",
              "4  [data, scientist, join, data, science, team, d...  \n",
              "5  [structured, unstructured, data, scale, analyt...  \n",
              "6  [research, evaluate, design, implement, machin...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cz5boUJNAk6",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3T7_v6er_xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(df['Tokens'], min_count=5, size=100, seed=420)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0bivBswr_1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "564db38c-df19-45a5-9ac4-de32d88ba9d7"
      },
      "source": [
        "model.wv.most_similar('python')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('r', 0.9873111248016357),\n",
              " ('talend', 0.9763953685760498),\n",
              " ('hadoop', 0.9690969586372375),\n",
              " ('spark', 0.9584083557128906),\n",
              " ('tableau', 0.9536489248275757),\n",
              " ('programming', 0.9522199630737305),\n",
              " ('eg', 0.9513393640518188),\n",
              " ('language', 0.949604868888855),\n",
              " ('preferred', 0.9491496086120605),\n",
              " ('demonstrated', 0.9485483765602112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD7Xw-RSNLk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "02a00413-7145-478e-ebb4-3ff7fc0b870f"
      },
      "source": [
        "model.wv.similarity('visualization', 'python')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8155188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUZO9qLaNDpQ",
        "colab_type": "text"
      },
      "source": [
        "# Count Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVVs7iWaNPvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove list brackets\n",
        "df['Tokens'] = [','.join(map(str, l)) for l in df['Tokens']]\n",
        "\n",
        "# Get rid of numbers\n",
        "df['Tokens'] = df['Tokens'].str.replace('\\d+', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDDv4rtqsAP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "be6a0d91-dc78-4a32-d085-993f4daf4553"
      },
      "source": [
        "# Instantiate CountVectorizer\n",
        "cvec = CountVectorizer(ngram_range=(2,2), max_features=20000)\n",
        "\n",
        "# Fir the CountVectorizer and transform the data\n",
        "vec_ds = cvec.fit_transform(df['Tokens'])\n",
        "vec_ds = cvec.fit_transform(df['Tokens'])\n",
        "\n",
        "\n",
        "df_vec  = pd.DataFrame(vectorizers.todense(), columns=cvec.get_feature_names())\n",
        "print (df_vec.shape)\n",
        "df_vec.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13180, 20000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ab testing</th>\n",
              "      <th>ab tests</th>\n",
              "      <th>abilities create</th>\n",
              "      <th>abilities passionate</th>\n",
              "      <th>ability analyze</th>\n",
              "      <th>ability apply</th>\n",
              "      <th>ability assist</th>\n",
              "      <th>ability bring</th>\n",
              "      <th>ability build</th>\n",
              "      <th>ability clearly</th>\n",
              "      <th>...</th>\n",
              "      <th>zestfinance develop</th>\n",
              "      <th>zillow group</th>\n",
              "      <th>zillow groups</th>\n",
              "      <th>zipcar seeking</th>\n",
              "      <th>zynga one</th>\n",
              "      <th>专业为统计经济信息系统计算机等stem类皆可 需要有基础编程技能比如对sql较熟悉</th>\n",
              "      <th>医疗保险公司data analyst全职实习机会</th>\n",
              "      <th>工作地点可选san francisco</th>\n",
              "      <th>总部位于湾区的医疗保险公司招数据分析师需要至少学士学历或在读生 专业为统计经济信息系统计算机等stem类皆可</th>\n",
              "      <th>需要有基础编程技能比如对sql较熟悉 工作地点可选san</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 20000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ab testing  ab tests  abilities create  abilities passionate  \\\n",
              "0           0         0                 0                     0   \n",
              "1           0         0                 0                     0   \n",
              "2           0         0                 0                     0   \n",
              "3           0         0                 0                     0   \n",
              "4           0         0                 0                     0   \n",
              "\n",
              "   ability analyze  ability apply  ability assist  ability bring  \\\n",
              "0                0              0               0              0   \n",
              "1                0              0               0              0   \n",
              "2                0              0               0              0   \n",
              "3                0              0               0              0   \n",
              "4                0              0               0              0   \n",
              "\n",
              "   ability build  ability clearly  ...  zestfinance develop  zillow group  \\\n",
              "0              0                0  ...                    0             0   \n",
              "1              0                0  ...                    0             0   \n",
              "2              0                0  ...                    0             0   \n",
              "3              0                0  ...                    0             0   \n",
              "4              0                0  ...                    0             0   \n",
              "\n",
              "   zillow groups  zipcar seeking  zynga one  \\\n",
              "0              0               0          0   \n",
              "1              0               0          0   \n",
              "2              0               0          0   \n",
              "3              0               0          0   \n",
              "4              0               0          0   \n",
              "\n",
              "   专业为统计经济信息系统计算机等stem类皆可 需要有基础编程技能比如对sql较熟悉  医疗保险公司data analyst全职实习机会  \\\n",
              "0                                          0                         0   \n",
              "1                                          0                         0   \n",
              "2                                          0                         0   \n",
              "3                                          0                         0   \n",
              "4                                          0                         0   \n",
              "\n",
              "   工作地点可选san francisco  \\\n",
              "0                    0   \n",
              "1                    0   \n",
              "2                    0   \n",
              "3                    0   \n",
              "4                    0   \n",
              "\n",
              "   总部位于湾区的医疗保险公司招数据分析师需要至少学士学历或在读生 专业为统计经济信息系统计算机等stem类皆可  \\\n",
              "0                                                  0        \n",
              "1                                                  0        \n",
              "2                                                  0        \n",
              "3                                                  0        \n",
              "4                                                  0        \n",
              "\n",
              "   需要有基础编程技能比如对sql较熟悉 工作地点可选san  \n",
              "0                             0  \n",
              "1                             0  \n",
              "2                             0  \n",
              "3                             0  \n",
              "4                             0  \n",
              "\n",
              "[5 rows x 20000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Il2Pjg5JMjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "070e099d-20df-40b8-9b91-52393f7f779d"
      },
      "source": [
        "# The most common words\n",
        "\n",
        "df_vec.sum().sort_values(ascending=False)[40:80]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "engineer data            231\n",
              "data collection          229\n",
              "analyst data             221\n",
              "management data          220\n",
              "join team                219\n",
              "data platform            217\n",
              "including data           217\n",
              "analyst responsible      215\n",
              "data governance          213\n",
              "business data            212\n",
              "work closely             203\n",
              "engineers data           191\n",
              "modeling data            184\n",
              "build data               183\n",
              "data structures          182\n",
              "data integrity           177\n",
              "knowledge data           176\n",
              "new data                 173\n",
              "scientists data          171\n",
              "experienced data         169\n",
              "work data                169\n",
              "analyst join             164\n",
              "use data                 160\n",
              "business intelligence    160\n",
              "science team             159\n",
              "scientist join           157\n",
              "data cleansing           156\n",
              "mining data              156\n",
              "working data             153\n",
              "perform data             152\n",
              "multiple data            149\n",
              "using data               148\n",
              "data technologies        144\n",
              "building data            143\n",
              "scientist data           142\n",
              "quality data             140\n",
              "engineer responsible     136\n",
              "healthcare data          135\n",
              "data multiple            134\n",
              "unstructured data        134\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xkzdOZm38yQ_"
      },
      "source": [
        "### Stretch Goals:\n",
        "\n",
        "1) Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents.\n",
        "\n",
        "2) Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "  - Load the pre-trained word vectors and train the Word2vec model\n",
        "\n",
        "  - Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "  - Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "  - Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "  - For example:\n",
        "\n",
        "    - model.similarity('house', 'bungalow')\n",
        "\n",
        "    - model.similarity('house', 'umbrella')\n",
        "    \n",
        "3) Word2Vec is not the latest or greatest wording embedding model, but it has been around long enough to have well developed APIs in several of the major packages. Try one of the State of the Art (SoA) embeddings models on your dataset. The state of the art models are BERT and Elmo (and yes, that is a pun). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gakr5rP76IAJ",
        "colab_type": "text"
      },
      "source": [
        "# Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU9shTHbXS9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmMzTZd-QeYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5e37a57d-ba45-4cc1-d5c9-75d125891221"
      },
      "source": [
        "df = pd.concat([data_scientist, data_analyst, data_engineer], axis=0).drop_duplicates()\n",
        "df = df[['Title', 'Summary']]\n",
        "df.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Natural Language Processing (NLP),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Conduct exploratory data analysis,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Data Scientist to join the Data Science team. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Structured and unstructured data:....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Research, evaluate, design and imp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Title                                            Summary\n",
              "0  Data Scientist              Natural Language Processing (NLP),...\n",
              "1  Data Scientist              Conduct exploratory data analysis,...\n",
              "4  Data Scientist  Data Scientist to join the Data Science team. ...\n",
              "5  Data Scientist              Structured and unstructured data:....\n",
              "6  Data Scientist              Research, evaluate, design and imp..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsMZcj-IXOte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "270f67a8-4f08-40b1-bd5c-efa0c033a9d1"
      },
      "source": [
        "# Tokenize the sentences\n",
        "\n",
        "df['Summary'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "455292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglVL7WeWKMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_process(text):\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "    return text\n",
        "\n",
        "df['Summary'] = df['Summary'].apply(cleanText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFb1svjFYw33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2e648e04-bd61-4ab9-9a62-3779df0db334"
      },
      "source": [
        "df['Summary'][0]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                natural language processing (nlp),...\n",
              "0    data/systems integration analyst:. business an...\n",
              "0                ensures adherence to enterprise da...\n",
              "Name: Summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhMRW_g4Yw6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import doc2vec\n",
        "\n",
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the complaint narrative.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1LZNRM6bgaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.Summary, df.Title, random_state=420, test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0c_ZvZHbr6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bb03bef-4538-49cc-d13d-98d44b9af56e"
      },
      "source": [
        "len(all_data)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG015u4Sbsup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "94b9a044-2b76-4f4c-912e-8ea3a7a8fc32"
      },
      "source": [
        "all_data[:2]\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['data', 'scientist,', 'life', 'eperience', 'studies.', 'as', 'our', 'data', 'scientist,', 'life', 'eperience', 'studies,', 'you', 'will', 'have', 'an', 'ecellent', 'opportunity', 'to', 'join', 'a', 'highly-motivated', 'team', 'responsible', 'for', 'providing', 'analytical', 'insights', 'and', 'eperience', 'studies', 'to', 'the...'], tags=['Train_0']),\n",
              " TaggedDocument(words=['do', 'you', 'enjoy', 'designing', 'enterprise', 'level', 'research', 'and', 'analytic', 'frameworks?', 'how', 'would', 'you', 'like', 'to', 'turn', 'on', 'your', 'creative', 'genius', 'and', 'problem-solving', 'alongside', 'busin...'], tags=['Train_1'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlOL5Nq_bxUZ",
        "colab_type": "text"
      },
      "source": [
        "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are “continuous bag of words” (cbow) and “skip-gram” (sg); in the Doc2Vec architecture, the corresponding algorithms are “distributed bag of words” (dbow) and “distributed memory” (dm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJmLVvLub55I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a058571-77a4-4cfd-8b73-bd90d88519db"
      },
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13180/13180 [00:00<00:00, 1555194.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghDxvbkbb-em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "ba2c3e43-531f-4d8b-cf57-24ed18f7fee4"
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13180/13180 [00:00<00:00, 2158483.73it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1032246.46it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2163975.84it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2255996.03it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1534473.07it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2203656.49it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2203920.05it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1475574.60it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2178558.69it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2269518.30it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2237550.66it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2335386.20it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2396017.98it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2253145.58it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2301549.89it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2101377.08it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1833408.29it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1505102.96it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2323802.04it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2185103.23it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2289444.49it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1576392.34it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2174530.99it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2323411.37it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1632728.65it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 1162047.56it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2158399.45it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2163891.13it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2204886.99it/s]\n",
            "100%|██████████| 13180/13180 [00:00<00:00, 2292862.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 43.3 s, sys: 4.48 s, total: 47.8 s\n",
            "Wall time: 27.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKtiYrwwcBZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lPRbhyLcINt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdBFLXe3iD8k",
        "colab_type": "text"
      },
      "source": [
        "# Now let's classify the vectors..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P30T-PSiNQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs',\n",
        "                             max_iter=1000)\n",
        "xgb = XGBClassifier(max_depth=7, n_estimators=100, n_jobs=-1)\n",
        "svc = LinearSVC()\n",
        "ada = AdaBoostClassifier()\n",
        "gaus = GaussianNB()\n",
        "\n",
        "def train_and_score(classifier):\n",
        "  classifier.fit(train_vectors_dbow, y_train)\n",
        "  print(classifier.score(test_vectors_dbow, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsX-hX_bcLqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eb8394f6-d0fa-4afd-8213-367266812d2d"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_and_score(log_reg)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8335862417804755\n",
            "CPU times: user 5.82 s, sys: 952 ms, total: 6.77 s\n",
            "Wall time: 3.44 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EUHDquIeYt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "75ee3e77-ec24-492b-db21-d3321ab8b502"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_and_score(xgb)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8662114314618108\n",
            "CPU times: user 2min 25s, sys: 29.9 ms, total: 2min 25s\n",
            "Wall time: 2min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssROBqnzinw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3b949ad5-4326-4623-bf91-dcb7e3caba29"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_and_score(svc)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8394031360647446\n",
            "CPU times: user 16.2 s, sys: 81.3 ms, total: 16.3 s\n",
            "Wall time: 16.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vf6ASEmsLLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e85816a-8c3b-45ca-e0ad-47c6176035d7"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_and_score(ada)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7384926656550329\n",
            "CPU times: user 21.2 s, sys: 4.78 ms, total: 21.2 s\n",
            "Wall time: 21.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRitzscdsMlC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9df0b977-f36a-4f1e-8bfc-5cbfe00d8a63"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_and_score(gaus)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7835103692463329\n",
            "CPU times: user 86.7 ms, sys: 2.8 ms, total: 89.5 ms\n",
            "Wall time: 90.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSaNR5-_keCW",
        "colab_type": "text"
      },
      "source": [
        "I was able able to predict which job summaries fall into the categories of Data Science, Data Engineer, and Data Analyst with ~85% accuracy."
      ]
    }
  ]
}