{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_424_Word_Embeddings_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donw385/DS-Unit-4-Sprint-2-NLP/blob/master/module4-Word-Embeddings/LS_DS_424_Word_Embeddings_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfGeMfI6Kgs",
        "colab_type": "text"
      },
      "source": [
        "### Use Word2Vec to train your own model on a dataset.\n",
        "\n",
        "1) **Optional** - Find your own dataset of documents to train you model on. You are going to need a lot of data, so it's probably not realistic to scrape data for this assignment given the time constraints that we're working under. Try to find a dataset that has > 5000 documents.\n",
        "\n",
        "- If you can't find a dataset to use try this one: <https://www.kaggle.com/c/quora-question-pairs>\n",
        "\n",
        "2) Clean/Tokenize the documents.\n",
        "\n",
        "3) Vectorize the model using Word2Vec and explore the results using each of the following at least one time:\n",
        "\n",
        "- your_model.wv.most_similar()\n",
        "- your_model.wv.similarity()\n",
        "- your_model.wv.doesn't_match()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy5lYo4K8wEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "bbe13353-f5af-4ce0-efa8-2512fdd8c4cb"
      },
      "source": [
        "!pip install kaggle\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS3Jjw23Jmw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d26d43a3-4794-4abb-b13f-6f243745e359"
      },
      "source": [
        "!pip install -U gensim\n",
        "import gensim"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/f1/d25dfdf1d28222124b920108b89f3f7acc2dad506014990c10fb34f7104b/gensim-3.7.2-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.138)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.138 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.138)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.24.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.138->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.138->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzY6JvbYJA0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b42c4761-b1e8-46e5-b525-3d98ccc459aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAbfFEV6JKfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "117759b6-9a41-4535-cbb3-093e25b56509"
      },
      "source": [
        "%env KAGGLE_CONFIG_DIR=/content/drive/My Drive/\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: KAGGLE_CONFIG_DIR=/content/drive/My Drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzfrenpzJLl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "a4d1496f-7cfa-45e0-e18a-caae39ef9225"
      },
      "source": [
        "!kaggle competitions download -c quora-question-pairs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/4.95M [00:00<?, ?B/s]\n",
            "100% 4.95M/4.95M [00:00<00:00, 79.4MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 43% 9.00M/21.2M [00:00<00:00, 32.5MB/s]\n",
            "100% 21.2M/21.2M [00:00<00:00, 53.5MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 93% 105M/112M [00:01<00:00, 89.5MB/s] \n",
            "100% 112M/112M [00:01<00:00, 95.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jLN9iKNJx2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "460f9adb-9a56-4846-881d-3901c67b5062"
      },
      "source": [
        "!unzip train.csv.zip\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVXvaxwuKQzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "outputId": "a5e5bda0-ee2b-4ad7-afa1-6fdc63b081e5"
      },
      "source": [
        "!pip install modin"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting modin\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ea/57f24305ce56c95536ebd70842b8fd12416aa1adca904dfb82371d461861/modin-0.4.0-py3-none-any.whl (184kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hCollecting pandas==0.24.1 (from modin)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/de/a0d3defd8f338eaf53ef716e40ef6d6c277c35d50e09b586e170169cdf0d/pandas-0.24.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 47.2MB/s \n",
            "\u001b[?25hCollecting ray==0.6.2 (from modin)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/57/05e2ccf236e0b05b5d2831e79486e84b4eeadce68e8927ee338c61511568/ray-0.6.2-cp36-cp36m-manylinux1_x86_64.whl (73.0MB)\n",
            "\u001b[K     |████████████████████████████████| 73.0MB 42.9MB/s \n",
            "\u001b[?25hCollecting numpy<=1.15.0 (from modin)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/29/f4c845648ed23264e986cdc5fbab5f8eace1be5e62144ef69ccc7189461d/numpy-1.15.0-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9MB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from modin) (3.6.6)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.1->modin) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.1->modin) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray==0.6.2->modin) (7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray==0.6.2->modin) (3.0.10)\n",
            "Collecting colorama (from ray==0.6.2->modin)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray==0.6.2->modin) (3.13)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.6.2->modin) (1.12.0)\n",
            "Collecting funcsigs (from ray==0.6.2->modin)\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting redis (from ray==0.6.2->modin)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/a7/cff10cc5f1180834a3ed564d148fb4329c989cbb1f2e196fc9a10fa07072/redis-3.2.1-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 24.7MB/s \n",
            "\u001b[?25hCollecting flatbuffers (from ray==0.6.2->modin)\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/84/adf5837f96c39990bc55afdfddf460b38b4562f50341359afa32e4a98de7/flatbuffers-1.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray==0.6.2->modin) (3.6.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (19.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (7.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (41.0.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray==0.6.2->modin) (1.3.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, pandas, colorama, funcsigs, redis, flatbuffers, ray, modin\n",
            "  Found existing installation: numpy 1.16.3\n",
            "    Uninstalling numpy-1.16.3:\n",
            "      Successfully uninstalled numpy-1.16.3\n",
            "  Found existing installation: pandas 0.24.2\n",
            "    Uninstalling pandas-0.24.2:\n",
            "      Successfully uninstalled pandas-0.24.2\n",
            "Successfully installed colorama-0.4.1 flatbuffers-1.11 funcsigs-1.0.2 modin-0.4.0 numpy-1.15.0 pandas-0.24.1 ray-0.6.2 redis-3.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdmNodD8J51r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "2c620fe0-535e-4f62-f3d7-1473f16f01b9"
      },
      "source": [
        "import modin.pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
            "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-03_00-48-24_452/logs.\n",
            "Waiting for redis server at 127.0.0.1:29125 to respond...\n",
            "Waiting for redis server at 127.0.0.1:14016 to respond...\n",
            "Starting Redis shard with 10.0 GB max memory.\n",
            "WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 6442450944 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n",
            "Starting the Plasma object store with 7.0 GB memory using /tmp.\n",
            "'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WQZoabnJ2O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no23o1uPKm_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns=['id', 'qid1', 'qid2'])\n",
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keuJecMwJ8ju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "79d49923-e116-4415-ec05-0513d455ef9f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  \\\n",
              "0  What is the step by step guide to invest in sh...   \n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2  How can I increase the speed of my internet co...   \n",
              "3  Why am I mentally very lonely? How can I solve...   \n",
              "4  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsPxpnkKwz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanup(col):\n",
        "    table = str.maketrans(string.punctuation,' '*32)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "    cleaned_listings = []\n",
        "    \n",
        "    for row in col:\n",
        "#         print(row)\n",
        "        # Strip punctuation everywhere,\n",
        "        # replacing it with spaces so that words separated only\n",
        "        # by punctuation don't get smooshed together\n",
        "        no_punctuation = row.translate(table)\n",
        "    #     print(\"No Punctuation:\", no_punctuation)\n",
        "    #     print('------------------------------------------')\n",
        "\n",
        "        # Tokenize by word\n",
        "        tokens = word_tokenize(no_punctuation)\n",
        "    #     print(\"Tokens:\", tokens)\n",
        "    #     print('------------------------------------------')\n",
        "\n",
        "        # Make all words lowercase\n",
        "        lowercase_tokens = [w.lower() for w in tokens]\n",
        "    #     print(\"Lowercase:\", lowercase_tokens)\n",
        "    #     print('------------------------------------------')\n",
        "\n",
        "        # Remove words that aren't alphabetic\n",
        "        alphabetic = [w for w in lowercase_tokens if w.isalpha()]\n",
        "    #     print(\"Alphabetic:\", alphabetic)\n",
        "    #     print('------------------------------------------')\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = [w for w in alphabetic if not w in stop_words]\n",
        "    #     print(\"Cleaned Words:\", words)\n",
        "    #     print(\"--------------------------------\")\n",
        "\n",
        "#         # lemmatize!\n",
        "#         lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
        "        # Append to list\n",
        "        cleaned_listings.append(words)\n",
        "        \n",
        "    return cleaned_listings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmAN7OrPK1KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q1 = df.question1.tolist()\n",
        "q2 = df.question2.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFqvoDFnLEs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6c14adce-c923-47ee-9b06-57b42e22046f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYAzqAa5K4xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q1_clean = cleanup(q1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FroeQhGrLOz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q2_clean = cleanup(q2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyhC_RSMLPkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['q1_clean'] = q1_clean\n",
        "df['q2_clean'] = q2_clean\n",
        "all_qs = q1_clean + q2_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVNmXs1MLYIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2741
        },
        "outputId": "718f89be-f92b-43d9-f44f-d0e26331ee18"
      },
      "source": [
        "\n",
        "%%time\n",
        "model1 = Word2Vec(all_qs, min_count=20, window=3, size=200)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collecting all words and their counts\n",
            "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "PROGRESS: at sentence #10000, processed 53464 words, keeping 11202 word types\n",
            "PROGRESS: at sentence #20000, processed 107171 words, keeping 16008 word types\n",
            "PROGRESS: at sentence #30000, processed 160656 words, keeping 19551 word types\n",
            "PROGRESS: at sentence #40000, processed 213584 words, keeping 22394 word types\n",
            "PROGRESS: at sentence #50000, processed 267253 words, keeping 25055 word types\n",
            "PROGRESS: at sentence #60000, processed 320680 words, keeping 27280 word types\n",
            "PROGRESS: at sentence #70000, processed 373981 words, keeping 29275 word types\n",
            "PROGRESS: at sentence #80000, processed 427468 words, keeping 31006 word types\n",
            "PROGRESS: at sentence #90000, processed 480867 words, keeping 32662 word types\n",
            "PROGRESS: at sentence #100000, processed 534302 words, keeping 34257 word types\n",
            "PROGRESS: at sentence #110000, processed 587129 words, keeping 35737 word types\n",
            "PROGRESS: at sentence #120000, processed 640381 words, keeping 37139 word types\n",
            "PROGRESS: at sentence #130000, processed 693988 words, keeping 38520 word types\n",
            "PROGRESS: at sentence #140000, processed 747481 words, keeping 39776 word types\n",
            "PROGRESS: at sentence #150000, processed 801333 words, keeping 41055 word types\n",
            "PROGRESS: at sentence #160000, processed 855121 words, keeping 42285 word types\n",
            "PROGRESS: at sentence #170000, processed 908790 words, keeping 43458 word types\n",
            "PROGRESS: at sentence #180000, processed 962552 words, keeping 44588 word types\n",
            "PROGRESS: at sentence #190000, processed 1016137 words, keeping 45653 word types\n",
            "PROGRESS: at sentence #200000, processed 1069233 words, keeping 46724 word types\n",
            "PROGRESS: at sentence #210000, processed 1122280 words, keeping 47703 word types\n",
            "PROGRESS: at sentence #220000, processed 1175531 words, keeping 48630 word types\n",
            "PROGRESS: at sentence #230000, processed 1229522 words, keeping 49596 word types\n",
            "PROGRESS: at sentence #240000, processed 1282846 words, keeping 50491 word types\n",
            "PROGRESS: at sentence #250000, processed 1336833 words, keeping 51394 word types\n",
            "PROGRESS: at sentence #260000, processed 1390763 words, keeping 52330 word types\n",
            "PROGRESS: at sentence #270000, processed 1444739 words, keeping 53185 word types\n",
            "PROGRESS: at sentence #280000, processed 1498180 words, keeping 54018 word types\n",
            "PROGRESS: at sentence #290000, processed 1551728 words, keeping 54860 word types\n",
            "PROGRESS: at sentence #300000, processed 1604880 words, keeping 55692 word types\n",
            "PROGRESS: at sentence #310000, processed 1658441 words, keeping 56496 word types\n",
            "PROGRESS: at sentence #320000, processed 1711723 words, keeping 57285 word types\n",
            "PROGRESS: at sentence #330000, processed 1765596 words, keeping 58083 word types\n",
            "PROGRESS: at sentence #340000, processed 1818932 words, keeping 58789 word types\n",
            "PROGRESS: at sentence #350000, processed 1872558 words, keeping 59450 word types\n",
            "PROGRESS: at sentence #360000, processed 1925908 words, keeping 60104 word types\n",
            "PROGRESS: at sentence #370000, processed 1979541 words, keeping 60800 word types\n",
            "PROGRESS: at sentence #380000, processed 2033432 words, keeping 61487 word types\n",
            "PROGRESS: at sentence #390000, processed 2087894 words, keeping 62220 word types\n",
            "PROGRESS: at sentence #400000, processed 2141861 words, keeping 62862 word types\n",
            "PROGRESS: at sentence #410000, processed 2195918 words, keeping 63437 word types\n",
            "PROGRESS: at sentence #420000, processed 2249760 words, keeping 63903 word types\n",
            "PROGRESS: at sentence #430000, processed 2304055 words, keeping 64406 word types\n",
            "PROGRESS: at sentence #440000, processed 2357974 words, keeping 64853 word types\n",
            "PROGRESS: at sentence #450000, processed 2412341 words, keeping 65295 word types\n",
            "PROGRESS: at sentence #460000, processed 2466461 words, keeping 65740 word types\n",
            "PROGRESS: at sentence #470000, processed 2520350 words, keeping 66191 word types\n",
            "PROGRESS: at sentence #480000, processed 2574445 words, keeping 66659 word types\n",
            "PROGRESS: at sentence #490000, processed 2628720 words, keeping 67096 word types\n",
            "PROGRESS: at sentence #500000, processed 2683000 words, keeping 67549 word types\n",
            "PROGRESS: at sentence #510000, processed 2736238 words, keeping 67976 word types\n",
            "PROGRESS: at sentence #520000, processed 2790050 words, keeping 68367 word types\n",
            "PROGRESS: at sentence #530000, processed 2844408 words, keeping 68764 word types\n",
            "PROGRESS: at sentence #540000, processed 2898833 words, keeping 69205 word types\n",
            "PROGRESS: at sentence #550000, processed 2953391 words, keeping 69647 word types\n",
            "PROGRESS: at sentence #560000, processed 3007642 words, keeping 70081 word types\n",
            "PROGRESS: at sentence #570000, processed 3062128 words, keeping 70484 word types\n",
            "PROGRESS: at sentence #580000, processed 3116547 words, keeping 70896 word types\n",
            "PROGRESS: at sentence #590000, processed 3171206 words, keeping 71294 word types\n",
            "PROGRESS: at sentence #600000, processed 3224708 words, keeping 71640 word types\n",
            "PROGRESS: at sentence #610000, processed 3278965 words, keeping 72027 word types\n",
            "PROGRESS: at sentence #620000, processed 3333023 words, keeping 72404 word types\n",
            "PROGRESS: at sentence #630000, processed 3387080 words, keeping 72845 word types\n",
            "PROGRESS: at sentence #640000, processed 3440456 words, keeping 73222 word types\n",
            "PROGRESS: at sentence #650000, processed 3494812 words, keeping 73618 word types\n",
            "PROGRESS: at sentence #660000, processed 3548828 words, keeping 74033 word types\n",
            "PROGRESS: at sentence #670000, processed 3603325 words, keeping 74405 word types\n",
            "PROGRESS: at sentence #680000, processed 3657883 words, keeping 74769 word types\n",
            "PROGRESS: at sentence #690000, processed 3711972 words, keeping 75132 word types\n",
            "PROGRESS: at sentence #700000, processed 3766030 words, keeping 75505 word types\n",
            "PROGRESS: at sentence #710000, processed 3819780 words, keeping 75866 word types\n",
            "PROGRESS: at sentence #720000, processed 3874052 words, keeping 76224 word types\n",
            "PROGRESS: at sentence #730000, processed 3927704 words, keeping 76547 word types\n",
            "PROGRESS: at sentence #740000, processed 3981848 words, keeping 76887 word types\n",
            "PROGRESS: at sentence #750000, processed 4035706 words, keeping 77245 word types\n",
            "PROGRESS: at sentence #760000, processed 4089994 words, keeping 77589 word types\n",
            "PROGRESS: at sentence #770000, processed 4144143 words, keeping 77940 word types\n",
            "PROGRESS: at sentence #780000, processed 4198334 words, keeping 78273 word types\n",
            "PROGRESS: at sentence #790000, processed 4252648 words, keeping 78637 word types\n",
            "PROGRESS: at sentence #800000, processed 4307604 words, keeping 78983 word types\n",
            "collected 79308 word types from a corpus of 4354601 raw words and 808574 sentences\n",
            "Loading a fresh vocabulary\n",
            "effective_min_count=20 retains 13899 unique words (17% of original 79308, drops 65409)\n",
            "effective_min_count=20 leaves 4128701 word corpus (94% of original 4354601, drops 225900)\n",
            "deleting the raw counts dictionary of 79308 items\n",
            "sample=0.001 downsamples 24 most-common words\n",
            "downsampling leaves estimated 3955099 word corpus (95.8% of prior 4128701)\n",
            "estimated required memory for 13899 words and 200 dimensions: 29187900 bytes\n",
            "resetting layer weights\n",
            "training model with 3 workers on 13899 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
            "EPOCH 1 - PROGRESS: at 11.10% examples, 434334 words/s, in_qsize 4, out_qsize 1\n",
            "EPOCH 1 - PROGRESS: at 22.89% examples, 443804 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 34.43% examples, 444206 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 45.99% examples, 445379 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 57.64% examples, 447538 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 69.29% examples, 448320 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 80.71% examples, 449122 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 1 - PROGRESS: at 91.91% examples, 447768 words/s, in_qsize 4, out_qsize 1\n",
            "worker thread finished; awaiting finish of 2 more threads\n",
            "worker thread finished; awaiting finish of 1 more threads\n",
            "worker thread finished; awaiting finish of 0 more threads\n",
            "EPOCH - 1 : training on 4354601 raw words (3955060 effective words) took 8.8s, 449450 effective words/s\n",
            "EPOCH 2 - PROGRESS: at 11.57% examples, 436782 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 23.11% examples, 444198 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 34.66% examples, 443973 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 46.22% examples, 445053 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 57.64% examples, 445853 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 69.06% examples, 446817 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 2 - PROGRESS: at 80.26% examples, 444764 words/s, in_qsize 6, out_qsize 2\n",
            "EPOCH 2 - PROGRESS: at 91.91% examples, 446007 words/s, in_qsize 6, out_qsize 1\n",
            "worker thread finished; awaiting finish of 2 more threads\n",
            "worker thread finished; awaiting finish of 1 more threads\n",
            "worker thread finished; awaiting finish of 0 more threads\n",
            "EPOCH - 2 : training on 4354601 raw words (3955414 effective words) took 8.8s, 448711 effective words/s\n",
            "EPOCH 3 - PROGRESS: at 11.10% examples, 432076 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 3 - PROGRESS: at 22.66% examples, 441406 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 3 - PROGRESS: at 34.43% examples, 445668 words/s, in_qsize 4, out_qsize 1\n",
            "EPOCH 3 - PROGRESS: at 46.22% examples, 447944 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 3 - PROGRESS: at 57.64% examples, 447814 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 3 - PROGRESS: at 68.38% examples, 442924 words/s, in_qsize 4, out_qsize 1\n",
            "EPOCH 3 - PROGRESS: at 79.57% examples, 442230 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 3 - PROGRESS: at 90.76% examples, 442181 words/s, in_qsize 5, out_qsize 0\n",
            "worker thread finished; awaiting finish of 2 more threads\n",
            "worker thread finished; awaiting finish of 1 more threads\n",
            "worker thread finished; awaiting finish of 0 more threads\n",
            "EPOCH - 3 : training on 4354601 raw words (3955743 effective words) took 8.9s, 445026 effective words/s\n",
            "EPOCH 4 - PROGRESS: at 11.11% examples, 433426 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 22.65% examples, 442601 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 34.19% examples, 443076 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 46.21% examples, 445865 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 57.88% examples, 448833 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 69.06% examples, 447807 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 80.94% examples, 450945 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 4 - PROGRESS: at 92.37% examples, 450471 words/s, in_qsize 5, out_qsize 0\n",
            "worker thread finished; awaiting finish of 2 more threads\n",
            "worker thread finished; awaiting finish of 1 more threads\n",
            "worker thread finished; awaiting finish of 0 more threads\n",
            "EPOCH - 4 : training on 4354601 raw words (3954996 effective words) took 8.7s, 452997 effective words/s\n",
            "EPOCH 5 - PROGRESS: at 11.34% examples, 442507 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 23.11% examples, 448712 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 34.66% examples, 447606 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 46.45% examples, 449412 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 58.33% examples, 450301 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 69.97% examples, 450300 words/s, in_qsize 5, out_qsize 0\n",
            "EPOCH 5 - PROGRESS: at 81.40% examples, 450987 words/s, in_qsize 6, out_qsize 2\n",
            "EPOCH 5 - PROGRESS: at 92.82% examples, 451428 words/s, in_qsize 5, out_qsize 0\n",
            "worker thread finished; awaiting finish of 2 more threads\n",
            "worker thread finished; awaiting finish of 1 more threads\n",
            "worker thread finished; awaiting finish of 0 more threads\n",
            "EPOCH - 5 : training on 4354601 raw words (3954655 effective words) took 8.7s, 453214 effective words/s\n",
            "training on a 21773005 raw words (19775868 effective words) took 44.0s, 449159 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 26s, sys: 409 ms, total: 1min 26s\n",
            "Wall time: 46.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYWc39rCLbKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17628
        },
        "outputId": "ef4f56dd-648b-4d64-977c-95c0b1678cb0"
      },
      "source": [
        "words = list(model1.wv.vocab)\n",
        "print(len(words))\n",
        "words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['step',\n",
              " 'guide',\n",
              " 'invest',\n",
              " 'share',\n",
              " 'market',\n",
              " 'india',\n",
              " 'story',\n",
              " 'kohinoor',\n",
              " 'koh',\n",
              " 'diamond',\n",
              " 'increase',\n",
              " 'speed',\n",
              " 'internet',\n",
              " 'connection',\n",
              " 'using',\n",
              " 'vpn',\n",
              " 'mentally',\n",
              " 'lonely',\n",
              " 'solve',\n",
              " 'one',\n",
              " 'dissolve',\n",
              " 'water',\n",
              " 'sugar',\n",
              " 'salt',\n",
              " 'methane',\n",
              " 'carbon',\n",
              " 'di',\n",
              " 'oxide',\n",
              " 'astrology',\n",
              " 'capricorn',\n",
              " 'sun',\n",
              " 'cap',\n",
              " 'moon',\n",
              " 'rising',\n",
              " 'say',\n",
              " 'buy',\n",
              " 'good',\n",
              " 'use',\n",
              " 'instead',\n",
              " 'motorola',\n",
              " 'company',\n",
              " 'hack',\n",
              " 'charter',\n",
              " 'method',\n",
              " 'find',\n",
              " 'separation',\n",
              " 'read',\n",
              " 'youtube',\n",
              " 'comments',\n",
              " 'make',\n",
              " 'physics',\n",
              " 'easy',\n",
              " 'learn',\n",
              " 'first',\n",
              " 'sexual',\n",
              " 'experience',\n",
              " 'like',\n",
              " 'laws',\n",
              " 'change',\n",
              " 'status',\n",
              " 'student',\n",
              " 'visa',\n",
              " 'green',\n",
              " 'card',\n",
              " 'us',\n",
              " 'compare',\n",
              " 'immigration',\n",
              " 'canada',\n",
              " 'would',\n",
              " 'trump',\n",
              " 'presidency',\n",
              " 'mean',\n",
              " 'current',\n",
              " 'international',\n",
              " 'master',\n",
              " 'students',\n",
              " 'manipulation',\n",
              " 'girls',\n",
              " 'want',\n",
              " 'friends',\n",
              " 'guy',\n",
              " 'reject',\n",
              " 'many',\n",
              " 'quora',\n",
              " 'users',\n",
              " 'posting',\n",
              " 'questions',\n",
              " 'readily',\n",
              " 'answered',\n",
              " 'google',\n",
              " 'best',\n",
              " 'digital',\n",
              " 'marketing',\n",
              " 'institution',\n",
              " 'banglore',\n",
              " 'rockets',\n",
              " 'look',\n",
              " 'white',\n",
              " 'causing',\n",
              " 'someone',\n",
              " 'jealous',\n",
              " 'ask',\n",
              " 'much',\n",
              " 'hp',\n",
              " 'every',\n",
              " 'time',\n",
              " 'clock',\n",
              " 'numbers',\n",
              " 'tips',\n",
              " 'making',\n",
              " 'job',\n",
              " 'interview',\n",
              " 'process',\n",
              " 'medicines',\n",
              " 'web',\n",
              " 'application',\n",
              " 'society',\n",
              " 'place',\n",
              " 'importance',\n",
              " 'sports',\n",
              " 'way',\n",
              " 'money',\n",
              " 'online',\n",
              " 'prepare',\n",
              " 'ca',\n",
              " 'final',\n",
              " 'law',\n",
              " 'thing',\n",
              " 'better',\n",
              " 'special',\n",
              " 'cares',\n",
              " 'nose',\n",
              " 'gets',\n",
              " 'stuffy',\n",
              " 'night',\n",
              " 'game',\n",
              " 'thrones',\n",
              " 'villain',\n",
              " 'likely',\n",
              " 'give',\n",
              " 'mercy',\n",
              " 'united',\n",
              " 'states',\n",
              " 'government',\n",
              " 'still',\n",
              " 'employment',\n",
              " 'etc',\n",
              " 'citizens',\n",
              " 'political',\n",
              " 'views',\n",
              " 'travel',\n",
              " 'website',\n",
              " 'spain',\n",
              " 'people',\n",
              " 'think',\n",
              " 'obama',\n",
              " 'try',\n",
              " 'take',\n",
              " 'guns',\n",
              " 'away',\n",
              " 'year',\n",
              " 'old',\n",
              " 'improve',\n",
              " 'skills',\n",
              " 'become',\n",
              " 'entrepreneur',\n",
              " 'next',\n",
              " 'years',\n",
              " 'girlfriend',\n",
              " 'asks',\n",
              " 'boyfriend',\n",
              " 'choose',\n",
              " 'makes',\n",
              " 'reply',\n",
              " 'upsc',\n",
              " 'stall',\n",
              " 'f',\n",
              " 'wings',\n",
              " 'fully',\n",
              " 'back',\n",
              " 'squat',\n",
              " 'expect',\n",
              " 'cognizant',\n",
              " 'confirmation',\n",
              " 'mail',\n",
              " 'month',\n",
              " 'day',\n",
              " 'trading',\n",
              " 'kid',\n",
              " 'rebel',\n",
              " 'worth',\n",
              " 'long',\n",
              " 'run',\n",
              " 'universities',\n",
              " 'recruit',\n",
              " 'new',\n",
              " 'grads',\n",
              " 'majors',\n",
              " 'looking',\n",
              " 'quickest',\n",
              " 'instagram',\n",
              " 'followers',\n",
              " 'darth',\n",
              " 'vader',\n",
              " 'fought',\n",
              " 'star',\n",
              " 'wars',\n",
              " 'legends',\n",
              " 'stages',\n",
              " 'breaking',\n",
              " 'couple',\n",
              " 'happens',\n",
              " 'emotionally',\n",
              " 'whether',\n",
              " 'male',\n",
              " 'female',\n",
              " 'examples',\n",
              " 'products',\n",
              " 'crude',\n",
              " 'oil',\n",
              " 'career',\n",
              " 'launcher',\n",
              " 'rbi',\n",
              " 'grade',\n",
              " 'b',\n",
              " 'preparation',\n",
              " 'blu',\n",
              " 'ray',\n",
              " 'play',\n",
              " 'regular',\n",
              " 'dvd',\n",
              " 'player',\n",
              " 'nd',\n",
              " 'always',\n",
              " 'sad',\n",
              " 'memorable',\n",
              " 'ever',\n",
              " 'eaten',\n",
              " 'gst',\n",
              " 'affects',\n",
              " 'tax',\n",
              " 'officers',\n",
              " 'difficult',\n",
              " 'get',\n",
              " 'friend',\n",
              " 'rap',\n",
              " 'songs',\n",
              " 'dance',\n",
              " 'suddenly',\n",
              " 'logged',\n",
              " 'gmail',\n",
              " 'remember',\n",
              " 'password',\n",
              " 'realized',\n",
              " 'recovery',\n",
              " 'email',\n",
              " 'longer',\n",
              " 'alive',\n",
              " 'ways',\n",
              " 'french',\n",
              " 'download',\n",
              " 'content',\n",
              " 'kickass',\n",
              " 'torrent',\n",
              " 'without',\n",
              " 'registration',\n",
              " 'normal',\n",
              " 'dark',\n",
              " 'ring',\n",
              " 'around',\n",
              " 'iris',\n",
              " 'eye',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'book',\n",
              " 'cursed',\n",
              " 'child',\n",
              " 'depressed',\n",
              " 'european',\n",
              " 'family',\n",
              " 'office',\n",
              " 'database',\n",
              " 'java',\n",
              " 'programming',\n",
              " 'language',\n",
              " 'made',\n",
              " 'store',\n",
              " 'energy',\n",
              " 'produced',\n",
              " 'lightning',\n",
              " 'review',\n",
              " 'performance',\n",
              " 'testing',\n",
              " 'cost',\n",
              " 'privacy',\n",
              " 'germany',\n",
              " 'come',\n",
              " 'else',\n",
              " 'lost',\n",
              " 'gain',\n",
              " 'types',\n",
              " 'immunity',\n",
              " 'narcissistic',\n",
              " 'personality',\n",
              " 'disorder',\n",
              " 'speak',\n",
              " 'english',\n",
              " 'fluently',\n",
              " 'helpful',\n",
              " 'quickbooks',\n",
              " 'auto',\n",
              " 'data',\n",
              " 'support',\n",
              " 'phone',\n",
              " 'number',\n",
              " 'recover',\n",
              " 'corrupted',\n",
              " 'files',\n",
              " 'richest',\n",
              " 'gambler',\n",
              " 'reach',\n",
              " 'level',\n",
              " 'fire',\n",
              " 'bullet',\n",
              " 'backward',\n",
              " 'aircraft',\n",
              " 'going',\n",
              " 'faster',\n",
              " 'backwards',\n",
              " 'prevent',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'log',\n",
              " 'account',\n",
              " 'purpose',\n",
              " 'life',\n",
              " 'bjp',\n",
              " 'strip',\n",
              " 'muslims',\n",
              " 'christians',\n",
              " 'indian',\n",
              " 'citizenship',\n",
              " 'put',\n",
              " 'boats',\n",
              " 'burma',\n",
              " 'right',\n",
              " 'etiquette',\n",
              " 'wishing',\n",
              " 'jehovah',\n",
              " 'witness',\n",
              " 'happy',\n",
              " 'birthday',\n",
              " 'wants',\n",
              " 'open',\n",
              " 'commercial',\n",
              " 'fm',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'city',\n",
              " 'procedure',\n",
              " 'swiss',\n",
              " 'despise',\n",
              " 'asians',\n",
              " 'high',\n",
              " 'salary',\n",
              " 'income',\n",
              " 'jobs',\n",
              " 'field',\n",
              " 'biotechnology',\n",
              " 'height',\n",
              " 'also',\n",
              " 'major',\n",
              " 'effects',\n",
              " 'cambodia',\n",
              " 'earthquake',\n",
              " 'kamchatca',\n",
              " 'earthquakes',\n",
              " 'difference',\n",
              " 'fairness',\n",
              " 'gaming',\n",
              " 'laptop',\n",
              " 'inr',\n",
              " 'warrior',\n",
              " 'proving',\n",
              " 'grounds',\n",
              " 'part',\n",
              " 'reference',\n",
              " 'class',\n",
              " 'national',\n",
              " 'institute',\n",
              " 'technology',\n",
              " 'kurukshetra',\n",
              " 'social',\n",
              " 'nitk',\n",
              " 'surathkal',\n",
              " 'romantic',\n",
              " 'movies',\n",
              " 'causes',\n",
              " 'nightmare',\n",
              " 'abstract',\n",
              " 'painting',\n",
              " 'printing',\n",
              " 'work',\n",
              " 'attend',\n",
              " 'caltech',\n",
              " 'jeremy',\n",
              " 'horcrux',\n",
              " 'associate',\n",
              " 'product',\n",
              " 'manager',\n",
              " 'programs',\n",
              " 'early',\n",
              " 'join',\n",
              " 'management',\n",
              " 'rewarding',\n",
              " 'skype',\n",
              " 'busy',\n",
              " 'really',\n",
              " 'war',\n",
              " 'pakistan',\n",
              " 'uri',\n",
              " 'attack',\n",
              " 'ronald',\n",
              " 'reagan',\n",
              " 'speech',\n",
              " 'strategies',\n",
              " 'union',\n",
              " 'civil',\n",
              " 'fiction',\n",
              " 'novel',\n",
              " 'forgot',\n",
              " 'recent',\n",
              " 'demonetisation',\n",
              " 'results',\n",
              " 'higher',\n",
              " 'gdp',\n",
              " 'heard',\n",
              " 'hacking',\n",
              " 'love',\n",
              " 'competitive',\n",
              " 'hiring',\n",
              " 'republic',\n",
              " 'bank',\n",
              " 'helps',\n",
              " 'spam',\n",
              " 'ranking',\n",
              " 'search',\n",
              " 'watch',\n",
              " 'subtitles',\n",
              " 'usa',\n",
              " 'powerful',\n",
              " 'country',\n",
              " 'world',\n",
              " 'obtain',\n",
              " 'instant',\n",
              " 'ulcer',\n",
              " 'pain',\n",
              " 'relief',\n",
              " 'china',\n",
              " 'food',\n",
              " 'taking',\n",
              " 'advantage',\n",
              " 'cry',\n",
              " 'stick',\n",
              " 'tongues',\n",
              " 'pictures',\n",
              " 'ending',\n",
              " 'depressing',\n",
              " 'mind',\n",
              " 'blowing',\n",
              " 'computer',\n",
              " 'tools',\n",
              " 'exist',\n",
              " 'know',\n",
              " 'toothbrush',\n",
              " 'wet',\n",
              " 'dry',\n",
              " 'applying',\n",
              " 'toothpaste',\n",
              " 'question',\n",
              " 'marked',\n",
              " 'needing',\n",
              " 'neutral',\n",
              " 'state',\n",
              " 'buffer',\n",
              " 'mineral',\n",
              " 'holds',\n",
              " 'highest',\n",
              " 'electrical',\n",
              " 'charge',\n",
              " 'greatest',\n",
              " 'mystery',\n",
              " 'universe',\n",
              " 'alternative',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'block',\n",
              " 'sanctions',\n",
              " 'un',\n",
              " 'e',\n",
              " 'mohammad',\n",
              " 'chief',\n",
              " 'masood',\n",
              " 'azhar',\n",
              " 'future',\n",
              " 'budget',\n",
              " 'excessive',\n",
              " 'amounts',\n",
              " 'vitamin',\n",
              " 'c',\n",
              " 'cause',\n",
              " 'miscarriage',\n",
              " 'glass',\n",
              " 'pay',\n",
              " 'scale',\n",
              " 'two',\n",
              " 'access',\n",
              " 'six',\n",
              " 'party',\n",
              " 'talks',\n",
              " 'successful',\n",
              " 'register',\n",
              " 'domain',\n",
              " 'site',\n",
              " 'older',\n",
              " 'men',\n",
              " 'attracted',\n",
              " 'young',\n",
              " 'women',\n",
              " 'strongest',\n",
              " 'structure',\n",
              " 'shape',\n",
              " 'compression',\n",
              " 'matter',\n",
              " 'humans',\n",
              " 'selfish',\n",
              " 'evil',\n",
              " 'active',\n",
              " 'passive',\n",
              " 'transport',\n",
              " 'hair',\n",
              " 'bald',\n",
              " 'head',\n",
              " 'ideal',\n",
              " 'retirement',\n",
              " 'stance',\n",
              " 'websites',\n",
              " 'escorts',\n",
              " 'polo',\n",
              " 'diesel',\n",
              " 'grand',\n",
              " 'petrol',\n",
              " 'black',\n",
              " 'hole',\n",
              " 'mass',\n",
              " 'morgan',\n",
              " 'correct',\n",
              " 'says',\n",
              " 'stop',\n",
              " 'racism',\n",
              " 'talking',\n",
              " 'currently',\n",
              " 'offer',\n",
              " 'employees',\n",
              " 'stock',\n",
              " 'options',\n",
              " 'rsus',\n",
              " 'distribute',\n",
              " 'identical',\n",
              " 'pencils',\n",
              " 'least',\n",
              " 'pencil',\n",
              " 'hire',\n",
              " 'jerry',\n",
              " 'seinfeld',\n",
              " 'hours',\n",
              " 'days',\n",
              " 'late',\n",
              " 'rabies',\n",
              " 'vaccine',\n",
              " 'possible',\n",
              " 'non',\n",
              " 'bite',\n",
              " 'exposure',\n",
              " 'britain',\n",
              " 'ruled',\n",
              " 'afraid',\n",
              " 'working',\n",
              " 'red',\n",
              " 'keep',\n",
              " 'keys',\n",
              " 'age',\n",
              " 'lose',\n",
              " 'virginity',\n",
              " 'could',\n",
              " 'secretly',\n",
              " 'monetize',\n",
              " 'videos',\n",
              " 'upload',\n",
              " 'copyright',\n",
              " 'chances',\n",
              " 'may',\n",
              " 'switch',\n",
              " 'canon',\n",
              " 'members',\n",
              " 'moderation',\n",
              " 'nobody',\n",
              " 'answer',\n",
              " 'funniest',\n",
              " 'joke',\n",
              " 'got',\n",
              " 'london',\n",
              " 'pm',\n",
              " 'deduction',\n",
              " 'pls',\n",
              " 'advice',\n",
              " 'monthly',\n",
              " 'expenses',\n",
              " 'saving',\n",
              " 'stereotypes',\n",
              " 'kingdom',\n",
              " 'twitter',\n",
              " 'business',\n",
              " 'source',\n",
              " 'transfer',\n",
              " 'paypal',\n",
              " 'earphone',\n",
              " 'deep',\n",
              " 'bass',\n",
              " 'eligible',\n",
              " 'yojana',\n",
              " 'forward',\n",
              " 'engineering',\n",
              " 'fields',\n",
              " 'suit',\n",
              " 'abusing',\n",
              " 'gross',\n",
              " 'ctc',\n",
              " 'towns',\n",
              " 'kerala',\n",
              " 'confidence',\n",
              " 'anyone',\n",
              " 'see',\n",
              " 'relation',\n",
              " 'greek',\n",
              " 'gods',\n",
              " 'hindu',\n",
              " 'apple',\n",
              " 'music',\n",
              " 'spotify',\n",
              " 'fixed',\n",
              " 'fund',\n",
              " 'live',\n",
              " 'cologne',\n",
              " 'robert',\n",
              " 'de',\n",
              " 'al',\n",
              " 'prefer',\n",
              " 'small',\n",
              " 'families',\n",
              " 'animals',\n",
              " 'kiss',\n",
              " 'deleted',\n",
              " 'chats',\n",
              " 'addicted',\n",
              " 'hired',\n",
              " 'private',\n",
              " 'eyes',\n",
              " 'ordered',\n",
              " 'follow',\n",
              " 'startup',\n",
              " 'accelerator',\n",
              " 'check',\n",
              " 'wifi',\n",
              " 'history',\n",
              " 'android',\n",
              " 'phones',\n",
              " 'significance',\n",
              " 'battle',\n",
              " 'somme',\n",
              " 'contrast',\n",
              " 'rostov',\n",
              " 'creative',\n",
              " 'college',\n",
              " 'admissions',\n",
              " 'essay',\n",
              " 'happen',\n",
              " 'cover',\n",
              " 'patch',\n",
              " 'pursue',\n",
              " 'different',\n",
              " 'things',\n",
              " 'ben',\n",
              " 'affleck',\n",
              " 'shine',\n",
              " 'christian',\n",
              " 'bale',\n",
              " 'batman',\n",
              " 'start',\n",
              " 'hyderabad',\n",
              " 'possessive',\n",
              " 'wife',\n",
              " 'lord',\n",
              " 'krishna',\n",
              " 'combination',\n",
              " 'courses',\n",
              " 'along',\n",
              " 'enhance',\n",
              " 'screenshot',\n",
              " 'mac',\n",
              " 'executive',\n",
              " 'recruiter',\n",
              " 'psychological',\n",
              " 'need',\n",
              " 'collecting',\n",
              " 'fulfill',\n",
              " 'must',\n",
              " 'tv',\n",
              " 'shows',\n",
              " 'die',\n",
              " 'fluent',\n",
              " 'chinese',\n",
              " 'demonitization',\n",
              " 'rupees',\n",
              " 'notes',\n",
              " 'real',\n",
              " 'estate',\n",
              " 'sector',\n",
              " 'easiest',\n",
              " 'billionaire',\n",
              " 'willing',\n",
              " 'free',\n",
              " 'lectures',\n",
              " 'conduct',\n",
              " 'workshop',\n",
              " 'colleges',\n",
              " 'robotics',\n",
              " 'hate',\n",
              " 'hillary',\n",
              " 'clinton',\n",
              " 'periods',\n",
              " 'regarded',\n",
              " 'prevented',\n",
              " 'rituals',\n",
              " 'hinduism',\n",
              " 'atheism',\n",
              " 'lack',\n",
              " 'belief',\n",
              " 'term',\n",
              " 'god',\n",
              " 'jack',\n",
              " 'quotes',\n",
              " 'lessons',\n",
              " 'assassin',\n",
              " 'creed',\n",
              " 'series',\n",
              " 'printers',\n",
              " 'color',\n",
              " 'ink',\n",
              " 'documents',\n",
              " 'creativity',\n",
              " 'important',\n",
              " 'continue',\n",
              " 'presidential',\n",
              " 'campaign',\n",
              " 'democratic',\n",
              " 'candidate',\n",
              " 'headphones',\n",
              " 'antenna',\n",
              " 'channels',\n",
              " 'mobile',\n",
              " 'companies',\n",
              " 'install',\n",
              " 'inbuilt',\n",
              " 'able',\n",
              " 'growth',\n",
              " 'technologies',\n",
              " 'automation',\n",
              " 'engineers',\n",
              " 'apart',\n",
              " 'disadvantages',\n",
              " 'listing',\n",
              " 'nse',\n",
              " 'object',\n",
              " 'position',\n",
              " 'respect',\n",
              " 'continuous',\n",
              " 'neutron',\n",
              " 'gpa',\n",
              " 'enough',\n",
              " 'top',\n",
              " 'harvard',\n",
              " 'profitable',\n",
              " 'trade',\n",
              " 'binary',\n",
              " 'utilize',\n",
              " 'avoid',\n",
              " 'depression',\n",
              " 'security',\n",
              " 'earn',\n",
              " 'puk',\n",
              " 'code',\n",
              " 'japan',\n",
              " 'sentences',\n",
              " 'word',\n",
              " 'another',\n",
              " 'billion',\n",
              " 'dollar',\n",
              " 'lottery',\n",
              " 'scary',\n",
              " 'drive',\n",
              " 'road',\n",
              " 'hana',\n",
              " 'given',\n",
              " 'turns',\n",
              " 'create',\n",
              " 'shell',\n",
              " 'terminal',\n",
              " 'linux',\n",
              " 'cognition',\n",
              " 'affect',\n",
              " 'perception',\n",
              " 'tqwl',\n",
              " 'ckwl',\n",
              " 'tatkal',\n",
              " 'waiting',\n",
              " 'list',\n",
              " 'generations',\n",
              " 'champions',\n",
              " 'league',\n",
              " 'drinking',\n",
              " 'liters',\n",
              " 'unhealthy',\n",
              " 'great',\n",
              " 'lakes',\n",
              " 'wildlife',\n",
              " 'lake',\n",
              " 'porn',\n",
              " 'stars',\n",
              " 'supporters',\n",
              " 'feel',\n",
              " 'walking',\n",
              " 'promises',\n",
              " 'worried',\n",
              " 'others',\n",
              " 'opinions',\n",
              " 'potty',\n",
              " 'train',\n",
              " 'months',\n",
              " 'pitbull',\n",
              " 'jump',\n",
              " 'rope',\n",
              " 'five',\n",
              " 'minutes',\n",
              " 'calories',\n",
              " 'test',\n",
              " 'gate',\n",
              " 'cs',\n",
              " 'stream',\n",
              " 'material',\n",
              " 'understanding',\n",
              " 'algorithmic',\n",
              " 'analysis',\n",
              " 'newbie',\n",
              " 'temperament',\n",
              " 'husky',\n",
              " 'mix',\n",
              " 'balls',\n",
              " 'weigh',\n",
              " 'weight',\n",
              " 'heavier',\n",
              " 'lighter',\n",
              " 'odd',\n",
              " 'ball',\n",
              " 'weighs',\n",
              " 'walter',\n",
              " 'ad',\n",
              " 'argue',\n",
              " 'signs',\n",
              " 'ultra',\n",
              " 'smart',\n",
              " 'person',\n",
              " 'playing',\n",
              " 'dumb',\n",
              " 'created',\n",
              " 'expansion',\n",
              " 'infinite',\n",
              " 'published',\n",
              " 'bloomberg',\n",
              " 'flash',\n",
              " 'fast',\n",
              " 'legally',\n",
              " 'tangible',\n",
              " 'profits',\n",
              " 'relatively',\n",
              " 'short',\n",
              " 'period',\n",
              " 'implementation',\n",
              " 'bill',\n",
              " 'impact',\n",
              " 'lives',\n",
              " 'common',\n",
              " 'accurately',\n",
              " 'mental',\n",
              " 'illness',\n",
              " 'diagnosed',\n",
              " 'knee',\n",
              " 'girl',\n",
              " 'qualities',\n",
              " 'leader',\n",
              " 'bay',\n",
              " 'gulf',\n",
              " 'modi',\n",
              " 'win',\n",
              " 'franchise',\n",
              " 'medicine',\n",
              " 'swami',\n",
              " 'vivekananda',\n",
              " 'eat',\n",
              " 'veg',\n",
              " 'egg',\n",
              " 'journey',\n",
              " 'cancel',\n",
              " 'tickets',\n",
              " 'charting',\n",
              " 'done',\n",
              " 'exactly',\n",
              " 'core',\n",
              " 'initiative',\n",
              " 'standards',\n",
              " 'pros',\n",
              " 'cons',\n",
              " 'journal',\n",
              " 'publish',\n",
              " 'paper',\n",
              " 'resolutions',\n",
              " 'bad',\n",
              " 'dream',\n",
              " 'marks',\n",
              " 'score',\n",
              " 'aiims',\n",
              " 'rank',\n",
              " 'operating',\n",
              " 'room',\n",
              " 'surgeons',\n",
              " 'equal',\n",
              " 'disagree',\n",
              " 'surgical',\n",
              " 'tie',\n",
              " 'broken',\n",
              " 'knowledge',\n",
              " 'developing',\n",
              " 'apps',\n",
              " 'scratch',\n",
              " 'body',\n",
              " 'rights',\n",
              " 'prisoner',\n",
              " 'gems',\n",
              " 'clash',\n",
              " 'clans',\n",
              " 'lens',\n",
              " 'ex',\n",
              " 'suffering',\n",
              " 'urge',\n",
              " 'visiting',\n",
              " 'placements',\n",
              " 'idea',\n",
              " 'traffic',\n",
              " 'upvotes',\n",
              " 'films',\n",
              " 'feature',\n",
              " 'strong',\n",
              " 'far',\n",
              " 'go',\n",
              " 'wait',\n",
              " 'ir',\n",
              " 'move',\n",
              " 'clothes',\n",
              " 'polyester',\n",
              " 'cotton',\n",
              " 'bone',\n",
              " 'slave',\n",
              " 'name',\n",
              " 'dead',\n",
              " 'float',\n",
              " 'surface',\n",
              " 'drowning',\n",
              " 'talked',\n",
              " 'miss',\n",
              " 'travelling',\n",
              " 'region',\n",
              " 'cities',\n",
              " 'quality',\n",
              " 'customized',\n",
              " 'cupcakes',\n",
              " 'gold',\n",
              " 'coast',\n",
              " 'even',\n",
              " 'plan',\n",
              " 'deal',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G82iS_AOLfC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f4b7ebc-ff85-4396-e496-c2088a365111"
      },
      "source": [
        "model1.wv.similarity('cotton','addicted')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.00021207887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMes1VPpLfzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "127c55b5-5521-4e8c-8a31-a9e0e62e02d6"
      },
      "source": [
        "model1.wv.doesnt_match(['cotton','rabbit','shirt','polyester'])\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rabbit'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRSX0clvL45t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "9aa758e7-e4ba-4cf8-d68d-70e8334fb9df"
      },
      "source": [
        "model1.wv.most_similar('cotton', topn=10)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('polyester', 0.8174766302108765),\n",
              " ('wool', 0.7866145372390747),\n",
              " ('denim', 0.7802823781967163),\n",
              " ('fabric', 0.7706536054611206),\n",
              " ('gin', 0.7656160593032837),\n",
              " ('stain', 0.755588948726654),\n",
              " ('trousers', 0.7486355900764465),\n",
              " ('jacket', 0.7470711469650269),\n",
              " ('coat', 0.7406226992607117),\n",
              " ('pepper', 0.7401096820831299)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkzdOZm38yQ_",
        "colab_type": "text"
      },
      "source": [
        "### Stretch Goals:\n",
        "\n",
        "1) Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents.\n",
        "\n",
        "2) Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "Load the pre-trained word vectors and train the Word2vec model\n",
        "\n",
        "Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "For example:\n",
        "\n",
        "model.similarity('house', 'bungalow')\n",
        "\n",
        "model.similarity('house', 'umbrella')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gakr5rP76IAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}