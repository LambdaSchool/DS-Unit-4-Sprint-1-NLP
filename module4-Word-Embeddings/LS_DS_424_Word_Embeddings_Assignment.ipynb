{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASfGeMfI6Kgs"
   },
   "source": [
    "### Use Word2Vec to train your own model on a dataset.\n",
    "\n",
    "1) **Optional** - Find your own dataset of documents to train you model on. You are going to need a lot of data, so it's probably not realistic to scrape data for this assignment given the time constraints that we're working under. Try to find a dataset that has > 5000 documents.\n",
    "\n",
    "- If you can't find a dataset to use try this one: <https://www.kaggle.com/c/quora-question-pairs>\n",
    "\n",
    "2) Clean/Tokenize the documents.\n",
    "\n",
    "3) Vectorize the model using Word2Vec and explore the results using each of the following at least one time:\n",
    "\n",
    "- your_model.wv.most_similar()\n",
    "- your_model.wv.similarity()\n",
    "- your_model.wv.doesn't_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:35:27.055812Z",
     "start_time": "2019-03-28T23:35:06.065927Z"
    },
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wy5lYo4K8wEy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 1.1MB/s ta 0:00:011    88% |████████████████████████████▏   | 21.3MB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from gensim) (1.16.1)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 4.0MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from gensim) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/3d/03c8643f74c9a97d6badc39b890788aa8dbd51ee5d4029677a0dba67a94a/boto3-1.9.123-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/mark/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/mark/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Collecting botocore<1.13.0,>=1.12.123 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/c2/dbcb4520c1c890d69c5d9034998318f8ee69d8af8dea5c0a51a61c9994e9/botocore-1.12.123-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils>=0.10 in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/mark/anaconda3/envs/myf/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/mark/.cache/pip/wheels/f7/a6/ff/9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/mark/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "\u001b[31mawscli 1.16.125 has requirement botocore==1.12.115, but you'll have botocore 1.12.123 which is incompatible.\u001b[0m\n",
      "Installing collected packages: bz2file, botocore, boto3, smart-open, gensim\n",
      "  Found existing installation: botocore 1.12.115\n",
      "    Uninstalling botocore-1.12.115:\n",
      "      Successfully uninstalled botocore-1.12.115\n",
      "Successfully installed boto3-1.9.123 botocore-1.12.123 bz2file-0.98 gensim-3.7.1 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf 20_newsgroups.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:32.122822Z",
     "start_time": "2019-03-29T01:37:32.111334Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "ndir = '/home/mark/lambda/DS-Unit-4-Sprint-2-NLP/module4-Word-Embeddings/20_newsgroups/'\n",
    "# files = [f for f in listdir(ndir) if isfile(join(ndir, f))]\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:32.938443Z",
     "start_time": "2019-03-29T01:37:32.837138Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [val for sublist in [[os.path.join(i[0], j) for j in i[2]] for i in os.walk(ndir)] for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:33.625286Z",
     "start_time": "2019-03-29T01:37:33.614555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:38.365876Z",
     "start_time": "2019-03-29T01:37:34.210016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n"
     ]
    }
   ],
   "source": [
    "da = []\n",
    "for f in files:\n",
    "    m = re.search('((?<=/)([^/])+(?=/\\d+$))',f)\n",
    "#     print(m.groups()[0],f)\n",
    "    with open(f,'r',encoding=\"ISO-8859-1\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        L = 0\n",
    "        for i in range(10,len(lines)):\n",
    "            if (len(lines[i]) < 6) or (\"From article\" in lines[i]) or (\"Lines:\" in lines[i]):\n",
    "#                 print(i,lines[i])\n",
    "                continue\n",
    "            else:\n",
    "                L = i\n",
    "                break\n",
    "#         print('L',L)        \n",
    "        da.append({'text': lines[L:], 'ngName': m.groups()[0]})\n",
    "print(len(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:38.427416Z",
     "start_time": "2019-03-29T01:37:38.372285Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:38.445480Z",
     "start_time": "2019-03-29T01:37:38.430943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngName</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[&gt; erickson@azure.nmt.edu (Alan Erickson) writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[HI, I was wondering if anyone would be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[In article &lt;1993Apr23.105152.20155@news.cs.tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[&gt;Dean Anneser (anneser@pwa-b.uucp) wrote:\\n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[oops, that's KNX 1070. KNBR is up in 'Frisco,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ngName                                               text\n",
       "0  sci.electronics  [> erickson@azure.nmt.edu (Alan Erickson) writ...\n",
       "1  sci.electronics  [HI, I was wondering if anyone would be able t...\n",
       "2  sci.electronics  [In article <1993Apr23.105152.20155@news.cs.tu...\n",
       "3  sci.electronics  [>Dean Anneser (anneser@pwa-b.uucp) wrote:\\n, ...\n",
       "4  sci.electronics  [oops, that's KNX 1070. KNBR is up in 'Frisco,..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:37:38.457256Z",
     "start_time": "2019-03-29T01:37:38.448809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sci.electronics', 'comp.graphics', 'soc.religion.christian',\n",
       "       'misc.forsale', 'rec.sport.baseball', 'alt.atheism', 'rec.autos',\n",
       "       'talk.politics.mideast', 'sci.crypt', 'sci.med',\n",
       "       'comp.os.ms-windows.misc', 'comp.sys.mac.hardware',\n",
       "       'rec.motorcycles', 'talk.religion.misc', 'talk.politics.guns',\n",
       "       'comp.sys.ibm.pc.hardware', 'comp.windows.x', 'rec.sport.hockey',\n",
       "       'talk.politics.misc', 'sci.space'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ngName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:04:00.178796Z",
     "start_time": "2019-03-29T02:04:00.174273Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:54:10.593754Z",
     "start_time": "2019-03-29T01:53:06.699283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t | ['ericksonazurenmtedu', 'alan', 'erickson', 'writes'] | 4\n",
      "t | ['im', 'trying', 'to', 'bring', 'in', '8', 'bits', 'to', 'a', 'pc', 'and', 'would', 'like'] | 13\n",
      "t | ['to', 'use', 'interruptdriven', 'routines', 'without', 'buying', 'an', 'io'] | 8\n",
      "t | ['board', 'or', 'making', 'a', 'new', 'port', 'where', 'can', 'i', 'bring', 'in', 'these'] | 12\n",
      "t | ['bits', 'lpt', 'seems', 'to', 'have', 'only', 'a', 'few', 'inputs', 'but', 'ive', 'heard'] | 12\n",
      "t | ['rumours', 'that', 'some', 'lpts', 'have', 'bidirectional', 'lines', 'anybody'] | 8\n",
      "t | ['know', 'fer', 'sure', 'if', 'any', 'bid', 'lpts', 'which', 'boards', 'have', 'them'] | 11\n",
      "t | ['ill', 'be', 'running', 'a', 'new', '386dx33'] | 6\n",
      "t | ['yes', 'it', 'is', 'possible', 'im', 'making', 'a', '7', 'stepper', 'controller', 'board'] | 11\n",
      "t | ['with', '7', 'digital', 'inputs', 'and', 'up', 'to', '18', 'digital', 'outputs', 'from', 'the', 'port'] | 13\n",
      "[['ericksonazurenmtedu', 'alan', 'erickson', 'writes'], ['im', 'trying', 'to', 'bring', 'in', '8', 'bits', 'to', 'a', 'pc', 'and', 'would', 'like'], ['to', 'use', 'interruptdriven', 'routines', 'without', 'buying', 'an', 'io'], ['board', 'or', 'making', 'a', 'new', 'port', 'where', 'can', 'i', 'bring', 'in', 'these'], ['bits', 'lpt', 'seems', 'to', 'have', 'only', 'a', 'few', 'inputs', 'but', 'ive', 'heard'], ['rumours', 'that', 'some', 'lpts', 'have', 'bidirectional', 'lines', 'anybody'], ['know', 'fer', 'sure', 'if', 'any', 'bid', 'lpts', 'which', 'boards', 'have', 'them'], ['ill', 'be', 'running', 'a', 'new', '386dx33'], ['yes', 'it', 'is', 'possible', 'im', 'making', 'a', '7', 'stepper', 'controller', 'board'], ['with', '7', 'digital', 'inputs', 'and', 'up', 'to', '18', 'digital', 'outputs', 'from', 'the', 'port']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(string.punctuation)\n",
    "\n",
    "def process_text(text):\n",
    "  \"\"\"Remove punctuation, lowercase, and tokenize text.\"\"\"\n",
    "  # TODO: check for special cases like \"I'll\"\n",
    "  text = \"\".join([char.lower() for char in text\n",
    "                  if char not in string.punctuation])\n",
    "  return word_tokenize(text)\n",
    "sentences = []\n",
    "for texta in df.text:\n",
    "    for text in texta:\n",
    "        t = process_text(text)\n",
    "        if len(t) == 0:  #len(t) <= 1 and t != 'i':\n",
    "            continue\n",
    "        if len(sentences) < 10:\n",
    "            print('t |',t,'|',len(t))\n",
    "        sentences.append(t)\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:01:27.457386Z",
     "start_time": "2019-03-29T02:01:05.502002Z"
    }
   },
   "outputs": [],
   "source": [
    "news_model = Word2Vec(sentences, min_count=3, size=200, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:02:54.515168Z",
     "start_time": "2019-03-29T02:02:54.502415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dos', 0.7775426506996155),\n",
       " ('os2', 0.7130886316299438),\n",
       " ('xv', 0.7054568529129028),\n",
       " ('apps', 0.7029247879981995),\n",
       " ('mac', 0.6843228340148926),\n",
       " ('driver', 0.6786565780639648),\n",
       " ('motif', 0.6751431226730347),\n",
       " ('microsoft', 0.6671056151390076),\n",
       " ('desktop', 0.6580813527107239),\n",
       " ('drivers', 0.6558448076248169)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar('windows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:03:34.572646Z",
     "start_time": "2019-03-29T02:03:34.559794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apps', 0.7872177362442017),\n",
       " ('pcs', 0.7800400257110596),\n",
       " ('msdos', 0.7620121240615845),\n",
       " ('mswindows', 0.7596473693847656),\n",
       " ('xwindows', 0.7527232766151428),\n",
       " ('x11r4', 0.7419207096099854),\n",
       " ('unix', 0.7404413223266602),\n",
       " ('tcpip', 0.7356176376342773),\n",
       " ('sgi', 0.7253162860870361),\n",
       " ('x11r5', 0.7238607406616211)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.most_similar('linux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:05:21.570000Z",
     "start_time": "2019-03-29T02:05:21.563218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7060164"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('controller', 'board')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:06:00.617512Z",
     "start_time": "2019-03-29T02:06:00.604190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7121664"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('input', 'port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:06:17.826216Z",
     "start_time": "2019-03-29T02:06:17.812834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75221884"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('output', 'port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:06:35.742099Z",
     "start_time": "2019-03-29T02:06:35.729761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85966665"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.similarity('output', 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:11:44.988995Z",
     "start_time": "2019-03-29T02:11:44.973204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.doesnt_match(['windows', 'linux', 'like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:12:57.856343Z",
     "start_time": "2019-03-29T02:12:57.847044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bidirectional'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.wv.doesnt_match(['bidirectional', 'port', 'input', 'output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkzdOZm38yQ_"
   },
   "source": [
    "### Stretch Goals:\n",
    "\n",
    "1) Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents.\n",
    "\n",
    "2) Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
    "\n",
    "Load the pre-trained word vectors and train the Word2vec model\n",
    "\n",
    "Examine the first 100 keys or words of the vocabulary\n",
    "\n",
    "Outputs the vector representation for a select set of words - the words can be of your choice\n",
    "\n",
    "Examine the similarity between words - the words can be of your choice\n",
    "\n",
    "For example:\n",
    "\n",
    "model.similarity('house', 'bungalow')\n",
    "\n",
    "model.similarity('house', 'umbrella')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gakr5rP76IAJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_424_Word_Embeddings_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
