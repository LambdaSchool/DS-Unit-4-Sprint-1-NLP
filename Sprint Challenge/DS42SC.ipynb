{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has keine extra whitespace.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(whitespace_string.strip().split()).replace('a lot of', 'keine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [],
   "source": [
    "# tools for the job\n",
    "import pandas as pd\n",
    "import re as rx\n",
    "import requests\n",
    "\n",
    "s = requests.get('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_n = s.text.split('\\n')\n",
    "no_r = [s.replace('\\r', '') for s in no_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(raw):\n",
    "    m = [rx.findall(r'[A-Z][a-z]+', i)[0] for i in raw]\n",
    "    d = [rx.findall(r'[\\d]{1,2}', i)[0] for i in raw]\n",
    "    y = [rx.findall(r'[\\d]{4}', i)[0] for i in raw]\n",
    "    \n",
    "    df_dates = pd.DataFrame({'Month' : m,'Day' : d,'Year' : y})\n",
    "    \n",
    "    return df_dates\n",
    "\n",
    "clean(no_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lambda_school_loaner_32/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.copy()\n",
    "df.columns = ['sen', 'sen_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sen                                           sen_text\n",
       "0    0                       is so sad for my APL frie...\n",
       "1    0                     I missed the New Moon trail...\n",
       "2    1                            omg its already 7:30 :O\n",
       "3    0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4    0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somewhat inelegant clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize descriptions\n",
    "df.sen_text = df.sen_text.apply(word_tokenize)\n",
    "\n",
    "# convert to lowercase\n",
    "df.sen_text = [[el.lower() for el in row] for row in df.sen_text]\n",
    "\n",
    "# remove punctuation\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "df.sen_text = [[x.translate(table) for x in row] for row in df.sen_text]\n",
    "\n",
    "# filter for alpha characters\n",
    "df.sen_text = [[el for el in row if el.isalpha()] for row in df.sen_text]\n",
    "\n",
    "# remove stop words\n",
    "df.sen_text = [[el for el in row if not el in stop_words] for row in df.sen_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'apl', 'friend']\n",
      "['must', 'think', 'positive']\n",
      "['sad', 'iran']\n",
      "['hate', 'athlete', 'appears', 'tear', 'acl', 'live', 'television']\n",
      "['amp', 'amp', 'fightiin', 'wiit', 'babes']\n",
      "['baddest', 'day', 'eveer']\n",
      "['hate', 'u', 'leysh']\n",
      "['never', 'thought', 'become', 'second', 'choice']\n",
      "['send', 'sunshine', 'northern', 'ireland', 'going', 'swimming', 'today', 'kezbat']\n",
      "['jonas', 'day', 'almost']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df.sen_text[(i*10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "**TF-IDF** is all in the name: **_Term Frequency, Inverse Document Frequency_**.  These scores try and highlight how important certain terms are to **particular** documents of a corpus.  If a term occurs a lot accross documents (some words just occur more often!) it is penalized heavily while terms that occur less frequently accross a body of documents gain a slight boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "# processing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_text</th>\n",
       "      <th>st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[omg, already]</td>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>think mi bf cheating tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[worry, much]</td>\n",
       "      <td>worry much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
       "      <td>juuuuuuuuuuuuuuuuussssst chillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
       "      <td>sunny work tomorrow tv tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[handed, uniform, today, miss, already]</td>\n",
       "      <td>handed uniform today miss already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[hmmmm, wonder, number]</td>\n",
       "      <td>hmmmm wonder number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[must, think, positive]</td>\n",
       "      <td>must think positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, haters, face, day]</td>\n",
       "      <td>thanks haters face day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[weekend, sucked, far]</td>\n",
       "      <td>weekend sucked far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[jb, isnt, showing, australia]</td>\n",
       "      <td>jb isnt showing australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, thats, win]</td>\n",
       "      <td>ok thats win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, way, feel, right]</td>\n",
       "      <td>lt way feel right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[awhhe, man, completely, useless, rt, funny, t...</td>\n",
       "      <td>awhhe man completely useless rt funny twitter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, strangely, fine, gon, na, go, listen...</td>\n",
       "      <td>feeling strangely fine gon na go listen semiso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[huge, roll, thunder, scary]</td>\n",
       "      <td>huge roll thunder scary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[cut, beard, growing, well, year, gon, na, sta...</td>\n",
       "      <td>cut beard growing well year gon na start shaun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>[sad, iran]</td>\n",
       "      <td>sad iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[wompppp, wompp]</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>[one, see, cause, one, else, following, pretty...</td>\n",
       "      <td>one see cause one else following pretty awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>[lt, sad, level, writing, massive, blog, tweet...</td>\n",
       "      <td>lt sad level writing massive blog tweet myspac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>[headed, hospitol, pull, golf, tourny, place, ...</td>\n",
       "      <td>headed hospitol pull golf tourny place think r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>[boring, whats, wrong, please, tell]</td>\n",
       "      <td>boring whats wrong please tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>[ca, nt, bothered, wish, could, spend, rest, l...</td>\n",
       "      <td>ca nt bothered wish could spend rest life sat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>[feeeling, like, shit, right, really, want, sl...</td>\n",
       "      <td>feeeling like shit right really want sleep noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>[goodbye, exams, hello, alcohol, tonight]</td>\n",
       "      <td>goodbye exams hello alcohol tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>[nt, realize, deep, geez, give, girl, warning,...</td>\n",
       "      <td>nt realize deep geez give girl warning atleast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>0</td>\n",
       "      <td>[uclabruin, made, sad, means, albums]</td>\n",
       "      <td>uclabruin made sad means albums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>1</td>\n",
       "      <td>[agree, think, fetish]</td>\n",
       "      <td>agree think fetish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>0</td>\n",
       "      <td>[hope, serious, injury, worried, take, relaxin...</td>\n",
       "      <td>hope serious injury worried take relaxing bath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>0</td>\n",
       "      <td>[ctabita, consolation, weekend, isnt, quite, e...</td>\n",
       "      <td>ctabita consolation weekend isnt quite expecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>1</td>\n",
       "      <td>[ctayah, got, back, yo]</td>\n",
       "      <td>ctayah got back yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>1</td>\n",
       "      <td>[ca, nt, wait, see, movie, enjoy]</td>\n",
       "      <td>ca nt wait see movie enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>1</td>\n",
       "      <td>[smelby, excited, little, nervous, ca, nt, wait]</td>\n",
       "      <td>smelby excited little nervous ca nt wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>0</td>\n",
       "      <td>[yeah, sorrygoing, concert, nightnon, returnab...</td>\n",
       "      <td>yeah sorrygoing concert nightnon returnable ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>1</td>\n",
       "      <td>[ctcash, buildingateam, diabetescure, chocolat...</td>\n",
       "      <td>ctcash buildingateam diabetescure chocolatetwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>1</td>\n",
       "      <td>[im, glad, went, china, town, actually, think,...</td>\n",
       "      <td>im glad went china town actually think biryani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>0</td>\n",
       "      <td>[sorry]</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>0</td>\n",
       "      <td>[damn, dont, sky]</td>\n",
       "      <td>damn dont sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>0</td>\n",
       "      <td>[thing, new, raft, star, wars, films, raft, ep...</td>\n",
       "      <td>thing new raft star wars films raft epicfail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>1</td>\n",
       "      <td>[cthagod]</td>\n",
       "      <td>cthagod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>1</td>\n",
       "      <td>[ctham, followfriday]</td>\n",
       "      <td>ctham followfriday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>0</td>\n",
       "      <td>[ctham, awaresg, wrong, male, point, view, fel...</td>\n",
       "      <td>ctham awaresg wrong male point view felt exclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>0</td>\n",
       "      <td>[ctham, mommyfizz, cuz, big, burly, man, hahah...</td>\n",
       "      <td>ctham mommyfizz cuz big burly man hahahahahaha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>1</td>\n",
       "      <td>[ctham, wilsurn, trying, get, wider, range, sh...</td>\n",
       "      <td>ctham wilsurn trying get wider range shirts su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>1</td>\n",
       "      <td>[ctham, haha, love, passion, support]</td>\n",
       "      <td>ctham haha love passion support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>1</td>\n",
       "      <td>[cthulhullahoop, sucks, like, living, coopersv...</td>\n",
       "      <td>cthulhullahoop sucks like living coopersville ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>1</td>\n",
       "      <td>[cunningstunts, till, go, home, till, saturday...</td>\n",
       "      <td>cunningstunts till go home till saturday x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>1</td>\n",
       "      <td>[afternoon, jim, hows, x]</td>\n",
       "      <td>afternoon jim hows x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>0</td>\n",
       "      <td>[cupatea, foot, really, bad, like, worst, ever...</td>\n",
       "      <td>cupatea foot really bad like worst ever barely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>1</td>\n",
       "      <td>[cupofkaty, fun, health, amp, safety, switch, ...</td>\n",
       "      <td>cupofkaty fun health amp safety switch look sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>0</td>\n",
       "      <td>[cupati, took, waaay, long, get, message, asha...</td>\n",
       "      <td>cupati took waaay long get message ashamed rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>0</td>\n",
       "      <td>[cupcake, seems, like, repeating, problem, hop...</td>\n",
       "      <td>cupcake seems like repeating problem hope able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>1</td>\n",
       "      <td>[cupcake, arrrr, replied, different, tweets, t...</td>\n",
       "      <td>cupcake arrrr replied different tweets time se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>0</td>\n",
       "      <td>[ya, thought]</td>\n",
       "      <td>ya thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>1</td>\n",
       "      <td>[cupcakedollie, yes, yes, glad, fun]</td>\n",
       "      <td>cupcakedollie yes yes glad fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>1</td>\n",
       "      <td>[cupcakekayla, haha, yes]</td>\n",
       "      <td>cupcakekayla haha yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sen                                           sen_text  \\\n",
       "0        0                                 [sad, apl, friend]   \n",
       "1        0                       [missed, new, moon, trailer]   \n",
       "2        1                                     [omg, already]   \n",
       "3        0  [omgaga, im, sooo, im, gunna, cry, dentist, si...   \n",
       "4        0                      [think, mi, bf, cheating, tt]   \n",
       "5        0                                      [worry, much]   \n",
       "6        1                [juuuuuuuuuuuuuuuuussssst, chillin]   \n",
       "7        0               [sunny, work, tomorrow, tv, tonight]   \n",
       "8        1            [handed, uniform, today, miss, already]   \n",
       "9        1                            [hmmmm, wonder, number]   \n",
       "10       0                            [must, think, positive]   \n",
       "11       1                        [thanks, haters, face, day]   \n",
       "12       0                             [weekend, sucked, far]   \n",
       "13       0                     [jb, isnt, showing, australia]   \n",
       "14       0                                   [ok, thats, win]   \n",
       "15       0                             [lt, way, feel, right]   \n",
       "16       0  [awhhe, man, completely, useless, rt, funny, t...   \n",
       "17       1  [feeling, strangely, fine, gon, na, go, listen...   \n",
       "18       0                       [huge, roll, thunder, scary]   \n",
       "19       0  [cut, beard, growing, well, year, gon, na, sta...   \n",
       "20       0                                        [sad, iran]   \n",
       "21       0                                   [wompppp, wompp]   \n",
       "22       1  [one, see, cause, one, else, following, pretty...   \n",
       "23       0  [lt, sad, level, writing, massive, blog, tweet...   \n",
       "24       0  [headed, hospitol, pull, golf, tourny, place, ...   \n",
       "25       0               [boring, whats, wrong, please, tell]   \n",
       "26       0  [ca, nt, bothered, wish, could, spend, rest, l...   \n",
       "27       0  [feeeling, like, shit, right, really, want, sl...   \n",
       "28       1          [goodbye, exams, hello, alcohol, tonight]   \n",
       "29       0  [nt, realize, deep, geez, give, girl, warning,...   \n",
       "...    ...                                                ...   \n",
       "99959    0              [uclabruin, made, sad, means, albums]   \n",
       "99960    1                             [agree, think, fetish]   \n",
       "99961    0  [hope, serious, injury, worried, take, relaxin...   \n",
       "99962    0  [ctabita, consolation, weekend, isnt, quite, e...   \n",
       "99963    1                            [ctayah, got, back, yo]   \n",
       "99964    1                  [ca, nt, wait, see, movie, enjoy]   \n",
       "99965    1   [smelby, excited, little, nervous, ca, nt, wait]   \n",
       "99966    0  [yeah, sorrygoing, concert, nightnon, returnab...   \n",
       "99967    1  [ctcash, buildingateam, diabetescure, chocolat...   \n",
       "99968    1  [im, glad, went, china, town, actually, think,...   \n",
       "99969    0                                            [sorry]   \n",
       "99970    0                                  [damn, dont, sky]   \n",
       "99971    0  [thing, new, raft, star, wars, films, raft, ep...   \n",
       "99972    1                                          [cthagod]   \n",
       "99973    1                              [ctham, followfriday]   \n",
       "99974    0  [ctham, awaresg, wrong, male, point, view, fel...   \n",
       "99975    0  [ctham, mommyfizz, cuz, big, burly, man, hahah...   \n",
       "99976    1  [ctham, wilsurn, trying, get, wider, range, sh...   \n",
       "99977    1              [ctham, haha, love, passion, support]   \n",
       "99978    1  [cthulhullahoop, sucks, like, living, coopersv...   \n",
       "99979    1  [cunningstunts, till, go, home, till, saturday...   \n",
       "99980    1                          [afternoon, jim, hows, x]   \n",
       "99981    0  [cupatea, foot, really, bad, like, worst, ever...   \n",
       "99982    1  [cupofkaty, fun, health, amp, safety, switch, ...   \n",
       "99983    0  [cupati, took, waaay, long, get, message, asha...   \n",
       "99984    0  [cupcake, seems, like, repeating, problem, hop...   \n",
       "99985    1  [cupcake, arrrr, replied, different, tweets, t...   \n",
       "99986    0                                      [ya, thought]   \n",
       "99987    1               [cupcakedollie, yes, yes, glad, fun]   \n",
       "99988    1                          [cupcakekayla, haha, yes]   \n",
       "\n",
       "                                                      st  \n",
       "0                                         sad apl friend  \n",
       "1                                missed new moon trailer  \n",
       "2                                            omg already  \n",
       "3      omgaga im sooo im gunna cry dentist since supo...  \n",
       "4                                think mi bf cheating tt  \n",
       "5                                             worry much  \n",
       "6                       juuuuuuuuuuuuuuuuussssst chillin  \n",
       "7                         sunny work tomorrow tv tonight  \n",
       "8                      handed uniform today miss already  \n",
       "9                                    hmmmm wonder number  \n",
       "10                                   must think positive  \n",
       "11                                thanks haters face day  \n",
       "12                                    weekend sucked far  \n",
       "13                             jb isnt showing australia  \n",
       "14                                          ok thats win  \n",
       "15                                     lt way feel right  \n",
       "16     awhhe man completely useless rt funny twitter ...  \n",
       "17     feeling strangely fine gon na go listen semiso...  \n",
       "18                               huge roll thunder scary  \n",
       "19     cut beard growing well year gon na start shaun...  \n",
       "20                                              sad iran  \n",
       "21                                         wompppp wompp  \n",
       "22       one see cause one else following pretty awesome  \n",
       "23     lt sad level writing massive blog tweet myspac...  \n",
       "24     headed hospitol pull golf tourny place think r...  \n",
       "25                        boring whats wrong please tell  \n",
       "26     ca nt bothered wish could spend rest life sat ...  \n",
       "27     feeeling like shit right really want sleep noo...  \n",
       "28                   goodbye exams hello alcohol tonight  \n",
       "29        nt realize deep geez give girl warning atleast  \n",
       "...                                                  ...  \n",
       "99959                    uclabruin made sad means albums  \n",
       "99960                                 agree think fetish  \n",
       "99961  hope serious injury worried take relaxing bath...  \n",
       "99962  ctabita consolation weekend isnt quite expecti...  \n",
       "99963                                 ctayah got back yo  \n",
       "99964                         ca nt wait see movie enjoy  \n",
       "99965           smelby excited little nervous ca nt wait  \n",
       "99966  yeah sorrygoing concert nightnon returnable ti...  \n",
       "99967  ctcash buildingateam diabetescure chocolatetwe...  \n",
       "99968  im glad went china town actually think biryani...  \n",
       "99969                                              sorry  \n",
       "99970                                      damn dont sky  \n",
       "99971       thing new raft star wars films raft epicfail  \n",
       "99972                                            cthagod  \n",
       "99973                                 ctham followfriday  \n",
       "99974  ctham awaresg wrong male point view felt exclu...  \n",
       "99975  ctham mommyfizz cuz big burly man hahahahahaha...  \n",
       "99976  ctham wilsurn trying get wider range shirts su...  \n",
       "99977                    ctham haha love passion support  \n",
       "99978  cthulhullahoop sucks like living coopersville ...  \n",
       "99979         cunningstunts till go home till saturday x  \n",
       "99980                               afternoon jim hows x  \n",
       "99981  cupatea foot really bad like worst ever barely...  \n",
       "99982  cupofkaty fun health amp safety switch look sp...  \n",
       "99983  cupati took waaay long get message ashamed rig...  \n",
       "99984  cupcake seems like repeating problem hope able...  \n",
       "99985  cupcake arrrr replied different tweets time se...  \n",
       "99986                                         ya thought  \n",
       "99987                     cupcakedollie yes yes glad fun  \n",
       "99988                              cupcakekayla haha yes  \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was getting an error having these as arrays so this converts back to strings for modeling.\n",
    "patch = [' '.join(i) for i in df.sen_text]\n",
    "df.assign(st = patch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/target assignment\n",
    "X = df.st\n",
    "y = df.sen\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params =  {'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "           'tfidfvectorizer__max_features' : [50, 100, None],\n",
    "          }\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                     LogisticRegression(solver='lbfgs',\n",
    "                     max_iter=200))\n",
    "\n",
    "log_grid_cv = GridSearchCV(pipe, params, cv=3, scoring='roc_auc')\n",
    "log_grid_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__max_features': None, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "Train ROCAUC: 0.89\n",
      "Test ROCAUC: 0.76\n"
     ]
    }
   ],
   "source": [
    "print (log_grid_cv.best_params_)\n",
    "\n",
    "print(f'Train ROCAUC: {roc_auc_score(log_grid_cv.predict(X_train), y_train):.2f}')\n",
    "print(f'Test ROCAUC: {roc_auc_score(log_grid_cv.predict(X_test), y_test):.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the best params from the first tfidf\n",
    "params =  {'tfidfvectorizer__ngram_range' : [(1,2)],\n",
    "           'tfidfvectorizer__max_features' : [None],\n",
    "          }\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                     MultinomialNB())\n",
    "\n",
    "nb_grid_cv = GridSearchCV(pipe, params, cv=3, scoring='roc_auc')\n",
    "nb_grid_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROCAUC: 0.96\n",
      "Test ROCAUC: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(f'Train ROCAUC: {roc_auc_score(nb_grid_cv.predict(X_train), y_train):.2f}')\n",
    "print(f'Test ROCAUC: {roc_auc_score(nb_grid_cv.predict(X_test), y_test):.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sad',\n",
       " 'friend',\n",
       " 'missed',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'trailer',\n",
       " 'omg',\n",
       " 'already',\n",
       " 'im',\n",
       " 'sooo',\n",
       " 'gunna',\n",
       " 'cry',\n",
       " 'dentist',\n",
       " 'since',\n",
       " 'get',\n",
       " 'crown',\n",
       " 'put',\n",
       " 'think',\n",
       " 'mi',\n",
       " 'bf',\n",
       " 'cheating',\n",
       " 'tt',\n",
       " 'worry',\n",
       " 'much',\n",
       " 'chillin',\n",
       " 'sunny',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'tv',\n",
       " 'tonight',\n",
       " 'handed',\n",
       " 'uniform',\n",
       " 'today',\n",
       " 'miss',\n",
       " 'hmmmm',\n",
       " 'wonder',\n",
       " 'number',\n",
       " 'must',\n",
       " 'positive',\n",
       " 'thanks',\n",
       " 'haters',\n",
       " 'face',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'sucked',\n",
       " 'far',\n",
       " 'jb',\n",
       " 'isnt',\n",
       " 'showing',\n",
       " 'australia',\n",
       " 'ok',\n",
       " 'thats',\n",
       " 'win',\n",
       " 'lt',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'right',\n",
       " 'man',\n",
       " 'completely',\n",
       " 'useless',\n",
       " 'rt',\n",
       " 'funny',\n",
       " 'twitter',\n",
       " 'http',\n",
       " 'feeling',\n",
       " 'strangely',\n",
       " 'fine',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'go',\n",
       " 'listen',\n",
       " 'celebrate',\n",
       " 'huge',\n",
       " 'roll',\n",
       " 'thunder',\n",
       " 'scary',\n",
       " 'cut',\n",
       " 'beard',\n",
       " 'growing',\n",
       " 'well',\n",
       " 'year',\n",
       " 'start',\n",
       " 'happy',\n",
       " 'meantime',\n",
       " 'iran',\n",
       " 'one',\n",
       " 'see',\n",
       " 'cause',\n",
       " 'else',\n",
       " 'following',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'level',\n",
       " 'writing',\n",
       " 'massive',\n",
       " 'blog',\n",
       " 'tweet',\n",
       " 'myspace',\n",
       " 'comp',\n",
       " 'shut',\n",
       " 'lost',\n",
       " 'lays',\n",
       " 'position',\n",
       " 'headed',\n",
       " 'pull',\n",
       " 'golf',\n",
       " 'place',\n",
       " 'something',\n",
       " 'yeah',\n",
       " 'boring',\n",
       " 'whats',\n",
       " 'wrong',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'ca',\n",
       " 'nt',\n",
       " 'bothered',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'rest',\n",
       " 'life',\n",
       " 'sat',\n",
       " 'going',\n",
       " 'gigs',\n",
       " 'seriously',\n",
       " 'like',\n",
       " 'shit',\n",
       " 'really',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'nooo',\n",
       " 'hours',\n",
       " 'dancing',\n",
       " 'art',\n",
       " 'assignment',\n",
       " 'finish',\n",
       " 'goodbye',\n",
       " 'exams',\n",
       " 'hello',\n",
       " 'alcohol',\n",
       " 'realize',\n",
       " 'deep',\n",
       " 'geez',\n",
       " 'give',\n",
       " 'girl',\n",
       " 'warning',\n",
       " 'atleast',\n",
       " 'hate',\n",
       " 'appears',\n",
       " 'tear',\n",
       " 'live',\n",
       " 'television',\n",
       " 'guys',\n",
       " 'wearing',\n",
       " 'skinny',\n",
       " 'jeans',\n",
       " 'cute',\n",
       " 'sweater',\n",
       " 'heels',\n",
       " 'sure',\n",
       " 'meet',\n",
       " 'meat',\n",
       " 'moving',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " 'need',\n",
       " 'days',\n",
       " 'week',\n",
       " 'dont',\n",
       " 'room',\n",
       " 'sick',\n",
       " 'wardrobe',\n",
       " 'cant',\n",
       " 'till',\n",
       " 'walk',\n",
       " 'yay',\n",
       " 'sox',\n",
       " 'great',\n",
       " 'times',\n",
       " 'million',\n",
       " 'uploading',\n",
       " 'pictures',\n",
       " 'type',\n",
       " 'spaz',\n",
       " 'downloads',\n",
       " 'virus',\n",
       " 'brother',\n",
       " 'msn',\n",
       " 'fucked',\n",
       " 'forever',\n",
       " 'amp',\n",
       " 'babes',\n",
       " 'wrote',\n",
       " 'last',\n",
       " 'got',\n",
       " 'call',\n",
       " 'someone',\n",
       " 'york',\n",
       " 'office',\n",
       " 'enough',\n",
       " 'said',\n",
       " 'even',\n",
       " 'say',\n",
       " 'anyways',\n",
       " 'chris',\n",
       " 'cornell',\n",
       " 'chicago',\n",
       " 'health',\n",
       " 'class',\n",
       " 'joke',\n",
       " 'show',\n",
       " 'makes',\n",
       " 'look',\n",
       " 'reality',\n",
       " 'time',\n",
       " 'low',\n",
       " 'shall',\n",
       " 'motivation',\n",
       " 'entertainment',\n",
       " 'complained',\n",
       " 'properly',\n",
       " 'experiment',\n",
       " 'melody',\n",
       " 'another',\n",
       " 'lakers',\n",
       " 'neither',\n",
       " 'magic',\n",
       " 'fun',\n",
       " 'bathroom',\n",
       " 'clean',\n",
       " 'enjoyable',\n",
       " 'tasks',\n",
       " 'boom',\n",
       " 'pow',\n",
       " 'proud',\n",
       " 'congrats',\n",
       " 'though',\n",
       " 'david',\n",
       " 'five',\n",
       " 'end',\n",
       " 'july',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'katie',\n",
       " 'concert',\n",
       " 'friends',\n",
       " 'leaving',\n",
       " 'stupid',\n",
       " 'love',\n",
       " 'ur',\n",
       " 'mom',\n",
       " 'hug',\n",
       " 'harry',\n",
       " 'sunday',\n",
       " 'happiness',\n",
       " 'hand',\n",
       " 'u',\n",
       " 'always',\n",
       " 'bend',\n",
       " 'backwards',\n",
       " 'sooooon',\n",
       " 'cody',\n",
       " 'booo',\n",
       " 'seen',\n",
       " 'allergies',\n",
       " 'hair',\n",
       " 'taking',\n",
       " 'public',\n",
       " 'poll',\n",
       " 'hurts',\n",
       " 'earl',\n",
       " 'jersey',\n",
       " 'first',\n",
       " 'hour',\n",
       " 'sytycd',\n",
       " 'night',\n",
       " 'find',\n",
       " 'online',\n",
       " 'fix',\n",
       " 'thought',\n",
       " 'become',\n",
       " 'second',\n",
       " 'choice',\n",
       " 'may',\n",
       " 'friendly',\n",
       " 'lol',\n",
       " 'basil',\n",
       " 'plant',\n",
       " 'wan',\n",
       " 'home',\n",
       " 'church',\n",
       " 'wht',\n",
       " 'make',\n",
       " 'pizza',\n",
       " 'inch',\n",
       " 'guitar',\n",
       " 'generous',\n",
       " 'p',\n",
       " 'x',\n",
       " 'miley',\n",
       " 'tour',\n",
       " 'wanted',\n",
       " 'mean',\n",
       " 'kid',\n",
       " 'stick',\n",
       " 'head',\n",
       " 'fly',\n",
       " 'away',\n",
       " 'slow',\n",
       " 'tix',\n",
       " 'send',\n",
       " 'sunshine',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'swimming',\n",
       " 'beach',\n",
       " 'would',\n",
       " 'happier',\n",
       " 'walls',\n",
       " 'bedroom',\n",
       " 'painted',\n",
       " 'white',\n",
       " 'idk',\n",
       " 'wat',\n",
       " 'trust',\n",
       " 'sorry',\n",
       " 'da',\n",
       " 'pain',\n",
       " 'caused',\n",
       " 'ima',\n",
       " 'take',\n",
       " 'dis',\n",
       " 'straighten',\n",
       " 'luv',\n",
       " 'yall',\n",
       " 'finding',\n",
       " 'banging',\n",
       " 'brain',\n",
       " 'heads',\n",
       " 'come',\n",
       " 'save',\n",
       " 'bed',\n",
       " 'cough',\n",
       " 'cab',\n",
       " 'airport',\n",
       " 'christy',\n",
       " 'gt',\n",
       " 'case',\n",
       " 'emo',\n",
       " 'camp',\n",
       " 'wee',\n",
       " 'bit',\n",
       " 'alr',\n",
       " 'bringing',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'watch',\n",
       " 'world',\n",
       " 'report',\n",
       " 'jonas',\n",
       " 'almost',\n",
       " 'jus',\n",
       " 'fr',\n",
       " 'funeral',\n",
       " 'cried',\n",
       " 'grandpa',\n",
       " 'quot',\n",
       " 'smile',\n",
       " 'cuz',\n",
       " 'graduated',\n",
       " 'longest',\n",
       " 'ever',\n",
       " 'ugh',\n",
       " 'wo',\n",
       " 'let',\n",
       " 'bball',\n",
       " 'game',\n",
       " 'grrr',\n",
       " 'says',\n",
       " 'phone',\n",
       " 'immediately',\n",
       " 'tmobile',\n",
       " 'paying',\n",
       " 'car',\n",
       " 'stolen',\n",
       " 'mother',\n",
       " 'pose',\n",
       " 'hang',\n",
       " 'girls',\n",
       " 'hope',\n",
       " 'movie',\n",
       " 'rats',\n",
       " 'plan',\n",
       " 'guess',\n",
       " 'means',\n",
       " 'two',\n",
       " 'presentations',\n",
       " 'cool',\n",
       " 'oh',\n",
       " 'thank',\n",
       " 'pleased',\n",
       " 'guna',\n",
       " 'soon',\n",
       " 'talkin',\n",
       " 'lose',\n",
       " 'rip',\n",
       " 'eddings',\n",
       " 'rose',\n",
       " 'back',\n",
       " 'xmas',\n",
       " 'special',\n",
       " 'damn',\n",
       " 'half',\n",
       " 'aww',\n",
       " 'neighbor',\n",
       " 'status',\n",
       " 'next',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'daily',\n",
       " 'dose',\n",
       " 'definition',\n",
       " 'reviewed',\n",
       " 'entry',\n",
       " 'decided',\n",
       " 'publish',\n",
       " 'help',\n",
       " 'explains',\n",
       " 'alot',\n",
       " 'sequel',\n",
       " 'better',\n",
       " 'fuck',\n",
       " 'told',\n",
       " 'drink',\n",
       " 'wmy',\n",
       " 'gave',\n",
       " 'bb',\n",
       " 'brought',\n",
       " 'neighbors',\n",
       " 'v',\n",
       " 'sweet',\n",
       " 'bought',\n",
       " 'suit',\n",
       " 'true',\n",
       " 'highly',\n",
       " 'actually',\n",
       " 'favorite',\n",
       " 'character',\n",
       " 'book',\n",
       " 'knw',\n",
       " 'whyy',\n",
       " 'getting',\n",
       " 'hub',\n",
       " 'standing',\n",
       " 'pouring',\n",
       " 'rain',\n",
       " 'sitting',\n",
       " 'top',\n",
       " 'summer',\n",
       " 'sucks',\n",
       " 'went',\n",
       " 'dog',\n",
       " 'vets',\n",
       " 'theyve',\n",
       " 'ear',\n",
       " 'charged',\n",
       " 'us',\n",
       " 'still',\n",
       " 'everytime',\n",
       " 'moves',\n",
       " 'picture',\n",
       " 'naked',\n",
       " 'trains',\n",
       " 'manchester',\n",
       " 'jonasbrothers',\n",
       " 'people',\n",
       " 'picky',\n",
       " 'dice',\n",
       " 'wide',\n",
       " 'awake',\n",
       " 'awful',\n",
       " 'weather',\n",
       " 'stinks',\n",
       " 'comin',\n",
       " 'moe',\n",
       " 'hi',\n",
       " 'stranger',\n",
       " 'frank',\n",
       " 'urself',\n",
       " 'gay',\n",
       " 'asylm',\n",
       " 'panel',\n",
       " 'normal',\n",
       " 'started',\n",
       " 'squarespace',\n",
       " 'brighten',\n",
       " 'bad',\n",
       " 'anything',\n",
       " 'susan',\n",
       " 'boyle',\n",
       " 'didnt',\n",
       " 'diversity',\n",
       " 'good',\n",
       " 'congratz',\n",
       " 'believe',\n",
       " 'goooood',\n",
       " 'graduate',\n",
       " 'paint',\n",
       " 'author',\n",
       " 'lesbian',\n",
       " 'story',\n",
       " 'news',\n",
       " 'unknown',\n",
       " 'error',\n",
       " 'uh',\n",
       " 'iphone',\n",
       " 'os',\n",
       " 'fail',\n",
       " 'experts',\n",
       " 'predict',\n",
       " 'eagles',\n",
       " 'giants',\n",
       " 'bs',\n",
       " 'know',\n",
       " 'hungry',\n",
       " 'lets',\n",
       " 'outside',\n",
       " 'balcony',\n",
       " 'eat',\n",
       " 'driving',\n",
       " 'mad',\n",
       " 'men',\n",
       " 'uk',\n",
       " 'female',\n",
       " 'internet',\n",
       " 'starting',\n",
       " 'business',\n",
       " 'women',\n",
       " 'truth',\n",
       " 'hiding',\n",
       " 'eyes',\n",
       " 'paramore',\n",
       " 'decode',\n",
       " 'try',\n",
       " 'turn',\n",
       " 'inside',\n",
       " 'admit',\n",
       " 'literally',\n",
       " 'coldplay',\n",
       " 'tears',\n",
       " 'kids',\n",
       " 'break',\n",
       " 'dads',\n",
       " 'sighs',\n",
       " 'unsure',\n",
       " 'annoyed',\n",
       " 'via',\n",
       " 'designed',\n",
       " 'gallery',\n",
       " 'exhibition',\n",
       " 'nyt',\n",
       " 'cries',\n",
       " 'potter',\n",
       " 'comes',\n",
       " 'vietnam',\n",
       " 'money',\n",
       " 'mouth',\n",
       " 'headin',\n",
       " 'cheers',\n",
       " 'sigh',\n",
       " 'stalker',\n",
       " 'divorce',\n",
       " 'move',\n",
       " 'lookin',\n",
       " 'around',\n",
       " 'reason',\n",
       " 'atm',\n",
       " 'little',\n",
       " 'atl',\n",
       " 'ily',\n",
       " 'dies',\n",
       " 'laughter',\n",
       " 'soo',\n",
       " 'watching',\n",
       " 'shopping',\n",
       " 'w',\n",
       " 'trent',\n",
       " 'hopefully',\n",
       " 'best',\n",
       " 'weird',\n",
       " 'many',\n",
       " 'rained',\n",
       " 'truck',\n",
       " 'race',\n",
       " 'worse',\n",
       " 'f',\n",
       " 'confusing',\n",
       " 'hot',\n",
       " 'choco',\n",
       " 'shirt',\n",
       " 'panties',\n",
       " 'north',\n",
       " 'due',\n",
       " 'body',\n",
       " 'line',\n",
       " 'missing',\n",
       " 'design',\n",
       " 'council',\n",
       " 'tech',\n",
       " 'transfer',\n",
       " 'event',\n",
       " 'everything',\n",
       " 'hard',\n",
       " 'bueno',\n",
       " 'bye',\n",
       " 'ya',\n",
       " 'amazing',\n",
       " 'twits',\n",
       " 'national',\n",
       " 'production',\n",
       " 'department',\n",
       " 'priority',\n",
       " 'beats',\n",
       " 'reach',\n",
       " 'enjoy',\n",
       " 'venue',\n",
       " 'shine',\n",
       " 'everywhere',\n",
       " 'everyone',\n",
       " 'train',\n",
       " 'crash',\n",
       " 'dc',\n",
       " 'weeks',\n",
       " 'child',\n",
       " 'nancy',\n",
       " 'adbert',\n",
       " 'video',\n",
       " 'wishing',\n",
       " 'ch',\n",
       " 'staying',\n",
       " 'fabulous',\n",
       " 'university',\n",
       " 'happened',\n",
       " 'angels',\n",
       " 'demons',\n",
       " 'lions',\n",
       " 'tigers',\n",
       " 'bears',\n",
       " 'americanwomannn',\n",
       " 'impact',\n",
       " 'arm',\n",
       " 'leg',\n",
       " 'wait',\n",
       " 'manners',\n",
       " 'saw',\n",
       " 'mum',\n",
       " 'fat',\n",
       " 'asses',\n",
       " 'codinghorror',\n",
       " 'three',\n",
       " 'external',\n",
       " 'monitors',\n",
       " 'laptop',\n",
       " 'maybe',\n",
       " 'later',\n",
       " 'c',\n",
       " 'making',\n",
       " 'garlic',\n",
       " 'bread',\n",
       " 'bday',\n",
       " 'pants',\n",
       " 'job',\n",
       " 'fml',\n",
       " 'sweep',\n",
       " 'haha',\n",
       " 'r',\n",
       " 'science',\n",
       " 'cartoons',\n",
       " 'edinburgh',\n",
       " 'fringe',\n",
       " 'monies',\n",
       " 'btw',\n",
       " 'angryfeet',\n",
       " 'blame',\n",
       " 'school',\n",
       " 'done',\n",
       " 'short',\n",
       " 'stack',\n",
       " 'thing',\n",
       " 'latter',\n",
       " 'store',\n",
       " 'comics',\n",
       " 'thinking',\n",
       " 'less',\n",
       " 'chance',\n",
       " 'stays',\n",
       " 'orlando',\n",
       " 'wud',\n",
       " 'beautiful',\n",
       " 'servers',\n",
       " 'backup',\n",
       " 'experience',\n",
       " 'problems',\n",
       " 'delay',\n",
       " 'yes',\n",
       " 'jk',\n",
       " 'gentle',\n",
       " 'breeze',\n",
       " 'blown',\n",
       " 'se',\n",
       " 'joseph',\n",
       " 'future',\n",
       " 'nyc',\n",
       " 'holler',\n",
       " 'cheese',\n",
       " 'burgers',\n",
       " 'nothing',\n",
       " 'kind',\n",
       " 'sort',\n",
       " 'pick',\n",
       " 'flowers',\n",
       " 'teacher',\n",
       " 'jealous',\n",
       " 'without',\n",
       " 'hospital',\n",
       " 'mas',\n",
       " 'security',\n",
       " 'chain',\n",
       " 'strong',\n",
       " 'link',\n",
       " 'youtube',\n",
       " 'account',\n",
       " 'suspended',\n",
       " 'single',\n",
       " 'ladies',\n",
       " 'dance',\n",
       " 'sold',\n",
       " 'k',\n",
       " 'hoping',\n",
       " 'nervous',\n",
       " 'aim',\n",
       " 'talk',\n",
       " 'followers',\n",
       " 'grocery',\n",
       " 'oldies',\n",
       " 'sugar',\n",
       " 'ray',\n",
       " 'convo',\n",
       " 'keep',\n",
       " 'roomies',\n",
       " 'catholic',\n",
       " 'pork',\n",
       " 'fridge',\n",
       " 'bitches',\n",
       " 'wake',\n",
       " 'early',\n",
       " 'la',\n",
       " 'opera',\n",
       " 'kay',\n",
       " 'etc',\n",
       " 'bother',\n",
       " 'gum',\n",
       " 'lift',\n",
       " 'surgery',\n",
       " 'tho',\n",
       " 'eek',\n",
       " 'tonite',\n",
       " 'hates',\n",
       " 'app',\n",
       " 'bhai',\n",
       " 'used',\n",
       " 'ka',\n",
       " 'chilling',\n",
       " 'blackberry',\n",
       " 'unlimited',\n",
       " 'every',\n",
       " 'pop',\n",
       " 'quite',\n",
       " 'active',\n",
       " 'shame',\n",
       " 'mamma',\n",
       " 'mia',\n",
       " 'horny',\n",
       " 'island',\n",
       " 'letting',\n",
       " 'check',\n",
       " 'knowledge',\n",
       " 'referring',\n",
       " 'hey',\n",
       " 'thx',\n",
       " 'ttyl',\n",
       " 'sowwy',\n",
       " 'mothers',\n",
       " 'knew',\n",
       " 'sharepoint',\n",
       " 'written',\n",
       " 'jamie',\n",
       " 'feels',\n",
       " 'bowling',\n",
       " 'ball',\n",
       " 'heart',\n",
       " 'badly',\n",
       " 'fs',\n",
       " 'keeps',\n",
       " 'crashing',\n",
       " 'load',\n",
       " 'plane',\n",
       " 'current',\n",
       " 'dad',\n",
       " 'mood',\n",
       " 'currently',\n",
       " 'blah',\n",
       " 'parking',\n",
       " 'lot',\n",
       " 'fed',\n",
       " 'matters',\n",
       " 'worst',\n",
       " 'super',\n",
       " 'nasty',\n",
       " 'rainy',\n",
       " 'yuck',\n",
       " 'aiden',\n",
       " 'davis',\n",
       " 'bummed',\n",
       " 'dias',\n",
       " 'para',\n",
       " 'drawing',\n",
       " 'fake',\n",
       " 'stayin',\n",
       " 'lawn',\n",
       " 'tanning',\n",
       " 'play',\n",
       " 'camera',\n",
       " 'settle',\n",
       " 'playing',\n",
       " 'iv',\n",
       " 'drive',\n",
       " 'display',\n",
       " 'upside',\n",
       " 'photos',\n",
       " 'bluetooth',\n",
       " 'computer',\n",
       " 'effort',\n",
       " 'bring',\n",
       " 'content',\n",
       " 'added',\n",
       " 'lease',\n",
       " 'release',\n",
       " 'digital',\n",
       " 'tight',\n",
       " 'response',\n",
       " 'j',\n",
       " 'kinda',\n",
       " 'moment',\n",
       " 'silence',\n",
       " 'childhood',\n",
       " 'homes',\n",
       " 'sadly',\n",
       " 'replaced',\n",
       " 'robotpickuplines',\n",
       " 'hilarious',\n",
       " 'punch',\n",
       " 'b',\n",
       " 'n',\n",
       " 'crew',\n",
       " 'worth',\n",
       " 'storms',\n",
       " 'satisfying',\n",
       " 'loves',\n",
       " 'god',\n",
       " 'friday',\n",
       " 'ti',\n",
       " 'connection',\n",
       " 'died',\n",
       " 'mobile',\n",
       " 'thrilled',\n",
       " 'hearts',\n",
       " 'ringtone',\n",
       " 'song',\n",
       " 'trackle',\n",
       " 'apple',\n",
       " 'card',\n",
       " 'helps',\n",
       " 'huh',\n",
       " 'forgot',\n",
       " 'hassle',\n",
       " 'music',\n",
       " 'exhausted',\n",
       " 'ben',\n",
       " 'cake',\n",
       " 'batter',\n",
       " 'somebody',\n",
       " 'ish',\n",
       " 'tlk',\n",
       " 'long',\n",
       " 'pacific',\n",
       " 'ocean',\n",
       " 'open',\n",
       " 'door',\n",
       " 'throw',\n",
       " 'pc',\n",
       " 'hell',\n",
       " 'bbc',\n",
       " 'canucks',\n",
       " 'huhuhu',\n",
       " 'made',\n",
       " 'nice',\n",
       " 'twice',\n",
       " 'brilliant',\n",
       " 'xd',\n",
       " 'lonely',\n",
       " 'blue',\n",
       " 'summertime',\n",
       " 'ill',\n",
       " 'fuckin',\n",
       " 'goin',\n",
       " 'portugal',\n",
       " 'drops',\n",
       " 'ye',\n",
       " 'eh',\n",
       " 'leave',\n",
       " 'mexico',\n",
       " 'maths',\n",
       " 'sounds',\n",
       " 'leadership',\n",
       " 'considering',\n",
       " 'library',\n",
       " 'cuts',\n",
       " 'tax',\n",
       " 'sake',\n",
       " 'things',\n",
       " 'town',\n",
       " 'netball',\n",
       " 'match',\n",
       " 'baby',\n",
       " 'member',\n",
       " 'family',\n",
       " 'bedtime',\n",
       " 'cold',\n",
       " 'chills',\n",
       " 'goes',\n",
       " 'books',\n",
       " 'results',\n",
       " 'poor',\n",
       " 'lil',\n",
       " 'enjoying',\n",
       " 'big',\n",
       " 'rat',\n",
       " 'dbl',\n",
       " 'standard',\n",
       " 'yep',\n",
       " 'epic',\n",
       " 'magazine',\n",
       " 'broke',\n",
       " 'tiger',\n",
       " 'woods',\n",
       " 'wifey',\n",
       " 'sux',\n",
       " 'wondering',\n",
       " 'lata',\n",
       " 'gather',\n",
       " 'hay',\n",
       " 'found',\n",
       " 'boy',\n",
       " 'stay',\n",
       " 'master',\n",
       " 'holy',\n",
       " 'iranelection',\n",
       " 'realizes',\n",
       " 'bodies',\n",
       " 'air',\n",
       " 'france',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'noooooooooo',\n",
       " 'follow',\n",
       " 'hear',\n",
       " 'asking',\n",
       " 'hardest',\n",
       " 'working',\n",
       " 'chica',\n",
       " 'mii',\n",
       " 'boss',\n",
       " 'lady',\n",
       " 'condition',\n",
       " 'series',\n",
       " 'phoenix',\n",
       " 'boston',\n",
       " 'cleveland',\n",
       " 'teams',\n",
       " 'played',\n",
       " 'terrible',\n",
       " 'soccer',\n",
       " 'basketball',\n",
       " 'mustache',\n",
       " 'desperate',\n",
       " 'bout',\n",
       " 'ahmier',\n",
       " 'marco',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(df.sen_text, window=3, size=500)\n",
    "list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 0.8626171350479126),\n",
       " ('facebook', 0.8599664568901062),\n",
       " ('link', 0.8536689281463623),\n",
       " ('dm', 0.8509138822555542),\n",
       " ('list', 0.837100088596344)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('twitter', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memo', 0.9879697561264038),\n",
       " ('whining', 0.9863419532775879),\n",
       " ('aots', 0.9845591187477112),\n",
       " ('hubby', 0.98436039686203),\n",
       " ('starving', 0.9843247532844543)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hmmmm\n",
    "model.wv.most_similar('drunk', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parents', 0.9831657409667969),\n",
       " ('dad', 0.983155369758606),\n",
       " ('focused', 0.9827832579612732),\n",
       " ('warped', 0.9826194047927856),\n",
       " ('noon', 0.9817023277282715)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('swimming', topn=5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
