{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpPcbYew_ttN"
   },
   "source": [
    "# Part 1 - Working with Text Data\n",
    "\n",
    "### Use Python string methods remove irregular whitespace from the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dtotEnsStY5o",
    "outputId": "c4e8a355-5366-4b7a-ddf8-740e9f1fbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is a    string   that has  \n",
      " a lot of  extra \n",
      "   whitespace.   \n"
     ]
    }
   ],
   "source": [
    "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
    "\n",
    "print(whitespace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9-MkBwasXx8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string that has keine extra whitespace.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(whitespace_string.strip().split()).replace('a lot of', 'keine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vg1-d2aAsXLn"
   },
   "source": [
    "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
    "\n",
    "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
    "\n",
    "- Day\n",
    "- Month\n",
    "- Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWDiN4C9_0sq"
   },
   "outputs": [],
   "source": [
    "# tools for the job\n",
    "import pandas as pd\n",
    "import re as rx\n",
    "import requests\n",
    "\n",
    "s = requests.get('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_n = s.text.split('\\n')\n",
    "no_r = [s.replace('\\r', '') for s in no_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>29</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>April</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>May</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>24</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>May</td>\n",
       "      <td>31</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>June</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>28</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>July</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>July</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>July</td>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month Day  Year\n",
       "0   March   8  2015\n",
       "1   March  15  2015\n",
       "2   March  22  2015\n",
       "3   March  29  2015\n",
       "4   April   5  2015\n",
       "5   April  12  2015\n",
       "6   April  19  2015\n",
       "7   April  26  2015\n",
       "8     May   3  2015\n",
       "9     May  10  2015\n",
       "10    May  17  2015\n",
       "11    May  24  2015\n",
       "12    May  31  2015\n",
       "13   June   7  2015\n",
       "14   June  14  2015\n",
       "15   June  21  2015\n",
       "16   June  28  2015\n",
       "17   July   5  2015\n",
       "18   July  12  2015\n",
       "19   July  19  2015"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(raw):\n",
    "    m = [rx.findall(r'[A-Z][a-z]+', i)[0] for i in raw]\n",
    "    d = [rx.findall(r'[\\d]{1,2}', i)[0] for i in raw]\n",
    "    y = [rx.findall(r'[\\d]{4}', i)[0] for i in raw]\n",
    "    \n",
    "    df_dates = pd.DataFrame({'Month' : m,'Day' : d,'Year' : y})\n",
    "    \n",
    "    return df_dates\n",
    "\n",
    "clean(no_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Part 2 - Bag of Words \n",
    "\n",
    "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
    "\n",
    "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
    "\n",
    " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
    "\n",
    "1) Text should be lowercase.\n",
    "\n",
    "2) Stopwords should be removed.\n",
    "\n",
    "3) Punctuation should be removed.\n",
    "\n",
    "4) Tweets should be tokenized at the word level. \n",
    "\n",
    "(The above don't necessarily need to be completed in that specific order.)\n",
    "\n",
    "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lambda_school_loaner_32/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzdhyTS_3F9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.copy()\n",
    "df.columns = ['sen', 'sen_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sen                                           sen_text\n",
       "0    0                       is so sad for my APL frie...\n",
       "1    0                     I missed the New Moon trail...\n",
       "2    1                            omg its already 7:30 :O\n",
       "3    0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4    0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somewhat inelegant clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize descriptions\n",
    "df.sen_text = df.sen_text.apply(word_tokenize)\n",
    "\n",
    "# convert to lowercase\n",
    "df.sen_text = [[el.lower() for el in row] for row in df.sen_text]\n",
    "\n",
    "# remove punctuation (to revisit)\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "df.sen_text = [[x.translate(table) for x in row] for row in df.sen_text]\n",
    "\n",
    "# filter for alpha characters\n",
    "df.sen_text = [[el for el in row if el.isalpha()] for row in df.sen_text]\n",
    "\n",
    "# remove stop words\n",
    "df.sen_text = [[el for el in row if not el in stop_words] for row in df.sen_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'apl', 'friend']\n",
      "['must', 'think', 'positive']\n",
      "['sad', 'iran']\n",
      "['hate', 'athlete', 'appears', 'tear', 'acl', 'live', 'television']\n",
      "['amp', 'amp', 'fightiin', 'wiit', 'babes']\n",
      "['baddest', 'day', 'eveer']\n",
      "['hate', 'u', 'leysh']\n",
      "['never', 'thought', 'become', 'second', 'choice']\n",
      "['send', 'sunshine', 'northern', 'ireland', 'going', 'swimming', 'today', 'kezbat']\n",
      "['jonas', 'day', 'almost']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df.sen_text[(i*10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q764vszGqiUh"
   },
   "source": [
    "### How should TF-IDF scores be interpreted? How are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2Ji7BMhqs3M"
   },
   "source": [
    "**TF-IDF** is all in the name: **_Term Frequency, Inverse Document Frequency_**.  These scores try and highlight how important certain terms are to **particular** documents of a corpus.  If a term occurs a lot accross documents (some words just occur more often!) it is penalized heavily while terms that occur less frequently accross a body of documents gain a slight boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "# processing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sen_text = [' '.join(i) for i in df.sen_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/target assignment\n",
    "X = df.sen_text\n",
    "y = df.sen\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111    0\n",
      "5078     0\n",
      "20029    1\n",
      "70187    0\n",
      "89492    0\n",
      "51725    1\n",
      "45990    1\n",
      "53562    1\n",
      "31009    1\n",
      "76743    1\n",
      "34358    1\n",
      "21810    1\n",
      "50973    1\n",
      "8810     1\n",
      "26928    0\n",
      "81952    0\n",
      "3601     0\n",
      "57022    1\n",
      "29240    0\n",
      "26681    1\n",
      "63014    1\n",
      "23719    1\n",
      "67997    1\n",
      "16014    0\n",
      "4302     0\n",
      "87070    0\n",
      "25963    1\n",
      "11089    0\n",
      "8267     0\n",
      "94345    1\n",
      "        ..\n",
      "17881    0\n",
      "78513    1\n",
      "22326    0\n",
      "49730    1\n",
      "61626    0\n",
      "52164    0\n",
      "57398    1\n",
      "84308    0\n",
      "83861    1\n",
      "9880     0\n",
      "96140    1\n",
      "30159    1\n",
      "38721    0\n",
      "85093    1\n",
      "51021    1\n",
      "18158    1\n",
      "80416    1\n",
      "75042    1\n",
      "76424    0\n",
      "8323     0\n",
      "3091     0\n",
      "60390    0\n",
      "1676     0\n",
      "70852    1\n",
      "55027    1\n",
      "97767    0\n",
      "31351    1\n",
      "98725    1\n",
      "49039    1\n",
      "72960    1\n",
      "Name: sen, Length: 74991, dtype: int64\n",
      "11111               [quot, heart, quot, like, stuck, hear]\n",
      "5078     [already, contemplating, skipping, church, hea...\n",
      "20029    [hayley, ace, well, sneaky, way, get, fit, fee...\n",
      "70187                        [good, pussy, dat, den, neva]\n",
      "89492    [nt, usually, really, good, making, sure, pale...\n",
      "51725           [ashleytisdale, congratulations, award, x]\n",
      "45990    [anukaisa, every, element, miyazaki, films, co...\n",
      "53562    [babycakesjase, chaos, amp, packing, moving, w...\n",
      "31009    [adventseer, nope, madam, reds, leavehaha, seb...\n",
      "76743                               [buzzedition, weekend]\n",
      "34358           [allriseup, voice, people, favorite, song]\n",
      "21810                                       [hello, steve]\n",
      "50973    [astroglidebrand, funny, shit, tho, say, astro...\n",
      "8810     [aaahhhhhh, excited, pushi, home, okay, aawww,...\n",
      "26928    [aeoth, anzmoneymanager, might, pick, thread, ...\n",
      "81952      [ashleydough, oh, headphones, nt, work, either]\n",
      "3601                  [im, fucking, sick, trying, failing]\n",
      "57022    [bartka, sure, wearing, pants, look, like, chi...\n",
      "29240    [acidnation, stop, angryfacing, shoulda, bough...\n",
      "26681    [aakomas, thank, missed, yes, right, thanks, r...\n",
      "63014    [blogsforbraces, lol, protect, updates, care, ...\n",
      "23719    [hate, say, im, tired, east, coast, im, gunna,...\n",
      "67997                             [booknutdc, would, read]\n",
      "16014                                       [almost, work]\n",
      "4302                                [shasta, magic, baaad]\n",
      "87070    [charleneortiz, miss, u, nt, chat, long, time,...\n",
      "25963    [got, faith, frog, community, might, finally, ...\n",
      "11089    [quot, wat, zit, da, euhl, hoe, lang, zit, dat...\n",
      "8267     [whereisjoeymcintyre, went, dance, like, left,...\n",
      "94345                                    [cosmicblaze, ok]\n",
      "                               ...                        \n",
      "17881    [home, hit, sacklong, day, bummed, paris, nt, ...\n",
      "78513    [ariaajaeger, good, blocked, lot, bogus, follo...\n",
      "22326    [yeah, especially, get, mess, far, home, fun, ...\n",
      "49730        [ashleytisdale, excited, watch, hope, u, win]\n",
      "61626    [probs, publish, gig, list, thursday, cardboar...\n",
      "52164                   [going, save, sandra, drowns, lot]\n",
      "57398    [amosabrahams, quot, ur, really, fantastic, wr...\n",
      "84308                               [choley, oh, happened]\n",
      "83861    [aku, ga, terlalu, suka, mbak, fringe, definit...\n",
      "9880     [quot, hey, quot, music, choice, spelled, mitc...\n",
      "96140    [clarelancaster, uber, fun, love, knowing, nex...\n",
      "30159                     [adamlindsay, absinthe, elitist]\n",
      "38721    [amrosario, newfresh, gas, ca, nt, hurt, gon, ...\n",
      "85093                                       [yup, already]\n",
      "51021                                               [kind]\n",
      "18158    [crc, funny, good, birthday, today, proud, kep...\n",
      "80416                            [lol, plain, white, seat]\n",
      "75042                                          [hello, ya]\n",
      "76424    [butadream, yeah, bit, coughing, mucus, genera...\n",
      "8323                                        [afraid, http]\n",
      "3091                                        [want, kisses]\n",
      "60390    [benjaminjfrost, threw, away, muffin, pains, a...\n",
      "1676                                   [didnt, calld, dad]\n",
      "70852                 [bwestmusic, dat, would, nice, papa]\n",
      "55027            [aznativegrandma, thanks, follow, friday]\n",
      "97767    [collectiblestv, unless, jewellery, becomes, c...\n",
      "31351              [adorkablegeek, saw, loved, want, news]\n",
      "98725     [countersuicide, thank, hopefully, wo, nt, need]\n",
      "49039                   [ashlaws, know, say, great, minds]\n",
      "72960    [brittanitaylor, juno, mean, girls, dresses, b...\n",
      "Name: sen_text, Length: 74991, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...enalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (3, 5)], 'tfidfvectorizer__max_features': [25, 50, 100, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params =  {'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "           'tfidfvectorizer__max_features' : [50, 100, None],\n",
    "          }\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                     LogisticRegression(solver='lbfgs',\n",
    "                     max_iter=200))\n",
    "\n",
    "log_grid_cv = GridSearchCV(pipe, params, cv=3, scoring='roc_auc')\n",
    "log_grid_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'tfidfvectorizer__max_features': None, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "Train ROCAUC: 0.89\n",
      "Test ROCAUC: 0.76\n"
     ]
    }
   ],
   "source": [
    "print ('Best Params', log_grid_cv.best_params_)\n",
    "\n",
    "print(f'Train ROCAUC: {roc_auc_score(log_grid_cv.predict(X_train), y_train):.2f}')\n",
    "print(f'Test ROCAUC: {roc_auc_score(log_grid_cv.predict(X_test), y_test):.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params =  {'tfidfvectorizer__ngram_range' : [(1,2)],\n",
    "           'tfidfvectorizer__max_features' : [None],\n",
    "          }\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                     MultinomialNB())\n",
    "\n",
    "nb_grid_cv = GridSearchCV(pipe, params, cv=3, scoring='roc_auc')\n",
    "nb_grid_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROCAUC: 0.96\n",
      "Test ROCAUC: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(f'Train ROCAUC: {roc_auc_score(nb_grid_cv.predict(X_train), y_train):.2f}')\n",
    "print(f'Test ROCAUC: {roc_auc_score(nb_grid_cv.predict(X_test), y_test):.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(df.sen_text, window=3, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sad',\n",
       " 'friend',\n",
       " 'missed',\n",
       " 'new',\n",
       " 'moon',\n",
       " 'trailer',\n",
       " 'omg',\n",
       " 'already',\n",
       " 'im',\n",
       " 'sooo',\n",
       " 'gunna',\n",
       " 'cry',\n",
       " 'dentist',\n",
       " 'since',\n",
       " 'get',\n",
       " 'crown',\n",
       " 'put',\n",
       " 'think',\n",
       " 'mi',\n",
       " 'bf',\n",
       " 'cheating',\n",
       " 'tt',\n",
       " 'worry',\n",
       " 'much',\n",
       " 'chillin',\n",
       " 'sunny',\n",
       " 'work',\n",
       " 'tomorrow',\n",
       " 'tv',\n",
       " 'tonight',\n",
       " 'handed',\n",
       " 'uniform',\n",
       " 'today',\n",
       " 'miss',\n",
       " 'hmmmm',\n",
       " 'wonder',\n",
       " 'number',\n",
       " 'must',\n",
       " 'positive',\n",
       " 'thanks',\n",
       " 'haters',\n",
       " 'face',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'sucked',\n",
       " 'far',\n",
       " 'jb',\n",
       " 'isnt',\n",
       " 'showing',\n",
       " 'australia',\n",
       " 'ok',\n",
       " 'thats',\n",
       " 'win',\n",
       " 'lt',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'right',\n",
       " 'man',\n",
       " 'completely',\n",
       " 'useless',\n",
       " 'rt',\n",
       " 'funny',\n",
       " 'twitter',\n",
       " 'http',\n",
       " 'feeling',\n",
       " 'strangely',\n",
       " 'fine',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'go',\n",
       " 'listen',\n",
       " 'celebrate',\n",
       " 'huge',\n",
       " 'roll',\n",
       " 'thunder',\n",
       " 'scary',\n",
       " 'cut',\n",
       " 'beard',\n",
       " 'growing',\n",
       " 'well',\n",
       " 'year',\n",
       " 'start',\n",
       " 'happy',\n",
       " 'meantime',\n",
       " 'iran',\n",
       " 'one',\n",
       " 'see',\n",
       " 'cause',\n",
       " 'else',\n",
       " 'following',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'level',\n",
       " 'writing',\n",
       " 'massive',\n",
       " 'blog',\n",
       " 'tweet',\n",
       " 'myspace',\n",
       " 'comp',\n",
       " 'shut',\n",
       " 'lost',\n",
       " 'lays',\n",
       " 'position',\n",
       " 'headed',\n",
       " 'pull',\n",
       " 'golf',\n",
       " 'place',\n",
       " 'something',\n",
       " 'yeah',\n",
       " 'boring',\n",
       " 'whats',\n",
       " 'wrong',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'ca',\n",
       " 'nt',\n",
       " 'bothered',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'spend',\n",
       " 'rest',\n",
       " 'life',\n",
       " 'sat',\n",
       " 'going',\n",
       " 'gigs',\n",
       " 'seriously',\n",
       " 'like',\n",
       " 'shit',\n",
       " 'really',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'nooo',\n",
       " 'hours',\n",
       " 'dancing',\n",
       " 'art',\n",
       " 'assignment',\n",
       " 'finish',\n",
       " 'goodbye',\n",
       " 'exams',\n",
       " 'hello',\n",
       " 'alcohol',\n",
       " 'realize',\n",
       " 'deep',\n",
       " 'geez',\n",
       " 'give',\n",
       " 'girl',\n",
       " 'warning',\n",
       " 'atleast',\n",
       " 'hate',\n",
       " 'appears',\n",
       " 'tear',\n",
       " 'live',\n",
       " 'television',\n",
       " 'guys',\n",
       " 'wearing',\n",
       " 'skinny',\n",
       " 'jeans',\n",
       " 'cute',\n",
       " 'sweater',\n",
       " 'heels',\n",
       " 'sure',\n",
       " 'meet',\n",
       " 'meat',\n",
       " 'moving',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " 'need',\n",
       " 'days',\n",
       " 'week',\n",
       " 'dont',\n",
       " 'room',\n",
       " 'sick',\n",
       " 'wardrobe',\n",
       " 'cant',\n",
       " 'till',\n",
       " 'walk',\n",
       " 'yay',\n",
       " 'sox',\n",
       " 'great',\n",
       " 'times',\n",
       " 'million',\n",
       " 'uploading',\n",
       " 'pictures',\n",
       " 'type',\n",
       " 'spaz',\n",
       " 'downloads',\n",
       " 'virus',\n",
       " 'brother',\n",
       " 'msn',\n",
       " 'fucked',\n",
       " 'forever',\n",
       " 'amp',\n",
       " 'babes',\n",
       " 'wrote',\n",
       " 'last',\n",
       " 'got',\n",
       " 'call',\n",
       " 'someone',\n",
       " 'york',\n",
       " 'office',\n",
       " 'enough',\n",
       " 'said',\n",
       " 'even',\n",
       " 'say',\n",
       " 'anyways',\n",
       " 'chris',\n",
       " 'cornell',\n",
       " 'chicago',\n",
       " 'health',\n",
       " 'class',\n",
       " 'joke',\n",
       " 'show',\n",
       " 'makes',\n",
       " 'look',\n",
       " 'reality',\n",
       " 'time',\n",
       " 'low',\n",
       " 'shall',\n",
       " 'motivation',\n",
       " 'entertainment',\n",
       " 'complained',\n",
       " 'properly',\n",
       " 'experiment',\n",
       " 'melody',\n",
       " 'another',\n",
       " 'lakers',\n",
       " 'neither',\n",
       " 'magic',\n",
       " 'fun',\n",
       " 'bathroom',\n",
       " 'clean',\n",
       " 'enjoyable',\n",
       " 'tasks',\n",
       " 'boom',\n",
       " 'pow',\n",
       " 'proud',\n",
       " 'congrats',\n",
       " 'though',\n",
       " 'david',\n",
       " 'five',\n",
       " 'end',\n",
       " 'july',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'katie',\n",
       " 'concert',\n",
       " 'friends',\n",
       " 'leaving',\n",
       " 'stupid',\n",
       " 'love',\n",
       " 'ur',\n",
       " 'mom',\n",
       " 'hug',\n",
       " 'harry',\n",
       " 'sunday',\n",
       " 'happiness',\n",
       " 'hand',\n",
       " 'u',\n",
       " 'always',\n",
       " 'bend',\n",
       " 'backwards',\n",
       " 'sooooon',\n",
       " 'cody',\n",
       " 'booo',\n",
       " 'seen',\n",
       " 'allergies',\n",
       " 'hair',\n",
       " 'taking',\n",
       " 'public',\n",
       " 'poll',\n",
       " 'hurts',\n",
       " 'earl',\n",
       " 'jersey',\n",
       " 'first',\n",
       " 'hour',\n",
       " 'sytycd',\n",
       " 'night',\n",
       " 'find',\n",
       " 'online',\n",
       " 'fix',\n",
       " 'thought',\n",
       " 'become',\n",
       " 'second',\n",
       " 'choice',\n",
       " 'may',\n",
       " 'friendly',\n",
       " 'lol',\n",
       " 'basil',\n",
       " 'plant',\n",
       " 'wan',\n",
       " 'home',\n",
       " 'church',\n",
       " 'wht',\n",
       " 'make',\n",
       " 'pizza',\n",
       " 'inch',\n",
       " 'guitar',\n",
       " 'generous',\n",
       " 'p',\n",
       " 'x',\n",
       " 'miley',\n",
       " 'tour',\n",
       " 'wanted',\n",
       " 'mean',\n",
       " 'kid',\n",
       " 'stick',\n",
       " 'head',\n",
       " 'fly',\n",
       " 'away',\n",
       " 'slow',\n",
       " 'tix',\n",
       " 'send',\n",
       " 'sunshine',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'swimming',\n",
       " 'beach',\n",
       " 'would',\n",
       " 'happier',\n",
       " 'walls',\n",
       " 'bedroom',\n",
       " 'painted',\n",
       " 'white',\n",
       " 'idk',\n",
       " 'wat',\n",
       " 'trust',\n",
       " 'sorry',\n",
       " 'da',\n",
       " 'pain',\n",
       " 'caused',\n",
       " 'ima',\n",
       " 'take',\n",
       " 'dis',\n",
       " 'straighten',\n",
       " 'luv',\n",
       " 'yall',\n",
       " 'finding',\n",
       " 'banging',\n",
       " 'brain',\n",
       " 'heads',\n",
       " 'come',\n",
       " 'save',\n",
       " 'bed',\n",
       " 'cough',\n",
       " 'cab',\n",
       " 'airport',\n",
       " 'christy',\n",
       " 'gt',\n",
       " 'case',\n",
       " 'emo',\n",
       " 'camp',\n",
       " 'wee',\n",
       " 'bit',\n",
       " 'alr',\n",
       " 'bringing',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'watch',\n",
       " 'world',\n",
       " 'report',\n",
       " 'jonas',\n",
       " 'almost',\n",
       " 'jus',\n",
       " 'fr',\n",
       " 'funeral',\n",
       " 'cried',\n",
       " 'grandpa',\n",
       " 'quot',\n",
       " 'smile',\n",
       " 'cuz',\n",
       " 'graduated',\n",
       " 'longest',\n",
       " 'ever',\n",
       " 'ugh',\n",
       " 'wo',\n",
       " 'let',\n",
       " 'bball',\n",
       " 'game',\n",
       " 'grrr',\n",
       " 'says',\n",
       " 'phone',\n",
       " 'immediately',\n",
       " 'tmobile',\n",
       " 'paying',\n",
       " 'car',\n",
       " 'stolen',\n",
       " 'mother',\n",
       " 'pose',\n",
       " 'hang',\n",
       " 'girls',\n",
       " 'hope',\n",
       " 'movie',\n",
       " 'rats',\n",
       " 'plan',\n",
       " 'guess',\n",
       " 'means',\n",
       " 'two',\n",
       " 'presentations',\n",
       " 'cool',\n",
       " 'oh',\n",
       " 'thank',\n",
       " 'pleased',\n",
       " 'guna',\n",
       " 'soon',\n",
       " 'talkin',\n",
       " 'lose',\n",
       " 'rip',\n",
       " 'eddings',\n",
       " 'rose',\n",
       " 'back',\n",
       " 'xmas',\n",
       " 'special',\n",
       " 'damn',\n",
       " 'half',\n",
       " 'aww',\n",
       " 'neighbor',\n",
       " 'status',\n",
       " 'next',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'daily',\n",
       " 'dose',\n",
       " 'definition',\n",
       " 'reviewed',\n",
       " 'entry',\n",
       " 'decided',\n",
       " 'publish',\n",
       " 'help',\n",
       " 'explains',\n",
       " 'alot',\n",
       " 'sequel',\n",
       " 'better',\n",
       " 'fuck',\n",
       " 'told',\n",
       " 'drink',\n",
       " 'wmy',\n",
       " 'gave',\n",
       " 'bb',\n",
       " 'brought',\n",
       " 'neighbors',\n",
       " 'v',\n",
       " 'sweet',\n",
       " 'bought',\n",
       " 'suit',\n",
       " 'true',\n",
       " 'highly',\n",
       " 'actually',\n",
       " 'favorite',\n",
       " 'character',\n",
       " 'book',\n",
       " 'knw',\n",
       " 'whyy',\n",
       " 'getting',\n",
       " 'hub',\n",
       " 'standing',\n",
       " 'pouring',\n",
       " 'rain',\n",
       " 'sitting',\n",
       " 'top',\n",
       " 'summer',\n",
       " 'sucks',\n",
       " 'went',\n",
       " 'dog',\n",
       " 'vets',\n",
       " 'theyve',\n",
       " 'ear',\n",
       " 'charged',\n",
       " 'us',\n",
       " 'still',\n",
       " 'everytime',\n",
       " 'moves',\n",
       " 'picture',\n",
       " 'naked',\n",
       " 'trains',\n",
       " 'manchester',\n",
       " 'jonasbrothers',\n",
       " 'people',\n",
       " 'picky',\n",
       " 'dice',\n",
       " 'wide',\n",
       " 'awake',\n",
       " 'awful',\n",
       " 'weather',\n",
       " 'stinks',\n",
       " 'comin',\n",
       " 'moe',\n",
       " 'hi',\n",
       " 'stranger',\n",
       " 'frank',\n",
       " 'urself',\n",
       " 'gay',\n",
       " 'asylm',\n",
       " 'panel',\n",
       " 'normal',\n",
       " 'started',\n",
       " 'squarespace',\n",
       " 'brighten',\n",
       " 'bad',\n",
       " 'anything',\n",
       " 'susan',\n",
       " 'boyle',\n",
       " 'didnt',\n",
       " 'diversity',\n",
       " 'good',\n",
       " 'congratz',\n",
       " 'believe',\n",
       " 'goooood',\n",
       " 'graduate',\n",
       " 'paint',\n",
       " 'author',\n",
       " 'lesbian',\n",
       " 'story',\n",
       " 'news',\n",
       " 'unknown',\n",
       " 'error',\n",
       " 'uh',\n",
       " 'iphone',\n",
       " 'os',\n",
       " 'fail',\n",
       " 'experts',\n",
       " 'predict',\n",
       " 'eagles',\n",
       " 'giants',\n",
       " 'bs',\n",
       " 'know',\n",
       " 'hungry',\n",
       " 'lets',\n",
       " 'outside',\n",
       " 'balcony',\n",
       " 'eat',\n",
       " 'driving',\n",
       " 'mad',\n",
       " 'men',\n",
       " 'uk',\n",
       " 'female',\n",
       " 'internet',\n",
       " 'starting',\n",
       " 'business',\n",
       " 'women',\n",
       " 'truth',\n",
       " 'hiding',\n",
       " 'eyes',\n",
       " 'paramore',\n",
       " 'decode',\n",
       " 'try',\n",
       " 'turn',\n",
       " 'inside',\n",
       " 'admit',\n",
       " 'literally',\n",
       " 'coldplay',\n",
       " 'tears',\n",
       " 'kids',\n",
       " 'break',\n",
       " 'dads',\n",
       " 'sighs',\n",
       " 'unsure',\n",
       " 'annoyed',\n",
       " 'via',\n",
       " 'designed',\n",
       " 'gallery',\n",
       " 'exhibition',\n",
       " 'nyt',\n",
       " 'cries',\n",
       " 'potter',\n",
       " 'comes',\n",
       " 'vietnam',\n",
       " 'money',\n",
       " 'mouth',\n",
       " 'headin',\n",
       " 'cheers',\n",
       " 'sigh',\n",
       " 'stalker',\n",
       " 'divorce',\n",
       " 'move',\n",
       " 'lookin',\n",
       " 'around',\n",
       " 'reason',\n",
       " 'atm',\n",
       " 'little',\n",
       " 'atl',\n",
       " 'ily',\n",
       " 'dies',\n",
       " 'laughter',\n",
       " 'soo',\n",
       " 'watching',\n",
       " 'shopping',\n",
       " 'w',\n",
       " 'trent',\n",
       " 'hopefully',\n",
       " 'best',\n",
       " 'weird',\n",
       " 'many',\n",
       " 'rained',\n",
       " 'truck',\n",
       " 'race',\n",
       " 'worse',\n",
       " 'f',\n",
       " 'confusing',\n",
       " 'hot',\n",
       " 'choco',\n",
       " 'shirt',\n",
       " 'panties',\n",
       " 'north',\n",
       " 'due',\n",
       " 'body',\n",
       " 'line',\n",
       " 'missing',\n",
       " 'design',\n",
       " 'council',\n",
       " 'tech',\n",
       " 'transfer',\n",
       " 'event',\n",
       " 'everything',\n",
       " 'hard',\n",
       " 'bueno',\n",
       " 'bye',\n",
       " 'ya',\n",
       " 'amazing',\n",
       " 'twits',\n",
       " 'national',\n",
       " 'production',\n",
       " 'department',\n",
       " 'priority',\n",
       " 'beats',\n",
       " 'reach',\n",
       " 'enjoy',\n",
       " 'venue',\n",
       " 'shine',\n",
       " 'everywhere',\n",
       " 'everyone',\n",
       " 'train',\n",
       " 'crash',\n",
       " 'dc',\n",
       " 'weeks',\n",
       " 'child',\n",
       " 'nancy',\n",
       " 'adbert',\n",
       " 'video',\n",
       " 'wishing',\n",
       " 'ch',\n",
       " 'staying',\n",
       " 'fabulous',\n",
       " 'university',\n",
       " 'happened',\n",
       " 'angels',\n",
       " 'demons',\n",
       " 'lions',\n",
       " 'tigers',\n",
       " 'bears',\n",
       " 'americanwomannn',\n",
       " 'impact',\n",
       " 'arm',\n",
       " 'leg',\n",
       " 'wait',\n",
       " 'manners',\n",
       " 'saw',\n",
       " 'mum',\n",
       " 'fat',\n",
       " 'asses',\n",
       " 'codinghorror',\n",
       " 'three',\n",
       " 'external',\n",
       " 'monitors',\n",
       " 'laptop',\n",
       " 'maybe',\n",
       " 'later',\n",
       " 'c',\n",
       " 'making',\n",
       " 'garlic',\n",
       " 'bread',\n",
       " 'bday',\n",
       " 'pants',\n",
       " 'job',\n",
       " 'fml',\n",
       " 'sweep',\n",
       " 'haha',\n",
       " 'r',\n",
       " 'science',\n",
       " 'cartoons',\n",
       " 'edinburgh',\n",
       " 'fringe',\n",
       " 'monies',\n",
       " 'btw',\n",
       " 'angryfeet',\n",
       " 'blame',\n",
       " 'school',\n",
       " 'done',\n",
       " 'short',\n",
       " 'stack',\n",
       " 'thing',\n",
       " 'latter',\n",
       " 'store',\n",
       " 'comics',\n",
       " 'thinking',\n",
       " 'less',\n",
       " 'chance',\n",
       " 'stays',\n",
       " 'orlando',\n",
       " 'wud',\n",
       " 'beautiful',\n",
       " 'servers',\n",
       " 'backup',\n",
       " 'experience',\n",
       " 'problems',\n",
       " 'delay',\n",
       " 'yes',\n",
       " 'jk',\n",
       " 'gentle',\n",
       " 'breeze',\n",
       " 'blown',\n",
       " 'se',\n",
       " 'joseph',\n",
       " 'future',\n",
       " 'nyc',\n",
       " 'holler',\n",
       " 'cheese',\n",
       " 'burgers',\n",
       " 'nothing',\n",
       " 'kind',\n",
       " 'sort',\n",
       " 'pick',\n",
       " 'flowers',\n",
       " 'teacher',\n",
       " 'jealous',\n",
       " 'without',\n",
       " 'hospital',\n",
       " 'mas',\n",
       " 'security',\n",
       " 'chain',\n",
       " 'strong',\n",
       " 'link',\n",
       " 'youtube',\n",
       " 'account',\n",
       " 'suspended',\n",
       " 'single',\n",
       " 'ladies',\n",
       " 'dance',\n",
       " 'sold',\n",
       " 'k',\n",
       " 'hoping',\n",
       " 'nervous',\n",
       " 'aim',\n",
       " 'talk',\n",
       " 'followers',\n",
       " 'grocery',\n",
       " 'oldies',\n",
       " 'sugar',\n",
       " 'ray',\n",
       " 'convo',\n",
       " 'keep',\n",
       " 'roomies',\n",
       " 'catholic',\n",
       " 'pork',\n",
       " 'fridge',\n",
       " 'bitches',\n",
       " 'wake',\n",
       " 'early',\n",
       " 'la',\n",
       " 'opera',\n",
       " 'kay',\n",
       " 'etc',\n",
       " 'bother',\n",
       " 'gum',\n",
       " 'lift',\n",
       " 'surgery',\n",
       " 'tho',\n",
       " 'eek',\n",
       " 'tonite',\n",
       " 'hates',\n",
       " 'app',\n",
       " 'bhai',\n",
       " 'used',\n",
       " 'ka',\n",
       " 'chilling',\n",
       " 'blackberry',\n",
       " 'unlimited',\n",
       " 'every',\n",
       " 'pop',\n",
       " 'quite',\n",
       " 'active',\n",
       " 'shame',\n",
       " 'mamma',\n",
       " 'mia',\n",
       " 'horny',\n",
       " 'island',\n",
       " 'letting',\n",
       " 'check',\n",
       " 'knowledge',\n",
       " 'referring',\n",
       " 'hey',\n",
       " 'thx',\n",
       " 'ttyl',\n",
       " 'sowwy',\n",
       " 'mothers',\n",
       " 'knew',\n",
       " 'sharepoint',\n",
       " 'written',\n",
       " 'jamie',\n",
       " 'feels',\n",
       " 'bowling',\n",
       " 'ball',\n",
       " 'heart',\n",
       " 'badly',\n",
       " 'fs',\n",
       " 'keeps',\n",
       " 'crashing',\n",
       " 'load',\n",
       " 'plane',\n",
       " 'current',\n",
       " 'dad',\n",
       " 'mood',\n",
       " 'currently',\n",
       " 'blah',\n",
       " 'parking',\n",
       " 'lot',\n",
       " 'fed',\n",
       " 'matters',\n",
       " 'worst',\n",
       " 'super',\n",
       " 'nasty',\n",
       " 'rainy',\n",
       " 'yuck',\n",
       " 'aiden',\n",
       " 'davis',\n",
       " 'bummed',\n",
       " 'dias',\n",
       " 'para',\n",
       " 'drawing',\n",
       " 'fake',\n",
       " 'stayin',\n",
       " 'lawn',\n",
       " 'tanning',\n",
       " 'play',\n",
       " 'camera',\n",
       " 'settle',\n",
       " 'playing',\n",
       " 'iv',\n",
       " 'drive',\n",
       " 'display',\n",
       " 'upside',\n",
       " 'photos',\n",
       " 'bluetooth',\n",
       " 'computer',\n",
       " 'effort',\n",
       " 'bring',\n",
       " 'content',\n",
       " 'added',\n",
       " 'lease',\n",
       " 'release',\n",
       " 'digital',\n",
       " 'tight',\n",
       " 'response',\n",
       " 'j',\n",
       " 'kinda',\n",
       " 'moment',\n",
       " 'silence',\n",
       " 'childhood',\n",
       " 'homes',\n",
       " 'sadly',\n",
       " 'replaced',\n",
       " 'robotpickuplines',\n",
       " 'hilarious',\n",
       " 'punch',\n",
       " 'b',\n",
       " 'n',\n",
       " 'crew',\n",
       " 'worth',\n",
       " 'storms',\n",
       " 'satisfying',\n",
       " 'loves',\n",
       " 'god',\n",
       " 'friday',\n",
       " 'ti',\n",
       " 'connection',\n",
       " 'died',\n",
       " 'mobile',\n",
       " 'thrilled',\n",
       " 'hearts',\n",
       " 'ringtone',\n",
       " 'song',\n",
       " 'trackle',\n",
       " 'apple',\n",
       " 'card',\n",
       " 'helps',\n",
       " 'huh',\n",
       " 'forgot',\n",
       " 'hassle',\n",
       " 'music',\n",
       " 'exhausted',\n",
       " 'ben',\n",
       " 'cake',\n",
       " 'batter',\n",
       " 'somebody',\n",
       " 'ish',\n",
       " 'tlk',\n",
       " 'long',\n",
       " 'pacific',\n",
       " 'ocean',\n",
       " 'open',\n",
       " 'door',\n",
       " 'throw',\n",
       " 'pc',\n",
       " 'hell',\n",
       " 'bbc',\n",
       " 'canucks',\n",
       " 'huhuhu',\n",
       " 'made',\n",
       " 'nice',\n",
       " 'twice',\n",
       " 'brilliant',\n",
       " 'xd',\n",
       " 'lonely',\n",
       " 'blue',\n",
       " 'summertime',\n",
       " 'ill',\n",
       " 'fuckin',\n",
       " 'goin',\n",
       " 'portugal',\n",
       " 'drops',\n",
       " 'ye',\n",
       " 'eh',\n",
       " 'leave',\n",
       " 'mexico',\n",
       " 'maths',\n",
       " 'sounds',\n",
       " 'leadership',\n",
       " 'considering',\n",
       " 'library',\n",
       " 'cuts',\n",
       " 'tax',\n",
       " 'sake',\n",
       " 'things',\n",
       " 'town',\n",
       " 'netball',\n",
       " 'match',\n",
       " 'baby',\n",
       " 'member',\n",
       " 'family',\n",
       " 'bedtime',\n",
       " 'cold',\n",
       " 'chills',\n",
       " 'goes',\n",
       " 'books',\n",
       " 'results',\n",
       " 'poor',\n",
       " 'lil',\n",
       " 'enjoying',\n",
       " 'big',\n",
       " 'rat',\n",
       " 'dbl',\n",
       " 'standard',\n",
       " 'yep',\n",
       " 'epic',\n",
       " 'magazine',\n",
       " 'broke',\n",
       " 'tiger',\n",
       " 'woods',\n",
       " 'wifey',\n",
       " 'sux',\n",
       " 'wondering',\n",
       " 'lata',\n",
       " 'gather',\n",
       " 'hay',\n",
       " 'found',\n",
       " 'boy',\n",
       " 'stay',\n",
       " 'master',\n",
       " 'holy',\n",
       " 'iranelection',\n",
       " 'realizes',\n",
       " 'bodies',\n",
       " 'air',\n",
       " 'france',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'noooooooooo',\n",
       " 'follow',\n",
       " 'hear',\n",
       " 'asking',\n",
       " 'hardest',\n",
       " 'working',\n",
       " 'chica',\n",
       " 'mii',\n",
       " 'boss',\n",
       " 'lady',\n",
       " 'condition',\n",
       " 'series',\n",
       " 'phoenix',\n",
       " 'boston',\n",
       " 'cleveland',\n",
       " 'teams',\n",
       " 'played',\n",
       " 'terrible',\n",
       " 'soccer',\n",
       " 'basketball',\n",
       " 'mustache',\n",
       " 'desperate',\n",
       " 'bout',\n",
       " 'ahmier',\n",
       " 'marco',\n",
       " ...]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.8317605257034302),\n",
       " ('sent', 0.8304129838943481),\n",
       " ('email', 0.8274568915367126),\n",
       " ('dm', 0.8265565633773804),\n",
       " ('link', 0.8208439350128174)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('twitter', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eventually', 0.9904677867889404),\n",
       " ('texts', 0.9903397560119629),\n",
       " ('boss', 0.9896416664123535),\n",
       " ('doctor', 0.9893132448196411),\n",
       " ('faint', 0.9890397191047668)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hmmmm\n",
    "model.wv.most_similar('drunk', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('somewhere', 0.9852827787399292),\n",
       " ('swim', 0.981758177280426),\n",
       " ('near', 0.9813928604125977),\n",
       " ('focused', 0.9805941581726074),\n",
       " ('nap', 0.9804528951644897)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('swimming', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
