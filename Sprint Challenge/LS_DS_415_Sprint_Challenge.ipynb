{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "df = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data dimensions.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4462\n",
       "4    2185\n",
       "1    1496\n",
       "3    1098\n",
       "2     759\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display range and distribution of star ratings.\n",
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Accepts one document at a time; returns a list of tokens.\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) \\\n",
    "        and ((token.pos_!= 'PRON') and token.like_num == False):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(column):\n",
    "    \"\"\"\n",
    "    Removes dollar signs and reduces any excess whitespace to a single\n",
    "    space per instance.\n",
    "    \"\"\"\n",
    "    column = column.copy()\n",
    "    \n",
    "    # Remove dollar signs.\n",
    "    column = column.str.replace('$', '')\n",
    "    \n",
    "    # Reduce any excess whitespace to a single space per instance.\n",
    "    column = column.str.replace(r'\\s+', ' ')\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and clean text.\n",
    "df['text'] = clean(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tokens from text.\n",
    "tokens = df['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beware',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'small',\n",
       " 'business',\n",
       " 'Los',\n",
       " 'Alamitos',\n",
       " 'receive',\n",
       " 'look',\n",
       " 'like',\n",
       " 'legitimate',\n",
       " 'bill',\n",
       " 'account',\n",
       " 'number',\n",
       " 'call',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'list',\n",
       " 'wait',\n",
       " 'time',\n",
       " 'hold',\n",
       " 'say',\n",
       " 'minute',\n",
       " 'leave',\n",
       " 'message',\n",
       " 'live',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'matter',\n",
       " 'number',\n",
       " 'select',\n",
       " 'leave',\n",
       " 'firm',\n",
       " 'message',\n",
       " 'contact',\n",
       " 'BBB',\n",
       " 'attorney',\n",
       " 'company',\n",
       " 'try',\n",
       " 'scam',\n",
       " 'business',\n",
       " 'illegal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview tokens.\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector representation of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>affordable</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>...</th>\n",
       "      <th>wrap</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>year ago</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 774 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    30  able  absolutely  accommodate  actually  add  affordable  afternoon  \\\n",
       "0  0.0   0.0         0.0          0.0  0.000000  0.0         0.0        0.0   \n",
       "1  0.0   0.0         0.0          0.0  0.000000  0.0         0.0        0.0   \n",
       "2  0.0   0.0         0.0          0.0  0.000000  0.0         0.0        0.0   \n",
       "3  0.0   0.0         0.0          0.0  0.254845  0.0         0.0        0.0   \n",
       "4  0.0   0.0         0.0          0.0  0.000000  0.0         0.0        0.0   \n",
       "\n",
       "   ago  agree  ...  wrap  write  wrong      year  year ago  yelp  yes  \\\n",
       "0  0.0    0.0  ...   0.0    0.0    0.0  0.000000       0.0   0.0  0.0   \n",
       "1  0.0    0.0  ...   0.0    0.0    0.0  0.000000       0.0   0.0  0.0   \n",
       "2  0.0    0.0  ...   0.0    0.0    0.0  0.095275       0.0   0.0  0.0   \n",
       "3  0.0    0.0  ...   0.0    0.0    0.0  0.000000       0.0   0.0  0.0   \n",
       "4  0.0    0.0  ...   0.0    0.0    0.0  0.000000       0.0   0.0  0.0   \n",
       "\n",
       "      young  yum  yummy  \n",
       "0  0.000000  0.0    0.0  \n",
       "1  0.000000  0.0    0.0  \n",
       "2  0.146312  0.0    0.0  \n",
       "3  0.000000  0.0    0.0  \n",
       "4  0.000000  0.0    0.0  \n",
       "\n",
       "[5 rows x 774 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate vectorizer object.\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.98,\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "# Create a vocabulary and get word counts per document.\n",
    "dtm = tfidf.fit_transform(tokens.astype(str))\n",
    "\n",
    "# Get feature names to use as dataframe column headers.\n",
    "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# Preview feature matrix.\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate nearest neighbors model.\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "# Fit on TF-IDF Vectors\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a fake review and query for the 10 most similiar reviews\n",
    "\n",
    "Print the text of the reviews. Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake negative review.\n",
    "bad_review = ('Boy, the food at this place is really terrible, and '\n",
    "              'such small portions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize fake review.\n",
    "bad_vec = tfidf.transform([bad_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 10 most similar reviews.\n",
    "bad_distances, bad_indices = nn.kneighbors(bad_vec.todense(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>1</td>\n",
       "      <td>Terrible terrible terrible went to get a \"hot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>1</td>\n",
       "      <td>Terrible airline. They have nothing to offer a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>1</td>\n",
       "      <td>This place just sucks. Food was terrible. Serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>1</td>\n",
       "      <td>Terrible.. absolutely terrible. Dr. Hinds is e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>1</td>\n",
       "      <td>Yuk! Terrible pizza! I think I got food poison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't shop here. The owner John Menard is PURE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>2</td>\n",
       "      <td>Terrible terrible terrible. After they lost my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>5</td>\n",
       "      <td>旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。 質問にも丁寧に...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>5</td>\n",
       "      <td>天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>1</td>\n",
       "      <td>Fries were burnt to shit. Jalapeños were so bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                                               text\n",
       "2627      1  Terrible terrible terrible went to get a \"hot ...\n",
       "2135      1  Terrible airline. They have nothing to offer a...\n",
       "3230      1  This place just sucks. Food was terrible. Serv...\n",
       "5298      1  Terrible.. absolutely terrible. Dr. Hinds is e...\n",
       "7006      1  Yuk! Terrible pizza! I think I got food poison...\n",
       "5366      1  Don't shop here. The owner John Menard is PURE...\n",
       "4673      2  Terrible terrible terrible. After they lost my...\n",
       "6204      5  旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。 質問にも丁寧に...\n",
       "6311      5  天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用...\n",
       "1104      1  Fries were burnt to shit. Jalapeños were so bu..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print star ratings and abbreviated text of similar reviews.\n",
    "df.iloc[bad_indices[0]][['stars', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrible terrible terrible went to get a \"hot and ready pizza and box\" which is supposed to be ready between the hours of 4 and 8 p.m. at 7:24 p.m. here it is 8 p.m. and I still have not received my order which I was told would be ready in 12 to 15 minutes this place is terrible terrible terrible and I will no longer be spending my money at this particular little Little Caesars if any Little Caesars Domino's here I come\n",
      "\n",
      "Terrible airline. They have nothing to offer accept a ride. Why is it that airlines have terrible business models? Virgin America is my favorite. We need more Airlines like them.\n",
      "\n",
      "This place just sucks. Food was terrible. Service was even worse some how. Just skip it all together.\n",
      "\n",
      "Terrible.. absolutely terrible. Dr. Hinds is especially rude, very difficult to get anyone to return a phone call, just overall terrible patient care.\n",
      "\n",
      "Yuk! Terrible pizza! I think I got food poisoning!! Yuk! Yuk! Yuk! Yuk! Yuk! Yuk! Yik! Still Yuk!!!!!@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Yikes yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk yuk\n",
      "\n",
      "Don't shop here. The owner John Menard is PURE EVIL... Menards is a terrible place to work, not a good corporate citizen, and a terrible customer for its' vendors. Spend your money in a local hardware store.\n",
      "\n",
      "Terrible terrible terrible. After they lost my paper work, I had to wait 2 and half hours after my scheduled appointment to be seen. No one apologized and the receptionist was down right nasty. The tech who did my MRI was awesome and friendly and is the only reason they get 2 stars. Go to a different location!\n",
      "\n",
      "旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。 質問にも丁寧に答えてくれましたし、日本人の方も日本語が話せる方も居て、とても綺麗で居心地のいいお店でした。 ネイルはちはるさんと言う綺麗な方が丁寧にしてくれとても気に入りました。 予定になかったまつ毛エクステもお願いし、日本ではまだあまりないブラウンカラーのエクステをしてもらい、とても気に入りました。 また是非マッサージなどで伺いたいと思います。\n",
      "\n",
      "天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用心製作，包含擺盤精緻、佐料衛生，夏日想開胃，這是一個不錯的選擇，服務人員也很敬業，以後會常常來\n",
      "\n",
      "Fries were burnt to shit. Jalapeños were so burnt they were black. Service was terrible. Over priced for nasty food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print full text of similar reviews.\n",
    "for text in df.iloc[bad_indices[0]]['text']:\n",
    "    print(f'{text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Unsurprisingly, our model does not handle non-English reviews well. With that exception noted, however, it otherwise does seem to do well at identifying clearly negative reviews.\n",
    "\n",
    "On the other hand, the model does not seem to reliably pick up on the type of business being reviewed - we have an airline, a couple of medical providers, a hardware store, and several restaurant reviews all lumped together here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pipeline\n",
    "\n",
    "Create a pipeline object with a sklearn CountVectorizer or TfidfVectorizer and any sklearn classifier. Use that pipeline to estimate a model to predict stars. Use the pipeline to predict a star rating for your fake review from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline components.\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline.\n",
    "pipe = Pipeline([('vect', vectorizer), \n",
    "                 ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pipeline to data.\n",
    "pipe.fit(tokens.astype(str), df['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the pipeline to predict a star rating for a fake review.\n",
    "pipe.predict([bad_review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the pipeline predicts the lowest possible rating for this clearly negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Pipeline\n",
    "\n",
    "Tune the entire pipeline with a GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space for exploration.\n",
    "parameters = {\n",
    "    'vect__max_df': (0.95, 0.98),\n",
    "    'vect__min_df': (0.02, 0.05),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'clf__n_neighbors': (5, 10),\n",
    "    'clf__weights':('uniform', 'distance')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                             n_neighbors=5, p=2,\n",
       "                                                             weights='uniform'))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__n_neighbors': (5, 10),\n",
       "                         'clf__weights': ('uniform', 'distance'),\n",
       "                         'vect__max_df': (0.95, 0.98),\n",
       "                         'vect__max_features': (500, 1000),\n",
       "                         'vect__min_df': (0.02, 0.05),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search parameter space for best model.\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(tokens.astype(str), df['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4892"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get accuracy of best model.\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4462"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to baseline accuracy.\n",
    "accuracy_score([5]*len(df['stars']), df['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty terrible! How wrong _are_ our mis-classified star ratings, though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                             weights='uniform'))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__n_neighbors': (5, 10),\n",
       "                         'clf__weights': ('uniform', 'distance'),\n",
       "                         'vect__max_df': (0.95, 0.98),\n",
       "                         'vect__max_features': (500, 1000),\n",
       "                         'vect__min_df': (0.02, 0.05),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-search grid, but penalize degree of error.\n",
    "grid_search_mae = GridSearchCV(pipe, \n",
    "                               parameters, \n",
    "                               scoring='neg_mean_absolute_error', \n",
    "                               cv=3, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=1)\n",
    "grid_search_mae.fit(tokens.astype(str), df['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get MAE of best model.\n",
    "abs(grid_search_mae.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on average, we're off by about one star. Not great, but not _terrible_ either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
