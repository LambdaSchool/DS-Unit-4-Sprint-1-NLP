{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS42SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarrinan/DS-Unit-4-Sprint-2-NLP/blob/master/sc/DS42SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QpPcbYew_ttN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Working with Text Data\n",
        "\n",
        "### Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "metadata": {
        "id": "dtotEnsStY5o",
        "colab_type": "code",
        "outputId": "3a567586-fd73-482c-de2b-7c0bd35b505d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "\n",
        "print(whitespace_string)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G9-MkBwasXx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1aee06b0-a18f-49a4-d041-2fdffe15a0f5"
      },
      "cell_type": "code",
      "source": [
        " ##### Your Code Here #####\n",
        "print(' '.join(whitespace_string.split()))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a string that has a lot of extra whitespace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vg1-d2aAsXLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "metadata": {
        "id": "KWDiN4C9_0sq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "9640151a-bf31-475f-cdd2-af39be137448"
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "import requests\n",
        "import re\n",
        "\n",
        "data = requests.get('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt')\n",
        "dates = data.text.replace('\\r', '').split('\\n')\n",
        "\n",
        "months = [re.findall(r'[A-Z][a-z]+', date)[0] for date in dates]\n",
        "days = [re.findall(r'[\\d]{1,2},', date)[0][:-1] for date in dates]\n",
        "years = [re.findall(r'[\\d]{4}', date)[0] for date in dates]\n",
        "df_dates = pd.DataFrame({'Month' : months,\n",
        "                        'Day' : days,\n",
        "                        'Year' : years})\n",
        "df_dates.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Day  Month  Year\n",
              "0   8  March  2015\n",
              "1  15  March  2015\n",
              "2  22  March  2015\n",
              "3  29  March  2015\n",
              "4   5  April  2015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "s4Q0dgoe_uBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "metadata": {
        "id": "1xzdhyTS_3F9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "d3c0182d-84a9-490a-a806-b6f74d31875f"
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "import pandas as pd\n",
        "twitters = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv')\n",
        "twitters.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7:30 :O\n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4          0           i think mi bf is cheating on me!!!   ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "qeDGPp19kRFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "twitters.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rErKrhyxc0-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pip install -U gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2L-KDsGncxpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "49816e2e-9961-433c-c1fc-ed0017c5ab0f"
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "NdIn6QrHcoaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = twitters['SentimentText']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnyfFLQmcg_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a7c9625d-bd75-4caf-fd8e-32aa7860480b"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  \n",
        "  doc = str(doc).lower()\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  \n",
        "\t# remove punctuation from each token\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  \n",
        "  \n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  #filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "sentences = [clean_doc(tweet) for tweet in text]\n",
        "print(sentences[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['sad', 'apl', 'friend'], ['missed', 'new', 'moon', 'trailer'], ['omg', 'already'], ['omgaga', 'im', 'sooo', 'im', 'gunna', 'cry', 'ive', 'dentist', 'since', 'suposed', 'get', 'crown', 'put'], ['think', 'mi', 'bf', 'cheating', 'tt'], ['worry', 'much'], ['juuuuuuuuuuuuuuuuussssst', 'chillin'], ['sunny', 'work', 'tomorrow', 'tv', 'tonight'], ['handed', 'uniform', 'today', 'miss', 'already'], ['hmmmm', 'wonder', 'number']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q764vszGqiUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "metadata": {
        "id": "e2Ji7BMhqs3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**: \n",
        "Term Frequency (percentage of words in document for each word) - Inverse Document Frequency (a penalty for the word existing in a high number of documents).\n",
        "\n",
        "The purpose of TF-IDF is to find what is _unique_ to each document. Because of this we will penalize the term frequencies of words that are common across all documents which will allow for each document's most different topics to rise to the top. It calculates how many times the word appears accross all documents and find a proportion 1/number of times. The within each document it multiplies that proportion for each occurence of that word"
      ]
    },
    {
      "metadata": {
        "id": "3iUeBKtG_uEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TX8OEgUP_3ee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2qdbRJ_5fUpy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "twitters['text'] = sentences\n",
        "twitters.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cMpZssSaguUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "twitters['text'] = [' '.join(tweet) for tweet in twitters['text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhE116QpfCMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "5a7de883-fae9-4221-b697-ee9465d2a293"
      },
      "cell_type": "code",
      "source": [
        "#split the data for the df\n",
        "def split_data(df, target):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X, y = df.text, df[target]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "#separate data into training and test\n",
        "X_train, X_test, y_train, y_test = split_data(twitters, 'Sentiment')\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79991,)\n",
            "(19998,)\n",
            "(79991,)\n",
            "(19998,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRKvpGvOgZqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4NU4-nrgUGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "25f3613e-4474-4779-f699-05bb5a22bc57"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=200, ngram_range=(1,1), stop_words='english')\n",
        "\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "train_word_counts = vectorizer.transform(X_train)\n",
        "\n",
        "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(X_train_vectorized.shape)\n",
        "X_train_vectorized.head()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79991, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actually</th>\n",
              "      <th>amazing</th>\n",
              "      <th>amp</th>\n",
              "      <th>away</th>\n",
              "      <th>awesome</th>\n",
              "      <th>aww</th>\n",
              "      <th>awww</th>\n",
              "      <th>baby</th>\n",
              "      <th>bad</th>\n",
              "      <th>bed</th>\n",
              "      <th>...</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrong</th>\n",
              "      <th>xx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>youre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   actually  amazing  amp  away  awesome  aww  awww  baby  bad  bed  ...    \\\n",
              "0       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "1       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "2       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "3       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "4       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "\n",
              "   wow  wrong   xx   ya  yay  yeah  year  yes  yesterday  youre  \n",
              "0  0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "1  0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "2  0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "3  0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "4  0.0    0.0  0.0  0.0  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "h8s_v0fwh1Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8911e56d-535d-4562-b0e3-35e82d93a4c8"
      },
      "cell_type": "code",
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sleep': 136, 'watching': 179, 'school': 131, 'haha': 56, 'love': 95, 'music': 106, 'little': 86, 'gotta': 51, 'dont': 26, 'know': 79, 'working': 189, 'wanna': 174, 'hear': 62, 'cute': 20, 'like': 85, 'friend': 39, 'got': 50, 'video': 172, 'ill': 74, 'week': 181, 'time': 158, 'long': 89, 'friends': 40, 'thing': 153, 'good': 49, 'night': 110, 'youre': 199, 'way': 180, 'thats': 151, 'today': 160, 'yes': 197, 'cool': 19, 'things': 154, 'hot': 70, 'right': 126, 'lol': 88, 'new': 108, 'nice': 109, 'happy': 58, 'day': 22, 'im': 75, 'sad': 127, 'come': 17, 'makes': 98, 'yeah': 195, 'doesnt': 25, 'think': 155, 'old': 114, 'far': 30, 'whats': 185, 'theres': 152, 'really': 125, 'want': 175, 'bad': 8, 'miss': 102, 'wont': 187, 'help': 64, 'following': 36, 'amp': 2, 'twitter': 169, 'looks': 92, 'gonna': 48, 'let': 83, 'oh': 111, 'didnt': 24, 'ok': 112, 'thanks': 150, 'said': 128, 'work': 188, 'year': 196, 'read': 123, 'talk': 147, 'tomorrow': 161, 'awww': 6, 'poor': 120, 'hope': 69, 'feel': 31, 'better': 11, 'soon': 138, 'phone': 117, 'ive': 77, 'hi': 67, 'amazing': 1, 'make': 97, 'going': 47, 'start': 141, 'probably': 122, 'follow': 33, 'left': 82, 'hes': 65, 'getting': 44, 'song': 137, 'bit': 14, 'use': 171, 'need': 107, 'free': 37, 'great': 52, 'looking': 91, 'wanted': 176, 'ya': 193, 'morning': 104, 'send': 133, 'people': 116, 'id': 72, 'coming': 18, 'wait': 173, 'play': 119, 'wow': 190, 'thank': 149, 'welcome': 183, 'ur': 170, 'birthday': 13, 'followers': 34, 'life': 84, 'wasnt': 177, 'big': 12, 'house': 71, 'fun': 41, 'stuff': 142, 'sorry': 139, 'job': 78, 'look': 90, 'friday': 38, 'trying': 166, 'followfriday': 35, 'guys': 54, 'girl': 45, 'hahaha': 57, 'wish': 186, 'aww': 5, 'later': 81, 'tonight': 162, 'tell': 148, 'game': 43, 'pic': 118, 'home': 68, 'baby': 7, 'seen': 132, 'movie': 105, 'sounds': 140, 'times': 159, 'early': 27, 'check': 16, 'thought': 157, 'bed': 9, 'omg': 115, 'saw': 129, 'ha': 55, 'awesome': 4, 'damn': 21, 'cause': 15, 'actually': 0, 'say': 130, 'lot': 94, 'hey': 66, 'real': 124, 'lost': 93, 'luck': 96, 'sure': 145, 'away': 3, 'havent': 61, 'hard': 59, 'watch': 178, 'best': 10, 'yesterday': 198, 'weekend': 182, 'yay': 194, 'hate': 60, 'went': 184, 'funny': 42, 'shes': 134, 'days': 23, 'hehe': 63, 'guess': 53, 'live': 87, 'maybe': 100, 'isnt': 76, 'glad': 46, 'try': 165, 'missed': 103, 'summer': 144, 'sweet': 146, 'totally': 163, 'feeling': 32, 'xx': 192, 'tweet': 167, 'idea': 73, 'pretty': 121, 'sucks': 143, 'excited': 29, 'tweets': 168, 'sick': 135, 'wrong': 191, 'late': 80, 'mean': 101, 'enjoy': 28, 'man': 99, 'okay': 113, 'tho': 156, 'true': 164}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gw2NX7pkh-Vu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "580d015b-8112-4895-b3f0-72a7de46aee6"
      },
      "cell_type": "code",
      "source": [
        "test_word_counts = vectorizer.transform(X_test)\n",
        "\n",
        "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(X_test_vectorized.shape)\n",
        "X_test_vectorized.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19998, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actually</th>\n",
              "      <th>amazing</th>\n",
              "      <th>amp</th>\n",
              "      <th>away</th>\n",
              "      <th>awesome</th>\n",
              "      <th>aww</th>\n",
              "      <th>awww</th>\n",
              "      <th>baby</th>\n",
              "      <th>bad</th>\n",
              "      <th>bed</th>\n",
              "      <th>...</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrong</th>\n",
              "      <th>xx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>youre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.66453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   actually  amazing  amp  away  awesome  aww  awww  baby  bad  bed  ...    \\\n",
              "0       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "1       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "2       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "3       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "4       0.0      0.0  0.0   0.0      0.0  0.0   0.0   0.0  0.0  0.0  ...     \n",
              "\n",
              "   wow  wrong   xx       ya  yay  yeah  year  yes  yesterday  youre  \n",
              "0  0.0    0.0  0.0  0.00000  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "1  0.0    0.0  0.0  0.66453  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "2  0.0    0.0  0.0  0.00000  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "3  0.0    0.0  0.0  0.00000  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "4  0.0    0.0  0.0  0.00000  0.0   0.0   0.0  0.0        0.0    0.0  \n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "RAXPYrOMiJSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c70ab786-b85c-4de2-ba71-b69f4a263f5d"
      },
      "cell_type": "code",
      "source": [
        "#Multinomial Naive Bayes Model on Count Vectorized data\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB = MultinomialNB().fit(X_train_vectorized, y_train)\n",
        "train_predictions = MNB.predict(X_train_vectorized)\n",
        "test_predictions = MNB.predict(X_test_vectorized)\n",
        "print(f'Train Roc-Auc: {roc_auc_score(y_train, train_predictions)}')\n",
        "print(f'Test Roc-Auc: {roc_auc_score(y_test, test_predictions)}')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Roc-Auc: 0.6641576477345039\n",
            "Test Roc-Auc: 0.6602522759601707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tWAd8jxbielK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#XGBoostClassifier Model on Count Vectorized data\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "XGB = XGBClassifier(n_estimators=200, num_class=len(twitters['Sentiment'].unique()), objective='multi:softmax').fit(X_train_vectorized, y_train)\n",
        "train_predictions = XGB.predict(X_train_vectorized)\n",
        "test_predictions = XGB.predict(X_test_vectorized)\n",
        "print(f'Train Roc-Auc: {roc_auc_score(y_train, train_predictions)}')\n",
        "print(f'Test Roc-Auc: {roc_auc_score(y_test, test_predictions)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sorF95UO_uGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "metadata": {
        "id": "ACSa0x1Dkcbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d84d74bd-5fee-4dfe-d604-0cf7e0f7fbed"
      },
      "cell_type": "code",
      "source": [
        "# get new data including stop words\n",
        "def clean_tweet_doc(doc):\n",
        "  \n",
        "  doc = str(doc).lower()\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  \n",
        "\t# remove punctuation from each token\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  \n",
        "  \n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "#   #filter out stop words\n",
        "#   stop_words = set(stopwords.words('english'))\n",
        "#   tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "w2v_sentences = [clean_tweet_doc(tweet) for tweet in twitters['SentimentText']]\n",
        "print(w2v_sentences[:10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['is', 'so', 'sad', 'for', 'my', 'apl', 'friend'], ['missed', 'the', 'new', 'moon', 'trailer'], ['omg', 'its', 'already'], ['omgaga', 'im', 'sooo', 'im', 'gunna', 'cry', 'ive', 'been', 'at', 'this', 'dentist', 'since', 'was', 'suposed', 'just', 'get', 'crown', 'put', 'on'], ['think', 'mi', 'bf', 'is', 'cheating', 'on', 'me', 'tt'], ['or', 'just', 'worry', 'too', 'much'], ['juuuuuuuuuuuuuuuuussssst', 'chillin'], ['sunny', 'again', 'work', 'tomorrow', 'tv', 'tonight'], ['handed', 'in', 'my', 'uniform', 'today', 'miss', 'you', 'already'], ['hmmmm', 'wonder', 'how', 'she', 'my', 'number']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DYno4d4N-LHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(w2v_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zSo2rZMjALk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e48fc0b-fa08-4de4-eace-18b234d7ebb6"
      },
      "cell_type": "code",
      "source": [
        "words = list(w2v.wv.vocab)\n",
        "print(f'Vocabulary Size: {len(words)}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "egXUj34jmljP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "7ac02c91-4c94-4cab-e074-f3b74e663c1d"
      },
      "cell_type": "code",
      "source": [
        "w2v.most_similar('twitter')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.7481160163879395),\n",
              " ('myspace', 0.7107597589492798),\n",
              " ('list', 0.6638249158859253),\n",
              " ('youtube', 0.6508448719978333),\n",
              " ('fb', 0.6391708254814148),\n",
              " ('message', 0.624267578125),\n",
              " ('comment', 0.6164907217025757),\n",
              " ('tweetdeck', 0.6153228878974915),\n",
              " ('updates', 0.6095948815345764),\n",
              " ('everyone', 0.6074749827384949)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}