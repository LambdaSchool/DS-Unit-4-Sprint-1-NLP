{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_sprint_challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QpPcbYew_ttN"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Sprint Challenge*\n",
        "\n",
        "# Natural Language Processing\n",
        "\n",
        "**Part 1 - Working with Text Data**\n",
        "Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dtotEnsStY5o",
        "outputId": "dc58fc36-1519-43b7-b118-665aac89cfd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "\n",
        "print(whitespace_string)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G9-MkBwasXx8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd7b0928-8274-42a2-afe1-a08b4312ddf3"
      },
      "source": [
        "\" \".join(whitespace_string.split())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a string that has a lot of extra whitespace.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vg1-d2aAsXLn"
      },
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fe4A3paR5Nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26ee28d5-8177-4eab-b161-0a7120c6cf04"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Full Date    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KWDiN4C9_0sq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7081be3d-c9ad-4034-d48f-c775ee5b9ca9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt', \n",
        "                 names=['Full Date'], sep='\\n')\n",
        "df['Full Date'] = pd.to_datetime(df['Full Date'])\n",
        "df['Full Date'] = df['Full Date'].astype('str')\n",
        "\n",
        "regex = '(?P<Year>[^-]+)-(?P<Month>[^-]+)-(?P<Day>[^-]+)'\n",
        "df = pd.concat([df, df['Full Date'].str.extract(regex).astype(int)], axis=1)\n",
        "df.head()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Full Date</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-03-08</td>\n",
              "      <td>2015</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-03-15</td>\n",
              "      <td>2015</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-03-22</td>\n",
              "      <td>2015</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-03-29</td>\n",
              "      <td>2015</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-04-05</td>\n",
              "      <td>2015</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Full Date  Year  Month  Day\n",
              "0  2015-03-08  2015      3    8\n",
              "1  2015-03-15  2015      3   15\n",
              "2  2015-03-22  2015      3   22\n",
              "3  2015-03-29  2015      3   29\n",
              "4  2015-04-05  2015      4    5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfmy1qKuP2e2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1fc9ed80-6b7e-4c8d-cb01-2f0666b18161"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Full Date    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s4Q0dgoe_uBW"
      },
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1xzdhyTS_3F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a39e0aa9-e272-4617-9d3b-1d0a877e2496"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv',\n",
        "                 encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7:30 :O\n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4          0           i think mi bf is cheating on me!!!   ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REcjbt9kTB0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80a9d2f2-9bab-41e5-a263-66612e2b9a2f"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99989, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnYExX5WUWLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2b8c7c36-52f2-45e9-d7ef-89b55638e35b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beER-e0KTDJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_text(text):\n",
        "  # Change to lowercase and remove punctuation\n",
        "  text = \"\".join([char.lower() for char in text\n",
        "                  if char not in string.punctuation])\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words('english')\n",
        "  # Tokenize at the word-level\n",
        "  return [i for i in word_tokenize(text) if i not in stop]\n",
        "\n",
        "df['Tokens'] = df['SentimentText'].apply(process_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_P9lGFZaYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9fccff0a-ace9-4879-f14d-bd7fe7162076"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "      <td>[sad, apl, friend]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "      <td>[missed, new, moon, trailer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "      <td>[omg, already, 730]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "      <td>[think, mi, bf, cheating, tt]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText  \\\n",
              "0          0                       is so sad for my APL frie...   \n",
              "1          0                     I missed the New Moon trail...   \n",
              "2          1                            omg its already 7:30 :O   \n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
              "4          0           i think mi bf is cheating on me!!!   ...   \n",
              "\n",
              "                                              Tokens  \n",
              "0                                 [sad, apl, friend]  \n",
              "1                       [missed, new, moon, trailer]  \n",
              "2                                [omg, already, 730]  \n",
              "3  [omgaga, im, sooo, im, gunna, cry, ive, dentis...  \n",
              "4                      [think, mi, bf, cheating, tt]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Q7hbGuUQ3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c2290180-dda2-4896-a77d-9b88a75759c3"
      },
      "source": [
        "pd.options.display.max_colwidth = 200\n",
        "df['Tokens'].sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2723                                                                                                 [hate, today]\n",
              "68530                                                                             [bryanlas, ah, well, thats, bad]\n",
              "93096                     [confessing7girl, yeah, disney, rocks, lol, well, streaming, rubbish, slow, skips, alot]\n",
              "25266                                                     [adamczar, yay, congrats, wedding, beautiful, much, fun]\n",
              "48178                                                                               [altonturley, thanx, much, ff]\n",
              "20226    [jaimemarie, oh, thank, jaime, jaime, im, genuinely, interested, jaime, youre, speech, awesome, jaime, x]\n",
              "40924    [amyatq13, happy, saturday, hot, stuff, sorry, desk, area, messy, promised, better, cubiclemate, miss, u]\n",
              "39220                                                                [amystark, wish, could, yoga, st, lukes, 530]\n",
              "94143                                                                                      [bealindo, im, nothing]\n",
              "11229    [ontd, making, tired, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd, ontd]\n",
              "Name: Tokens, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q764vszGqiUh"
      },
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e2Ji7BMhqs3M"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "The tf-idf vectorization of text gives each word in a document a number that is in proportion to its frequency in the document and inversely proportional to the number of documents in the data. This means that common words like “a” or “the” receive smaller tf-idf scores in comparison to words that are very specific to the document. The default matrix of tf-idf scores containts one row per document and as many columns as there are different words in the dataset.\n",
        "\n",
        "**Calculation**\n",
        "\n",
        "TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "IDF(w) = log_e(Total number of documents / Number of documents with term w in it)\n",
        "\n",
        "TF-IDF = TF * IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3iUeBKtG_uEK"
      },
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lik0ojkhhWOj",
        "colab_type": "text"
      },
      "source": [
        "# 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TX8OEgUP_3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4435fd13-99f2-4b35-b549-c8b8b6c5b121"
      },
      "source": [
        "# Train test split\n",
        "\n",
        "# Subset for memory purposes\n",
        "df = df.sample(25000)\n",
        "\n",
        "# First remove the list brackets and get rid of numbers...\n",
        "df['Tokens'] = [','.join(map(str, l)) for l in df['Tokens']]\n",
        "df['Tokens'] = df['Tokens'].str.replace('\\d+', '')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Tokens'], \n",
        "                              df['Sentiment'], random_state=420, test_size=0.3)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17500,), (7500,), (17500,), (7500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTpF2Q7necpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "48ec1a40-4e96-466d-8320-6f92f589b2f9"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16780    done,english,paper,today,tomorrow,science,spanish,wednesday,math,im,done,come,freshman,year\n",
              "36914                                           ali,yes,french,exam,took,last,week,horrible,horrible\n",
              "66618      bluefur,thinkreferrals,bikram,yoga,ultimate,workout,challenge,take,,class,together,decide\n",
              "22923                                                                               aaaarae,oh,sucks\n",
              "76133                         antondominique,yeah,meron,exam,dunno,halfday,pero,sa,monday,think,wala\n",
              "Name: Tokens, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IL-_jRhaTn",
        "colab_type": "text"
      },
      "source": [
        "# 2 & 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK9fF3pCgYpl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "8b33581b-e01e-4566-a4fa-d8d8f6602213"
      },
      "source": [
        "# Create a vocabulary with train data and transform train data\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=2500, stop_words='english')\n",
        "\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "train_word_counts = vectorizer.transform(X_train)\n",
        "\n",
        "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(X_train_vectorized.shape)\n",
        "X_train_vectorized.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17500, 2500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aalaap</th>\n",
              "      <th>aaron</th>\n",
              "      <th>aaroncarter</th>\n",
              "      <th>abby</th>\n",
              "      <th>abc</th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>abt</th>\n",
              "      <th>ac</th>\n",
              "      <th>accent</th>\n",
              "      <th>...</th>\n",
              "      <th>½ï</th>\n",
              "      <th>ð²ð</th>\n",
              "      <th>ðµ</th>\n",
              "      <th>ðµð</th>\n",
              "      <th>ð½ð</th>\n",
              "      <th>ð¾</th>\n",
              "      <th>ð¾ð</th>\n",
              "      <th>ð¾ñ</th>\n",
              "      <th>ñð</th>\n",
              "      <th>ññ</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aalaap  aaron  aaroncarter  abby  abc  able  absolutely  abt  ac  accent  \\\n",
              "0       0      0            0     0    0     0           0    0   0       0   \n",
              "1       0      0            0     0    0     0           0    0   0       0   \n",
              "2       0      0            0     0    0     0           0    0   0       0   \n",
              "3       0      0            0     0    0     0           0    0   0       0   \n",
              "4       0      0            0     0    0     0           0    0   0       0   \n",
              "\n",
              "   ...  ½ï  ð²ð  ðµ  ðµð  ð½ð  ð¾  ð¾ð  ð¾ñ  ñð  ññ  \n",
              "0  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "1  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "2  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "3  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "4  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "\n",
              "[5 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-fDVB-e-b3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "16085a43-982e-42c1-844e-2164fdddac10"
      },
      "source": [
        "# Transform test data with vocab from original model that we trained on\n",
        "\n",
        "test_word_counts = vectorizer.transform(X_test)\n",
        "\n",
        "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(X_test_vectorized.shape)\n",
        "X_test_vectorized.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 2500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aalaap</th>\n",
              "      <th>aaron</th>\n",
              "      <th>aaroncarter</th>\n",
              "      <th>abby</th>\n",
              "      <th>abc</th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>abt</th>\n",
              "      <th>ac</th>\n",
              "      <th>accent</th>\n",
              "      <th>...</th>\n",
              "      <th>½ï</th>\n",
              "      <th>ð²ð</th>\n",
              "      <th>ðµ</th>\n",
              "      <th>ðµð</th>\n",
              "      <th>ð½ð</th>\n",
              "      <th>ð¾</th>\n",
              "      <th>ð¾ð</th>\n",
              "      <th>ð¾ñ</th>\n",
              "      <th>ñð</th>\n",
              "      <th>ññ</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aalaap  aaron  aaroncarter  abby  abc  able  absolutely  abt  ac  accent  \\\n",
              "0       0      0            0     0    0     0           0    0   0       0   \n",
              "1       0      0            0     0    0     0           0    0   0       0   \n",
              "2       0      0            0     0    0     0           0    0   0       0   \n",
              "3       0      0            0     0    0     0           0    0   0       0   \n",
              "4       0      0            0     0    0     0           0    0   0       0   \n",
              "\n",
              "   ...  ½ï  ð²ð  ðµ  ðµð  ð½ð  ð¾  ð¾ð  ð¾ñ  ñð  ññ  \n",
              "0  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "1  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "2  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "3  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "4  ...   0    0   0    0    0   0    0    0   0   0  \n",
              "\n",
              "[5 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZS6EsM5hHr2",
        "colab_type": "text"
      },
      "source": [
        "# 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMMErrwDl_LE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fdb859bc-36d4-47f3-e243-8c28f2140ae8"
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    9844\n",
              "0    7656\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjXdpLkzhi-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e778ff5b-80be-44ad-9248-46f3a64c043f"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGB = XGBClassifier(max_depth=8,\n",
        "#                     n_estimators=20).fit(X_train_vectorized, y_train)\n",
        "logreg = LogisticRegression(solver='lbfgs', max_iter=500)\\\n",
        "                            .fit(X_train_vectorized, y_train)\n",
        "\n",
        "\n",
        "train_predictions = logreg.predict(X_train_vectorized)\n",
        "test_predictions = logreg.predict(X_test_vectorized)\n",
        "\n",
        "\n",
        "print('Train accuracy:', accuracy_score(y_train, train_predictions))\n",
        "print('Test accuracy', accuracy_score(y_test, test_predictions))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.7887428571428572\n",
            "Test accuracy 0.7206666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sorF95UO_uGx"
      },
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3NOMICpuwN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the tokens column back into list format. This is what Word2Vec wants and needs.\n",
        "\n",
        "df['Tokens'] = df['SentimentText'].apply(process_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DYno4d4N-LHR",
        "colab": {}
      },
      "source": [
        "df['Tokens'] = df['SentimentText'].apply(process_text)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(df['Tokens'], min_count=5, size=100, seed=420)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1auU2NLzm4dB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f74ea8fd-cdb6-4932-e64f-ac4fd211ca74"
      },
      "source": [
        "model.wv.most_similar('twitter')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.824047327041626),\n",
              " ('link', 0.8227037191390991),\n",
              " ('dm', 0.8197536468505859),\n",
              " ('blog', 0.819339394569397),\n",
              " ('following', 0.8165600895881653),\n",
              " ('follow', 0.8063817024230957),\n",
              " ('site', 0.8020864725112915),\n",
              " ('page', 0.7977306842803955),\n",
              " ('cherylanncole', 0.7902941107749939),\n",
              " ('carsonjdaly', 0.7894009351730347)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    }
  ]
}